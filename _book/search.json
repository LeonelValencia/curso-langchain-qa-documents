[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Curso Avanzado de T√©cnicas de Manejo y Recuperaci√≥n de Documentos con Langchain",
    "section": "",
    "text": "Introducci√≥n"
  },
  {
    "objectID": "index.html#configuraci√≥n-de-una-sola-vez",
    "href": "index.html#configuraci√≥n-de-una-sola-vez",
    "title": "Curso Avanzado de T√©cnicas de Manejo y Recuperaci√≥n de Documentos con Langchain",
    "section": "Configuraci√≥n de una sola vez",
    "text": "Configuraci√≥n de una sola vez\n\nInstalaci√≥n de pyenv.\nInstalaci√≥n de Python con:\n\npyenv install 3.11.2\n\nActivaci√≥n de Python con:\n\npyenv local 3.11.2\n\nInstalaci√≥n de Poetry.\nConfuguraci√≥n de Poetry para crear ambientes virtuales dentro de la ra√≠z del proyecto con:\n\npoetry config virtualenvs.in-project true\n\nInstalaci√≥n de dependencias con:\n\npoetry install\n\nActivaci√≥n del ambiente virtual con:\n\npoetry shell"
  },
  {
    "objectID": "index.html#ejecuci√≥n-de-la-aplicaci√≥n",
    "href": "index.html#ejecuci√≥n-de-la-aplicaci√≥n",
    "title": "Curso Avanzado de T√©cnicas de Manejo y Recuperaci√≥n de Documentos con Langchain",
    "section": "Ejecuci√≥n de la aplicaci√≥n",
    "text": "Ejecuci√≥n de la aplicaci√≥n\n\nObtenci√≥n de las variables de entorno:\n\nOPENAI_API_KEY: API Key de OpenAI.\nDOCUGAMI_API_KEY: API Key de Docugami.\nCO_API_KEY: API Key de Cohere.\n\nIngesta de datos con:\npython ingest.py\no\npoetry run python ingest.py\nInicializaci√≥n de la aplicaci√≥n con:\nstreamlit run app.py\no\npoetry run streamlit run app.py"
  },
  {
    "objectID": "notebooks/01_context_aware_text_extraction.html",
    "href": "notebooks/01_context_aware_text_extraction.html",
    "title": "Extracci√≥n de texto con base en el contexto",
    "section": "",
    "text": "Quickstart"
  },
  {
    "objectID": "notebooks/01_context_aware_text_extraction.html#importar-librer√≠as",
    "href": "notebooks/01_context_aware_text_extraction.html#importar-librer√≠as",
    "title": "Extracci√≥n de texto con base en el contexto",
    "section": "Importar librer√≠as",
    "text": "Importar librer√≠as\n\nimport pathlib\nimport re\nfrom functools import partial\nfrom typing import Generator\n\nfrom bs4 import BeautifulSoup, Doctype, NavigableString, SoupStrainer, Tag\nfrom dotenv import load_dotenv\nfrom html2text import HTML2Text\nfrom IPython.core.display import Markdown\nfrom langchain.document_loaders import DocugamiLoader, RecursiveUrlLoader\n\nload_dotenv()\n\nTrue"
  },
  {
    "objectID": "notebooks/01_context_aware_text_extraction.html#web",
    "href": "notebooks/01_context_aware_text_extraction.html#web",
    "title": "Extracci√≥n de texto con base en el contexto",
    "section": "Web",
    "text": "Web\n\nDataset y funci√≥n de utilidad\n\ndoc_url = \"https://python.langchain.com/docs/get_started/quickstart\"\n\nload_documents = partial(\n    RecursiveUrlLoader,\n    url=doc_url,\n    max_depth=3,\n    prevent_outside=True,\n    check_response_status=True,\n)\n\n\n\nExtracci√≥n de texto sin tener en cuenta el contexto\nLa primera aproximaci√≥n para extraer texto de una p√°gina web es simplemente obtener el texto de todos los elementos de la p√°gina.\n\ndef webpage_text_extractor(html: str) -&gt; str:\n    return BeautifulSoup(html, \"lxml\").get_text(separator=\"\\n\", strip=True)\n\n\nloader = load_documents(\n    extractor=webpage_text_extractor,\n)\n\ndocs_without_data_context = loader.load()\nprint(docs_without_data_context[0].page_content[:520])\n\nQuickstart | ü¶úÔ∏èüîó Langchain\nSkip to main content\nü¶úÔ∏èüîó LangChain\nDocs\nUse cases\nIntegrations\nAPI\nCommunity\nChat our docs\nLangSmith\nJS/TS Docs\nSearch\nCTRL\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nModules\nModel I/‚ÄãO\nRetrieval\nChains\nMemory\nAgents\nCallbacks\nModules\nLangChain Expression Language\nGuides\nMore\nGet started\nQuickstart\nOn this page\nQuickstart\nInstallation\n‚Äã\nTo install LangChain run:\nPip\nConda\npip\ninstall\nlangchain\nconda\ninstall\nlangchain -c conda-forge\nFor more details, see our\nInstallation guide\n.\nEn\n\n\n\n\nExtracci√≥n de texto teniendo un poco de contexto\nEl texto de la documentaci√≥n de Langchain est√° escrito en Markdown y por lo tanto, tiene una estructura que puede ser aprovechada para extraer el texto de manera m√°s precisa.\nUtilicemos una librer√≠a que nos permita convertir el texto de HTML a Markdown y as√≠ poder extraer el texto de manera m√°s precisa.\n\ndef markdown_extractor(html: str) -&gt; str:\n    html2text = HTML2Text()\n    html2text.ignore_links = False\n    html2text.ignore_images = False\n    return html2text.handle(html)\n\n\nloader = load_documents(\n    extractor=markdown_extractor,\n)\n\ndocs_with_a_bit_of_context = loader.load()\nprint(docs_with_a_bit_of_context[0].page_content[:3000])\n\nSkip to main content\n\n[ **ü¶úÔ∏èüîó LangChain**](/)[Docs](/docs/get_started/introduction)[Use\ncases](/docs/use_cases/question_answering/)[Integrations](/docs/integrations/providers)[API](https://api.python.langchain.com)[Community](/docs/community)\n\n[Chat our\ndocs](https://chat.langchain.com)[LangSmith](https://smith.langchain.com)[JS/TS\nDocs](https://js.langchain.com/docs)[](https://github.com/langchain-\nai/langchain)\n\nSearch\n\nCTRLK\n\n  * [Get started](/docs/get_started)\n\n    * [Introduction](/docs/get_started/introduction)\n    * [Installation](/docs/get_started/installation)\n    * [Quickstart](/docs/get_started/quickstart)\n  * [Modules](/docs/modules/)\n\n    * [Model I/‚ÄãO](/docs/modules/model_io/)\n\n    * [Retrieval](/docs/modules/data_connection/)\n\n    * [Chains](/docs/modules/chains/)\n\n    * [Memory](/docs/modules/memory/)\n\n    * [Agents](/docs/modules/agents/)\n\n    * [Callbacks](/docs/modules/callbacks/)\n\n    * [Modules](/docs/modules/)\n  * [LangChain Expression Language](/docs/expression_language/)\n\n  * [Guides](/docs/guides)\n\n  * [More](/docs/additional_resources)\n\n  * [](/)\n  * [Get started](/docs/get_started)\n  * Quickstart\n\nOn this page\n\n# Quickstart\n\n## Installation‚Äã\n\nTo install LangChain run:\n\n  * Pip\n  * Conda\n\n    \n    \n    pip install langchain  \n    \n    \n    \n    conda install langchain -c conda-forge  \n    \n\nFor more details, see our [Installation\nguide](/docs/get_started/installation.html).\n\n## Environment setup‚Äã\n\nUsing LangChain will usually require integrations with one or more model\nproviders, data stores, APIs, etc. For this example, we'll use OpenAI's model\nAPIs.\n\nFirst we'll need to install their Python package:\n\n    \n    \n    pip install openai  \n    \n\nAccessing the API requires an API key, which you can get by creating an\naccount and heading [here](https://platform.openai.com/account/api-keys). Once\nwe have a key we'll want to set it as an environment variable by running:\n\n    \n    \n    export OPENAI_API_KEY=\"...\"  \n    \n\nIf you'd prefer not to set an environment variable you can pass the key in\ndirectly via the `openai_api_key` named parameter when initiating the OpenAI\nLLM class:\n\n    \n    \n    from langchain.llms import OpenAI  \n      \n    llm = OpenAI(openai_api_key=\"...\")  \n    \n\n## Building an application‚Äã\n\nNow we can start building our language model application. LangChain provides\nmany modules that can be used to build language model applications. Modules\ncan be used as stand-alones in simple applications and they can be combined\nfor more complex use cases.\n\nThe most common and most important chain that LangChain helps create contains\nthree things:\n\n  * LLM: The language model is the core reasoning engine here. In order to work with LangChain, you need to understand the different types of language models and how to work with them.\n  * Prompt Templates: This provides instructions to the language model. This controls what the language model outputs, so understanding how to construct prompts and different prompting strategi\n\n\n\n\nExtracci√≥n de texto teniendo en cuenta el contexto\nSi bien, cuando utilizamos una librera√≠a para convertir el texto de HTML a Markdown pudimos extraer el texto de manera m√°s precisa, a√∫n hay algunos casos en los que no se logra extraer el texto de manera correcta.\nEs aqu√≠ donde entra en juego el dominio del problema. Con base en el conocimiento que tenemos del problema, podemos crear una funci√≥n que nos permita extraer el texto de manera m√°s precisa.\nImagina que langchain_docs_extractor es como un obrero especializado en una f√°brica cuyo trabajo es transformar materias primas (documentos HTML) en un producto terminado (un string limpio y formateado). Este obrero usa una herramienta especial, get_text, como una m√°quina para procesar las materias primas en piezas utilizables, examinando cada componente de la materia prima pieza por pieza, y usa el mismo proceso repetidamente (recursividad) para descomponer los componentes en su forma m√°s simple. Al final, ensambla todas las piezas procesadas en un producto completo y hace algunos refinamientos finales antes de que el producto salga de la f√°brica.\n\ndef langchain_docs_extractor(\n    html: str,\n    include_output_cells: bool,\n    path_url: str | None = None,\n) -&gt; str:\n    soup = BeautifulSoup(\n        html,\n        \"lxml\",\n        parse_only=SoupStrainer(name=\"article\"),\n    )\n\n    # Remove all the tags that are not meaningful for the extraction.\n    SCAPE_TAGS = [\"nav\", \"footer\", \"aside\", \"script\", \"style\"]\n    [tag.decompose() for tag in soup.find_all(SCAPE_TAGS)]\n\n    # get_text() method returns the text of the tag and all its children.\n    def get_text(tag: Tag) -&gt; Generator[str, None, None]:\n        for child in tag.children:\n            if isinstance(child, Doctype):\n                continue\n\n            if isinstance(child, NavigableString):\n                yield child.get_text()\n            elif isinstance(child, Tag):\n                if child.name in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]:\n                    text = child.get_text(strip=False)\n\n                    if text == \"API Reference:\":\n                        yield f\"&gt; **{text}**\\n\"\n                        ul = child.find_next_sibling(\"ul\")\n                        if ul is not None and isinstance(ul, Tag):\n                            ul.attrs[\"api_reference\"] = \"true\"\n                    else:\n                        yield f\"{'#' * int(child.name[1:])} \"\n                        yield from child.get_text(strip=False)\n\n                        if path_url is not None:\n                            link = child.find(\"a\")\n                            if link is not None:\n                                yield f\" [](/{path_url}/{link.get('href')})\"\n                        yield \"\\n\\n\"\n                elif child.name == \"a\":\n                    yield f\"[{child.get_text(strip=False)}]({child.get('href')})\"\n                elif child.name == \"img\":\n                    yield f\"![{child.get('alt', '')}]({child.get('src')})\"\n                elif child.name in [\"strong\", \"b\"]:\n                    yield f\"**{child.get_text(strip=False)}**\"\n                elif child.name in [\"em\", \"i\"]:\n                    yield f\"_{child.get_text(strip=False)}_\"\n                elif child.name == \"br\":\n                    yield \"\\n\"\n                elif child.name == \"code\":\n                    parent = child.find_parent()\n                    if parent is not None and parent.name == \"pre\":\n                        classes = parent.attrs.get(\"class\", \"\")\n\n                        language = next(\n                            filter(lambda x: re.match(r\"language-\\w+\", x), classes),\n                            None,\n                        )\n                        if language is None:\n                            language = \"\"\n                        else:\n                            language = language.split(\"-\")[1]\n\n                        if language in [\"pycon\", \"text\"] and not include_output_cells:\n                            continue\n\n                        lines: list[str] = []\n                        for span in child.find_all(\"span\", class_=\"token-line\"):\n                            line_content = \"\".join(\n                                token.get_text() for token in span.find_all(\"span\")\n                            )\n                            lines.append(line_content)\n\n                        code_content = \"\\n\".join(lines)\n                        yield f\"```{language}\\n{code_content}\\n```\\n\\n\"\n                    else:\n                        yield f\"`{child.get_text(strip=False)}`\"\n\n                elif child.name == \"p\":\n                    yield from get_text(child)\n                    yield \"\\n\\n\"\n                elif child.name == \"ul\":\n                    if \"api_reference\" in child.attrs:\n                        for li in child.find_all(\"li\", recursive=False):\n                            yield \"&gt; - \"\n                            yield from get_text(li)\n                            yield \"\\n\"\n                    else:\n                        for li in child.find_all(\"li\", recursive=False):\n                            yield \"- \"\n                            yield from get_text(li)\n                            yield \"\\n\"\n                    yield \"\\n\\n\"\n                elif child.name == \"ol\":\n                    for i, li in enumerate(child.find_all(\"li\", recursive=False)):\n                        yield f\"{i + 1}. \"\n                        yield from get_text(li)\n                        yield \"\\n\\n\"\n                elif child.name == \"div\" and \"tabs-container\" in child.attrs.get(\n                    \"class\", [\"\"]\n                ):\n                    tabs = child.find_all(\"li\", {\"role\": \"tab\"})\n                    tab_panels = child.find_all(\"div\", {\"role\": \"tabpanel\"})\n                    for tab, tab_panel in zip(tabs, tab_panels):\n                        tab_name = tab.get_text(strip=True)\n                        yield f\"{tab_name}\\n\"\n                        yield from get_text(tab_panel)\n                elif child.name == \"table\":\n                    thead = child.find(\"thead\")\n                    header_exists = isinstance(thead, Tag)\n                    if header_exists:\n                        headers = thead.find_all(\"th\")\n                        if headers:\n                            yield \"| \"\n                            yield \" | \".join(header.get_text() for header in headers)\n                            yield \" |\\n\"\n                            yield \"| \"\n                            yield \" | \".join(\"----\" for _ in headers)\n                            yield \" |\\n\"\n\n                    tbody = child.find(\"tbody\")\n                    tbody_exists = isinstance(tbody, Tag)\n                    if tbody_exists:\n                        for row in tbody.find_all(\"tr\"):\n                            yield \"| \"\n                            yield \" | \".join(\n                                cell.get_text(strip=True) for cell in row.find_all(\"td\")\n                            )\n                            yield \" |\\n\"\n\n                    yield \"\\n\\n\"\n                elif child.name in [\"button\"]:\n                    continue\n                else:\n                    yield from get_text(child)\n\n    joined = \"\".join(get_text(soup))\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", joined).strip()\n\n\nloader = load_documents(\n    extractor=partial(\n        langchain_docs_extractor,\n        include_output_cells=True,\n    ),\n)\n\ndocs_with_data_context = loader.load()\nprint(docs_with_data_context[0].page_content[:3000])\n\n# Quickstart\n\n## Installation‚Äã\n\nTo install LangChain run:\n\nPip\n```bash\npip install langchain\n```\n\nConda\n```bash\nconda install langchain -c conda-forge\n```\n\nFor more details, see our [Installation guide](/docs/get_started/installation.html).\n\n## Environment setup‚Äã\n\nUsing LangChain will usually require integrations with one or more model providers, data stores, APIs, etc. For this example, we'll use OpenAI's model APIs.\n\nFirst we'll need to install their Python package:\n\n```bash\npip install openai\n```\n\nAccessing the API requires an API key, which you can get by creating an account and heading [here](https://platform.openai.com/account/api-keys). Once we have a key we'll want to set it as an environment variable by running:\n\n```bash\nexport OPENAI_API_KEY=\"...\"\n```\n\nIf you'd prefer not to set an environment variable you can pass the key in directly via the `openai_api_key` named parameter when initiating the OpenAI LLM class:\n\n```python\nfrom langchain.llms import OpenAI\n\nllm = OpenAI(openai_api_key=\"...\")\n```\n\n## Building an application‚Äã\n\nNow we can start building our language model application. LangChain provides many modules that can be used to build language model applications.\nModules can be used as stand-alones in simple applications and they can be combined for more complex use cases.\n\nThe most common and most important chain that LangChain helps create contains three things:\n\n- LLM: The language model is the core reasoning engine here. In order to work with LangChain, you need to understand the different types of language models and how to work with them.\n- Prompt Templates: This provides instructions to the language model. This controls what the language model outputs, so understanding how to construct prompts and different prompting strategies is crucial.\n- Output Parsers: These translate the raw response from the LLM to a more workable format, making it easy to use the output downstream.\n\nIn this getting started guide we will cover those three components by themselves, and then go over how to combine all of them.\nUnderstanding these concepts will set you up well for being able to use and customize LangChain applications.\nMost LangChain applications allow you to configure the LLM and/or the prompt used, so knowing how to take advantage of this will be a big enabler.\n\n## LLMs‚Äã\n\nThere are two types of language models, which in LangChain are called:\n\n- LLMs: this is a language model which takes a string as input and returns a string\n- ChatModels: this is a language model which takes a list of messages as input and returns a message\n\nThe input/output for LLMs is simple and easy to understand - a string.\nBut what about ChatModels? The input there is a list of `ChatMessage`s, and the output is a single `ChatMessage`.\nA `ChatMessage` has two required components:\n\n- `content`: This is the content of the message.\n- `role`: This is the role of the entity from which the `ChatMessage` is coming from.\n\nLangChain provides several objects to easily disting\n\n\nEl archivo de salida es ahora en formato Markdown, lo que permite visualizarlo en cualquier editor de texto o en GitHub, ofreciendo una estructura de la informaci√≥n m√°s clara y accesible. Esta organizaci√≥n permite realizar cortes de texto con mayor precisi√≥n, facilitando as√≠ la obtenci√≥n de informaci√≥n m√°s pertinente y relevante.\n\nMarkdown(docs_with_data_context[0].page_content)\n\n\n\nInstallation‚Äã\nTo install LangChain run:\nPip\npip install langchain\nConda\nconda install langchain -c conda-forge\nFor more details, see our Installation guide.\n\n\nEnvironment setup‚Äã\nUsing LangChain will usually require integrations with one or more model providers, data stores, APIs, etc. For this example, we‚Äôll use OpenAI‚Äôs model APIs.\nFirst we‚Äôll need to install their Python package:\npip install openai\nAccessing the API requires an API key, which you can get by creating an account and heading here. Once we have a key we‚Äôll want to set it as an environment variable by running:\nexport OPENAI_API_KEY=\"...\"\nIf you‚Äôd prefer not to set an environment variable you can pass the key in directly via the openai_api_key named parameter when initiating the OpenAI LLM class:\nfrom langchain.llms import OpenAI\n\nllm = OpenAI(openai_api_key=\"...\")\n\n\nBuilding an application‚Äã\nNow we can start building our language model application. LangChain provides many modules that can be used to build language model applications. Modules can be used as stand-alones in simple applications and they can be combined for more complex use cases.\nThe most common and most important chain that LangChain helps create contains three things:\n\nLLM: The language model is the core reasoning engine here. In order to work with LangChain, you need to understand the different types of language models and how to work with them.\nPrompt Templates: This provides instructions to the language model. This controls what the language model outputs, so understanding how to construct prompts and different prompting strategies is crucial.\nOutput Parsers: These translate the raw response from the LLM to a more workable format, making it easy to use the output downstream.\n\nIn this getting started guide we will cover those three components by themselves, and then go over how to combine all of them. Understanding these concepts will set you up well for being able to use and customize LangChain applications. Most LangChain applications allow you to configure the LLM and/or the prompt used, so knowing how to take advantage of this will be a big enabler.\n\n\nLLMs‚Äã\nThere are two types of language models, which in LangChain are called:\n\nLLMs: this is a language model which takes a string as input and returns a string\nChatModels: this is a language model which takes a list of messages as input and returns a message\n\nThe input/output for LLMs is simple and easy to understand - a string. But what about ChatModels? The input there is a list of ChatMessages, and the output is a single ChatMessage. A ChatMessage has two required components:\n\ncontent: This is the content of the message.\nrole: This is the role of the entity from which the ChatMessage is coming from.\n\nLangChain provides several objects to easily distinguish between different roles:\n\nHumanMessage: A ChatMessage coming from a human/user.\nAIMessage: A ChatMessage coming from an AI/assistant.\nSystemMessage: A ChatMessage coming from the system.\nFunctionMessage: A ChatMessage coming from a function call.\n\nIf none of those roles sound right, there is also a ChatMessage class where you can specify the role manually. For more information on how to use these different messages most effectively, see our prompting guide.\nLangChain provides a standard interface for both, but it‚Äôs useful to understand this difference in order to construct prompts for a given language model. The standard interface that LangChain provides has two methods:\n\npredict: Takes in a string, returns a string\npredict_messages: Takes in a list of messages, returns a message.\n\nLet‚Äôs see how to work with these different types of models and these different types of inputs. First, let‚Äôs import an LLM and a ChatModel.\nfrom langchain.llms import OpenAI\nfrom langchain.chat_models import ChatOpenAI\n\nllm = OpenAI()\nchat_model = ChatOpenAI()\n\nllm.predict(\"hi!\")\n&gt;&gt;&gt; \"Hi\"\n\nchat_model.predict(\"hi!\")\n&gt;&gt;&gt; \"Hi\"\nThe OpenAI and ChatOpenAI objects are basically just configuration objects. You can initialize them with parameters like temperature and others, and pass them around.\nNext, let‚Äôs use the predict method to run over a string input.\ntext = \"What would be a good company name for a company that makes colorful socks?\"\n\nllm.predict(text)\n# &gt;&gt; Feetful of Fun\n\nchat_model.predict(text)\n# &gt;&gt; Socks O'Color\nFinally, let‚Äôs use the predict_messages method to run over a list of messages.\nfrom langchain.schema import HumanMessage\n\ntext = \"What would be a good company name for a company that makes colorful socks?\"\nmessages = [HumanMessage(content=text)]\n\nllm.predict_messages(messages)\n# &gt;&gt; Feetful of Fun\n\nchat_model.predict_messages(messages)\n# &gt;&gt; Socks O'Color\nFor both these methods, you can also pass in parameters as key word arguments. For example, you could pass in temperature=0 to adjust the temperature that is used from what the object was configured with. Whatever values are passed in during run time will always override what the object was configured with.\n\n\nPrompt templates‚Äã\nMost LLM applications do not pass user input directly into an LLM. Usually they will add the user input to a larger piece of text, called a prompt template, that provides additional context on the specific task at hand.\nIn the previous example, the text we passed to the model contained instructions to generate a company name. For our application, it‚Äôd be great if the user only had to provide the description of a company/product, without having to worry about giving the model instructions.\nPromptTemplates help with exactly this! They bundle up all the logic for going from user input into a fully formatted prompt. This can start off very simple - for example, a prompt to produce the above string would just be:\nfrom langchain.prompts import PromptTemplate\n\nprompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\nprompt.format(product=\"colorful socks\")\nWhat is a good name for a company that makes colorful socks?\nHowever, the advantages of using these over raw string formatting are several. You can ‚Äúpartial‚Äù out variables - e.g.¬†you can format only some of the variables at a time. You can compose them together, easily combining different templates into a single prompt. For explanations of these functionalities, see the section on prompts for more detail.\nPromptTemplates can also be used to produce a list of messages. In this case, the prompt not only contains information about the content, but also each message (its role, its position in the list, etc) Here, what happens most often is a ChatPromptTemplate is a list of ChatMessageTemplates. Each ChatMessageTemplate contains instructions for how to format that ChatMessage - its role, and then also its content. Let‚Äôs take a look at this below:\nfrom langchain.prompts.chat import ChatPromptTemplate\n\ntemplate = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\nhuman_template = \"{text}\"\n\nchat_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", template),\n    (\"human\", human_template),\n])\n\nchat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\n[\n    SystemMessage(content=\"You are a helpful assistant that translates English to French.\", additional_kwargs={}),\n    HumanMessage(content=\"I love programming.\")\n]\nChatPromptTemplates can also be constructed in other ways - see the section on prompts for more detail.\n\n\nOutput parsers‚Äã\nOutputParsers convert the raw output of an LLM into a format that can be used downstream. There are few main type of OutputParsers, including:\n\nConvert text from LLM -&gt; structured information (e.g.¬†JSON)\nConvert a ChatMessage into just a string\nConvert the extra information returned from a call besides the message (like OpenAI function invocation) into a string.\n\nFor full information on this, see the section on output parsers\nIn this getting started guide, we will write our own output parser - one that converts a comma separated list into a list.\nfrom langchain.schema import BaseOutputParser\n\nclass CommaSeparatedListOutputParser(BaseOutputParser):\n    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n\n    def parse(self, text: str):\n        \"\"\"Parse the output of an LLM call.\"\"\"\n        return text.strip().split(\", \")\n\nCommaSeparatedListOutputParser().parse(\"hi, bye\")\n# &gt;&gt; ['hi', 'bye']\n\n\nPromptTemplate + LLM + OutputParser‚Äã\nWe can now combine all these into one chain. This chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to a language model, and then pass the output through an (optional) output parser. This is a convenient way to bundle up a modular piece of logic. Let‚Äôs see it in action!\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import ChatPromptTemplate\nfrom langchain.schema import BaseOutputParser\n\nclass CommaSeparatedListOutputParser(BaseOutputParser):\n    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n\n    def parse(self, text: str):\n        \"\"\"Parse the output of an LLM call.\"\"\"\n        return text.strip().split(\", \")\n\ntemplate = \"\"\"You are a helpful assistant who generates comma separated lists.\nA user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\nONLY return a comma separated list, and nothing more.\"\"\"\nhuman_template = \"{text}\"\n\nchat_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", template),\n    (\"human\", human_template),\n])\nchain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()\nchain.invoke({\"text\": \"colors\"})\n# &gt;&gt; ['red', 'blue', 'green', 'yellow', 'orange']\nNote that we are using the | syntax to join these components together. This | syntax is called the LangChain Expression Language. To learn more about this syntax, read the documentation here.\n\n\nNext steps‚Äã\nThis is it! We‚Äôve now gone over how to create the core building block of LangChain applications. There is a lot more nuance in all these components (LLMs, prompts, output parsers) and a lot more different components to learn about as well. To continue on your journey:\n\nDive deeper into LLMs, prompts, and output parsers\nLearn the other key components\nRead up on LangChain Expression Language to learn how to chain these components together\nCheck out our helpful guides for detailed walkthroughs on particular topics\nExplore end-to-end use cases"
  },
  {
    "objectID": "notebooks/01_context_aware_text_extraction.html#installation",
    "href": "notebooks/01_context_aware_text_extraction.html#installation",
    "title": "Extracci√≥n de texto con base en el contexto",
    "section": "Installation‚Äã",
    "text": "Installation‚Äã\nTo install LangChain run:\nPip\npip install langchain\nConda\nconda install langchain -c conda-forge\nFor more details, see our Installation guide."
  },
  {
    "objectID": "notebooks/01_context_aware_text_extraction.html#environment-setup",
    "href": "notebooks/01_context_aware_text_extraction.html#environment-setup",
    "title": "Extracci√≥n de texto con base en el contexto",
    "section": "Environment setup‚Äã",
    "text": "Environment setup‚Äã\nUsing LangChain will usually require integrations with one or more model providers, data stores, APIs, etc. For this example, we‚Äôll use OpenAI‚Äôs model APIs.\nFirst we‚Äôll need to install their Python package:\npip install openai\nAccessing the API requires an API key, which you can get by creating an account and heading here. Once we have a key we‚Äôll want to set it as an environment variable by running:\nexport OPENAI_API_KEY=\"...\"\nIf you‚Äôd prefer not to set an environment variable you can pass the key in directly via the openai_api_key named parameter when initiating the OpenAI LLM class:\nfrom langchain.llms import OpenAI\n\nllm = OpenAI(openai_api_key=\"...\")"
  },
  {
    "objectID": "notebooks/01_context_aware_text_extraction.html#building-an-application",
    "href": "notebooks/01_context_aware_text_extraction.html#building-an-application",
    "title": "Extracci√≥n de texto con base en el contexto",
    "section": "Building an application‚Äã",
    "text": "Building an application‚Äã\nNow we can start building our language model application. LangChain provides many modules that can be used to build language model applications. Modules can be used as stand-alones in simple applications and they can be combined for more complex use cases.\nThe most common and most important chain that LangChain helps create contains three things:\n\nLLM: The language model is the core reasoning engine here. In order to work with LangChain, you need to understand the different types of language models and how to work with them.\nPrompt Templates: This provides instructions to the language model. This controls what the language model outputs, so understanding how to construct prompts and different prompting strategies is crucial.\nOutput Parsers: These translate the raw response from the LLM to a more workable format, making it easy to use the output downstream.\n\nIn this getting started guide we will cover those three components by themselves, and then go over how to combine all of them. Understanding these concepts will set you up well for being able to use and customize LangChain applications. Most LangChain applications allow you to configure the LLM and/or the prompt used, so knowing how to take advantage of this will be a big enabler."
  },
  {
    "objectID": "notebooks/01_context_aware_text_extraction.html#llms",
    "href": "notebooks/01_context_aware_text_extraction.html#llms",
    "title": "Extracci√≥n de texto con base en el contexto",
    "section": "LLMs‚Äã",
    "text": "LLMs‚Äã\nThere are two types of language models, which in LangChain are called:\n\nLLMs: this is a language model which takes a string as input and returns a string\nChatModels: this is a language model which takes a list of messages as input and returns a message\n\nThe input/output for LLMs is simple and easy to understand - a string. But what about ChatModels? The input there is a list of ChatMessages, and the output is a single ChatMessage. A ChatMessage has two required components:\n\ncontent: This is the content of the message.\nrole: This is the role of the entity from which the ChatMessage is coming from.\n\nLangChain provides several objects to easily distinguish between different roles:\n\nHumanMessage: A ChatMessage coming from a human/user.\nAIMessage: A ChatMessage coming from an AI/assistant.\nSystemMessage: A ChatMessage coming from the system.\nFunctionMessage: A ChatMessage coming from a function call.\n\nIf none of those roles sound right, there is also a ChatMessage class where you can specify the role manually. For more information on how to use these different messages most effectively, see our prompting guide.\nLangChain provides a standard interface for both, but it‚Äôs useful to understand this difference in order to construct prompts for a given language model. The standard interface that LangChain provides has two methods:\n\npredict: Takes in a string, returns a string\npredict_messages: Takes in a list of messages, returns a message.\n\nLet‚Äôs see how to work with these different types of models and these different types of inputs. First, let‚Äôs import an LLM and a ChatModel.\nfrom langchain.llms import OpenAI\nfrom langchain.chat_models import ChatOpenAI\n\nllm = OpenAI()\nchat_model = ChatOpenAI()\n\nllm.predict(\"hi!\")\n&gt;&gt;&gt; \"Hi\"\n\nchat_model.predict(\"hi!\")\n&gt;&gt;&gt; \"Hi\"\nThe OpenAI and ChatOpenAI objects are basically just configuration objects. You can initialize them with parameters like temperature and others, and pass them around.\nNext, let‚Äôs use the predict method to run over a string input.\ntext = \"What would be a good company name for a company that makes colorful socks?\"\n\nllm.predict(text)\n# &gt;&gt; Feetful of Fun\n\nchat_model.predict(text)\n# &gt;&gt; Socks O'Color\nFinally, let‚Äôs use the predict_messages method to run over a list of messages.\nfrom langchain.schema import HumanMessage\n\ntext = \"What would be a good company name for a company that makes colorful socks?\"\nmessages = [HumanMessage(content=text)]\n\nllm.predict_messages(messages)\n# &gt;&gt; Feetful of Fun\n\nchat_model.predict_messages(messages)\n# &gt;&gt; Socks O'Color\nFor both these methods, you can also pass in parameters as key word arguments. For example, you could pass in temperature=0 to adjust the temperature that is used from what the object was configured with. Whatever values are passed in during run time will always override what the object was configured with."
  },
  {
    "objectID": "notebooks/01_context_aware_text_extraction.html#prompt-templates",
    "href": "notebooks/01_context_aware_text_extraction.html#prompt-templates",
    "title": "Extracci√≥n de texto con base en el contexto",
    "section": "Prompt templates‚Äã",
    "text": "Prompt templates‚Äã\nMost LLM applications do not pass user input directly into an LLM. Usually they will add the user input to a larger piece of text, called a prompt template, that provides additional context on the specific task at hand.\nIn the previous example, the text we passed to the model contained instructions to generate a company name. For our application, it‚Äôd be great if the user only had to provide the description of a company/product, without having to worry about giving the model instructions.\nPromptTemplates help with exactly this! They bundle up all the logic for going from user input into a fully formatted prompt. This can start off very simple - for example, a prompt to produce the above string would just be:\nfrom langchain.prompts import PromptTemplate\n\nprompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\nprompt.format(product=\"colorful socks\")\nWhat is a good name for a company that makes colorful socks?\nHowever, the advantages of using these over raw string formatting are several. You can ‚Äúpartial‚Äù out variables - e.g.¬†you can format only some of the variables at a time. You can compose them together, easily combining different templates into a single prompt. For explanations of these functionalities, see the section on prompts for more detail.\nPromptTemplates can also be used to produce a list of messages. In this case, the prompt not only contains information about the content, but also each message (its role, its position in the list, etc) Here, what happens most often is a ChatPromptTemplate is a list of ChatMessageTemplates. Each ChatMessageTemplate contains instructions for how to format that ChatMessage - its role, and then also its content. Let‚Äôs take a look at this below:\nfrom langchain.prompts.chat import ChatPromptTemplate\n\ntemplate = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\nhuman_template = \"{text}\"\n\nchat_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", template),\n    (\"human\", human_template),\n])\n\nchat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\n[\n    SystemMessage(content=\"You are a helpful assistant that translates English to French.\", additional_kwargs={}),\n    HumanMessage(content=\"I love programming.\")\n]\nChatPromptTemplates can also be constructed in other ways - see the section on prompts for more detail."
  },
  {
    "objectID": "notebooks/01_context_aware_text_extraction.html#output-parsers",
    "href": "notebooks/01_context_aware_text_extraction.html#output-parsers",
    "title": "Extracci√≥n de texto con base en el contexto",
    "section": "Output parsers‚Äã",
    "text": "Output parsers‚Äã\nOutputParsers convert the raw output of an LLM into a format that can be used downstream. There are few main type of OutputParsers, including:\n\nConvert text from LLM -&gt; structured information (e.g.¬†JSON)\nConvert a ChatMessage into just a string\nConvert the extra information returned from a call besides the message (like OpenAI function invocation) into a string.\n\nFor full information on this, see the section on output parsers\nIn this getting started guide, we will write our own output parser - one that converts a comma separated list into a list.\nfrom langchain.schema import BaseOutputParser\n\nclass CommaSeparatedListOutputParser(BaseOutputParser):\n    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n\n    def parse(self, text: str):\n        \"\"\"Parse the output of an LLM call.\"\"\"\n        return text.strip().split(\", \")\n\nCommaSeparatedListOutputParser().parse(\"hi, bye\")\n# &gt;&gt; ['hi', 'bye']"
  },
  {
    "objectID": "notebooks/01_context_aware_text_extraction.html#prompttemplate-llm-outputparser",
    "href": "notebooks/01_context_aware_text_extraction.html#prompttemplate-llm-outputparser",
    "title": "Extracci√≥n de texto con base en el contexto",
    "section": "PromptTemplate + LLM + OutputParser‚Äã",
    "text": "PromptTemplate + LLM + OutputParser‚Äã\nWe can now combine all these into one chain. This chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to a language model, and then pass the output through an (optional) output parser. This is a convenient way to bundle up a modular piece of logic. Let‚Äôs see it in action!\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import ChatPromptTemplate\nfrom langchain.schema import BaseOutputParser\n\nclass CommaSeparatedListOutputParser(BaseOutputParser):\n    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n\n    def parse(self, text: str):\n        \"\"\"Parse the output of an LLM call.\"\"\"\n        return text.strip().split(\", \")\n\ntemplate = \"\"\"You are a helpful assistant who generates comma separated lists.\nA user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\nONLY return a comma separated list, and nothing more.\"\"\"\nhuman_template = \"{text}\"\n\nchat_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", template),\n    (\"human\", human_template),\n])\nchain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()\nchain.invoke({\"text\": \"colors\"})\n# &gt;&gt; ['red', 'blue', 'green', 'yellow', 'orange']\nNote that we are using the | syntax to join these components together. This | syntax is called the LangChain Expression Language. To learn more about this syntax, read the documentation here."
  },
  {
    "objectID": "notebooks/01_context_aware_text_extraction.html#next-steps",
    "href": "notebooks/01_context_aware_text_extraction.html#next-steps",
    "title": "Extracci√≥n de texto con base en el contexto",
    "section": "Next steps‚Äã",
    "text": "Next steps‚Äã\nThis is it! We‚Äôve now gone over how to create the core building block of LangChain applications. There is a lot more nuance in all these components (LLMs, prompts, output parsers) and a lot more different components to learn about as well. To continue on your journey:\n\nDive deeper into LLMs, prompts, and output parsers\nLearn the other key components\nRead up on LangChain Expression Language to learn how to chain these components together\nCheck out our helpful guides for detailed walkthroughs on particular topics\nExplore end-to-end use cases"
  },
  {
    "objectID": "notebooks/01_context_aware_text_extraction.html#pdf-docx-doc",
    "href": "notebooks/01_context_aware_text_extraction.html#pdf-docx-doc",
    "title": "Extracci√≥n de texto con base en el contexto",
    "section": "PDF / DOCX / DOC",
    "text": "PDF / DOCX / DOC\n\nDataset de prueba\nEn este ejemplo, vamos a emplear algunos archivos de muestra proporcionados por Docugami. Dichos archivos representan el producto de la extracci√≥n de texto de documentos aut√©nticos, en particular, de archivos PDF relativos a contratos de arrendamiento comercial.\n\nlease_data_dir = pathlib.Path(\"../data/docugami/commercial_lease\")\nlease_files = list(lease_data_dir.glob(\"*.xml\"))\nlease_files\n\n[PosixPath('../data/docugami/commercial_lease/TruTone Lane 6.xml'),\n PosixPath('../data/docugami/commercial_lease/TruTone Lane 5.xml'),\n PosixPath('../data/docugami/commercial_lease/TruTone Lane 4.xml'),\n PosixPath('../data/docugami/commercial_lease/TruTone Lane 1.xml'),\n PosixPath('../data/docugami/commercial_lease/TruTone Lane 3.xml'),\n PosixPath('../data/docugami/commercial_lease/TruTone Lane 2.xml')]\n\n\nAhora, carguemos los documentos de muestra y veamos qu√© propiedades tienen.\n\nloader = DocugamiLoader(\n    docset_id=None,\n    access_token=None,\n    document_ids=None,\n    file_paths=lease_files,\n)\n\nlease_docs = loader.load()\nf\"Loaded {len(lease_docs)} documents.\"\n\n'Loaded 1108 documents.'\n\n\nLa metadata obtenida del documento incluye los siguientes elementos:\n\nid, source_id y name: Estos campos identifican de manera un√≠voca al documento y al fragmento de texto que se ha extra√≠do de √©l.\nxpath: Es el XPath correspondiente dentro de la representaci√≥n XML del documento. Se refiere espec√≠ficamente al fragmento extra√≠do. Este campo es √∫til para referenciar directamente las citas del fragmento real dentro del documento XML.\nstructure: Incluye los atributos estructurales del fragmento, tales como p, h1, div, table, td, entre otros. Es √∫til para filtrar ciertos tipos de fragmentos, en caso de que el usuario los requiera.\ntag: Representa la etiqueta sem√°ntica para el fragmento. Se genera utilizando diversas t√©cnicas, tanto generativas como extractivas, para determinar el significado del fragmento en cuesti√≥n.\n\n\nlease_docs[0].metadata\n\n{'xpath': '/dg:chunk/docset:OFFICELEASEAGREEMENT-section/docset:OFFICELEASEAGREEMENT-section/docset:OFFICELEASEAGREEMENT/docset:Lease',\n 'id': 'TruTone Lane 6.xml',\n 'name': 'TruTone Lane 6.xml',\n 'source': 'TruTone Lane 6.xml',\n 'structure': 'p',\n 'tag': 'Lease'}\n\n\nDocugami tambi√©n posee la capacidad de asistir en la extracci√≥n de metadatos espec√≠ficos para cada chunk o fragmento de nuestros documentos. A continuaci√≥n, se presenta un ejemplo de c√≥mo se extraen y representan estos metadatos:\n{\n    'xpath': '/docset:OFFICELEASEAGREEMENT-section/docset:OFFICELEASEAGREEMENT/docset:LeaseParties',\n    'id': 'v1bvgaozfkak',\n    'source': 'TruTone Lane 2.docx',\n    'structure': 'p',\n    'tag': 'LeaseParties',\n    'Lease Date': 'April 24 \\n\\n ,',\n    'Landlord': 'BUBBA CENTER PARTNERSHIP',\n    'Tenant': 'Truetone Lane LLC',\n    'Lease Parties': 'Este ACUERDO DE ARRENDAMIENTO DE OFICINA (el \"Contrato\") es celebrado por y entre BUBBA CENTER PARTNERSHIP (\"Arrendador\"), y Truetone Lane LLC, una compa√±√≠a de responsabilidad limitada de Delaware (\"Arrendatario\").'\n}\nLos metadatos adicionales, como los mostrados arriba, pueden ser extremadamente √∫tiles cuando se implementan self-retrievers, los cuales ser√°n explorados adetalle m√°s adelante.\n\n\nCargar tus documentos\nSi prefieres utilizar tus propios documentos, puedes cargarlos a trav√©s de la interfaz gr√°fica de Docugami. Una vez cargados, necesitar√°s asignar cada uno a un docset. Un docset es un conjunto de documentos que presentan una estructura an√°loga. Por ejemplo, todos los contratos de arrendamiento comercial por lo general poseen estructuras similares, por lo que pueden ser agrupados en un √∫nico docset.\nDespu√©s de crear tu docset, los documentos cargados ser√°n procesados y estar√°n disponibles para su acceso mediante la API de Docugami.\nPara recuperar los ids de tus documentos y de sus correspondientes docsets, puedes ejecutar el siguiente comando:\ncurl --header \"Authorization: Bearer {YOUR_DOCUGAMI_TOKEN}\" \\\n  https://api.docugami.com/v1preview1/documents\nEste comando te facilitar√° el acceso a la informaci√≥n relevante, optimizando as√≠ la administraci√≥n y organizaci√≥n de tus documentos dentro de Docugami.\nUna vez hayas extra√≠do los ids de tus documentos o de los docsets, podr√°s emplearlos para acceder a la informaci√≥n de tus documentos utilizando el DocugamiLoader de Langchain. Esto te permitir√° manipular y gestionar tus documentos dentro de tu aplicaci√≥n.\n\nloader = DocugamiLoader(\n    docset_id=\"xpfpiyl7cep2\",\n    document_ids=None,\n    file_paths=None,\n)\n\npapers_docs = loader.load()\n\n\nlost_in_the_middle_paper_docs = [\n    doc for doc in papers_docs if doc.metadata[\"source\"] == \"2307.03172.pdf\"\n]\nfor doc in lost_in_the_middle_paper_docs:\n    print(doc.metadata[\"tag\"])\n\nLostintheMiddle\nchunk\nchunk\nAbstract\nchunk\nAnImportantAndFlexibleBuildingBlock\nTheseUse-cases\nchunk\nchunk\nchunk\nFigure1\nchunk\nTransformers\nExtended-contextLanguageModels\nTrolledExperiments\nADistinctiveU-shapedPerformance\n_5-turboS\nLanguageModels\nLanguageModels\nACaseStudy\n_2LanguageModels\nIncreasingLanguageModelMaximumContext\nOurGoal\nchunk\nModelPerformance\nOurMulti-documentQuestion\nThisTask\nNaturalquestions-open\nRandomDocuments\nAHigh-qualityAnswer\nAsian\nchunk\nchunk\nAHigh-qualityAnswer\nchunk\nDocument\nNorwegian\nQuestion\nchunk\nFigure3\nTheInputContextLength\nKandpal\nSearchResults\nAHigh-qualityAnswer\nchunk\nAsian\nQuestion\nchunk\nFigure4\nTheNaturalquestionsAnnotations\nchunk\nAMaximumContextLength\nchunk\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\n_7-cell\nExtracttheValueCorrespondingtotheSpecifiedKeyintheJSONObjectBelowVa\ntd\nExtracttheValueCorrespondingtotheSpecifiedKeyintheJSONObjectBelowEx\nExtracttheValueCorrespondingtotheSpecifiedKeyintheJSONObjectBelow\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\nFigure9-cell\ntd\ntd\ntd\nThe-art\nAMaximumContextLength\nClosedModels\nGpt-35-turbo\nTheAnthropicApi\nInputContexts\nModelPerformance\nASubset\nFigure6\nContexts\nModelPerformance\nPerformanceDecrease\nExtended-contextModels\nLanguageModels\nOurSyntheticKey-valueRetrievalTask\nOurSyntheticKey-valueRetrievalTask\nFigure8\nPresentPotentialConfounders\nKey-valueRetrievalPerformance\nchunk\nTheSyntheticKey-valueRetrievalTask\nTheKey-valueRetrievalTask\nThe140Key-valueSetting\nchunk\nOurMulti-documentQuestion\nchunk\nTheOpenModels\nOurExperiments\ntr\nchunk\nTheEnd\nTheSameSetting\nTheModels\nMpt-30bMpt-30b-instruct\nchunk\nFormation\nTheseObservations\nchunk\nPracticalSettings\nModels\nFigure\ntd\nchunk\nchunk\nARichLine\nchunk\nThePioneeringWork\nTheU-shapedCurve\nASeries\nInstruction-tuning\nSewonMin\nAviArampatzis\nIzBeltagy\nHyungWonChung\nZihangDai\nMicha√∏Daniluk\nTriDao\nchunk\nchunk\nDanielYFu\nAlbertGu\nchunk\nLong-textUnderstanding\nGautierIzacard\nGautierIzacard\nNikhilKandpal\nUrvashiKhandelwal\nchunk\nField\nKentonLee\nDachengLi\nAlexMallen\nSewonMin\nBennetBMurdockJr\nJoeOConnor\nDimitrisPapailiopoulos\nchunk\nHaoPeng\nFabioPetroni\nMichaelPoli\nOfirPress\nOfirPress\nGuanghuiQin\nchunk\nIneLee\nYoavLevine\nchunk\nchunk\nChinnadhuraiSankar\nTimoSchick\nchunk\nOmerLevy\nUriShaham\nVatsalSharan\nWeijiaShi\nchunk\nchunk\nKalpeshKrishna\nYiTay\nYiTay\nMostafaDehghani\nHugoTouvron\nAshishVaswani\nSinongWang\nchunk\nchunk\nchunk\nchunk\nManzilZaheer\nchunk\nPastWork\nchunk\nchunk\ntd\ntd\nFigure15\nMulti-documentQuestion\nchunk\nchunk\ntd\ntd\nchunk\nOurPrompt\nchunk\nASubset\nchunk\nFigure17\n_1st5th10th15th20thPosition\ntd\nchunk\nTable\ntd\ntd\ntd\ntd\ntd\nTable1\nTable\ntd\ntd\ntd\ntd\ntd\nTable2\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\nTable3\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\nTable4\nchunk\nModelPerformance\ntd\ntd\ntd\ntd\ntd\ntd\ntd\nTable\ntd\ntd\ntd\ntd\ntd\ntd\ntd\nTable\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\ntd\nTable"
  },
  {
    "objectID": "notebooks/03_indexing_vectors.html#librer√≠as",
    "href": "notebooks/03_indexing_vectors.html#librer√≠as",
    "title": "Indexaci√≥n de vectores",
    "section": "Librer√≠as",
    "text": "Librer√≠as\n\nfrom dotenv import load_dotenv\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.indexes import SQLRecordManager, index\nfrom langchain.schema import Document\nfrom langchain.vectorstores import Chroma\n\nfrom src.langchain_docs_loader import load_langchain_docs_splitted\n\nload_dotenv()  # It should output True\n\nTrue"
  },
  {
    "objectID": "notebooks/03_indexing_vectors.html#carga-de-datos",
    "href": "notebooks/03_indexing_vectors.html#carga-de-datos",
    "title": "Indexaci√≥n de vectores",
    "section": "Carga de datos",
    "text": "Carga de datos\n\ndocs = load_langchain_docs_splitted()\nf\"Loaded {len(docs)} documents\"\n\n'Loaded 2684 documents'"
  },
  {
    "objectID": "notebooks/03_indexing_vectors.html#inicializaci√≥n-de-componentes-de-un-√≠ndice",
    "href": "notebooks/03_indexing_vectors.html#inicializaci√≥n-de-componentes-de-un-√≠ndice",
    "title": "Indexaci√≥n de vectores",
    "section": "Inicializaci√≥n de componentes de un √≠ndice",
    "text": "Inicializaci√≥n de componentes de un √≠ndice\nPara crear un √≠ndice es necesario inicializar:\n\nVectorstore: para almacenar los vectores de los documentos.\nRecord Manager: para almacenar qu√© vectores han sidoe indexados y cu√°ndo.\n\n\ncollection_name = \"langchain_docs_index\"\nembeddings = OpenAIEmbeddings()\nvector_store = Chroma(\n    collection_name=collection_name,\n    embedding_function=embeddings,\n)\n\n\nnamespace = \"croma/{collection_name}\"\nrecord_manager = SQLRecordManager(\n    namespace=namespace,\n    db_url=\"sqlite:///:memory:\",\n)\nrecord_manager.create_schema()\n\n\nEs una buena practica inicializar el namespace de nuestro Record Manager con el nombre de nuestra Vectorstore y el nombre del Collection que estamos indexando."
  },
  {
    "objectID": "notebooks/03_indexing_vectors.html#indexaci√≥n",
    "href": "notebooks/03_indexing_vectors.html#indexaci√≥n",
    "title": "Indexaci√≥n de vectores",
    "section": "Indexaci√≥n",
    "text": "Indexaci√≥n\n\nFunci√≥n de utilidad para limpiar nuestro √≠ndice\n\ndef clear_index():\n    \"\"\"Hacky helper method to clear content. See the `full` mode section to to understand why it works.\"\"\"\n    index(\n        [],\n        record_manager=record_manager,\n        vector_store=vector_store,\n        cleanup=\"full\",\n        source_id_key=\"source\",\n    )\n\n\n\nIndexaci√≥n con limpieza de tipo None\nEsta implementaci√≥n es la opci√≥n por defecto. La especificaci√≥n no remueve los documentos previamente indexados. Sin embargo, s√≠ se encarga de de remover los documentos duplicados antes de indexarlos.\n\nindex(\n    docs_source=docs,\n    record_manager=record_manager,\n    vector_store=vector_store,\n    source_id_key=\"source\",\n    cleanup=None,\n)\n\n{'num_added': 2601, 'num_updated': 0, 'num_skipped': 2, 'num_deleted': 0}\n\n\nSi dentro de un tiempo volvemos a ejecutar nuestro c√≥digo de carga de datos y los documentos ya existen en el √≠ndice, no se volver√°n a indexar.\n\nindex(\n    docs_source=docs,\n    record_manager=record_manager,\n    vector_store=vector_store,\n    source_id_key=\"source\",\n    cleanup=None,\n)\n\n{'num_added': 0, 'num_updated': 0, 'num_skipped': 2603, 'num_deleted': 0}\n\n\n\nclear_index()\n\n\n\nIndexaci√≥n con limpieza de tipo incremental\nAl igual que la limpieza de tipo None, la limpieza incremental maneja los documentos duplicados antes de indexarlos. Sin embargo, si alguno de los vectores de un source / recurso es diferente al que ya existe en el √≠ndice, se reemplazar√° el vector existente por el nuevo.\n\nindex(\n    docs_source=docs,\n    record_manager=record_manager,\n    vector_store=vector_store,\n    source_id_key=\"source\",\n    cleanup=\"incremental\",\n)\n\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 917743 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 898707 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 921808 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 909775 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 899857 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 940071 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 925449 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 898953 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n\n\n{'num_added': 2601, 'num_updated': 0, 'num_skipped': 2, 'num_deleted': 0}\n\n\nSi llamamos a la funci√≥n nuevamente, pero sin ning√∫n documento, entonces nada se eliminar√° del √≠ndice.\n\nindex(\n    docs_source=[],\n    record_manager=record_manager,\n    vector_store=vector_store,\n    source_id_key=\"source\",\n    cleanup=\"incremental\",\n)\n\n{'num_added': 0, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}\n\n\nSi agregamos un nuevo documento, entonces se indexar√°.\n\nindex(\n    docs_source=[Document(page_content=\"Hello World\", metadata={\"source\": \"hello\"})],\n    record_manager=record_manager,\n    vector_store=vector_store,\n    source_id_key=\"source\",\n    cleanup=\"incremental\",\n)\n\n{'num_added': 1, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}\n\n\nY si modificamos un documento existente, entonces se reemplazar√° el vector existente por el nuevo.\n\nindex(\n    docs_source=[\n        Document(\n            page_content=\"Hello World, from Langchain!\", metadata={\"source\": \"hello\"}\n        )\n    ],\n    record_manager=record_manager,\n    vector_store=vector_store,\n    source_id_key=\"source\",\n    cleanup=\"incremental\",\n)\n\n{'num_added': 1, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 1}\n\n\n\nclear_index()\n\n\n\nIndexaci√≥n con limpieza de tipo full\nCualquier documento que no sea parte de la carga actual ser√° eliminado del √≠ndice. Esto es √∫til cuando se quiere mantener el √≠ndice actualizado con los documentos que se encuentran en el origen de datos. Los documentos que no han sido modificados no ser√°n indexados nuevamente.\n\nindex(\n    docs_source=docs,\n    record_manager=record_manager,\n    vector_store=vector_store,\n    source_id_key=\"source\",\n    cleanup=\"full\",\n)\n\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 893441 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 928130 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 923893 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 918641 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 922553 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 890289 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 888897 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 892716 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 889742 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 952551 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 951868 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n\n\n{'num_added': 2601, 'num_updated': 0, 'num_skipped': 2, 'num_deleted': 0}\n\n\nIndexemos nuevamente los documentos, pero s√≥lo con una peque√±a proporci√≥n de los mismos.\n\nindex(\n    docs_source=docs[0:100],\n    record_manager=record_manager,\n    vector_store=vector_store,\n    source_id_key=\"source\",\n    cleanup=\"full\",\n)\n\n{'num_added': 0, 'num_updated': 0, 'num_skipped': 97, 'num_deleted': 2504}\n\n\nLa funci√≥n clear_index es un caso de uso para la limpieza de tipo full\n\nclear_index()\n\n\n\nIndexaci√≥n a partir de un BaseLoader de Langchain\nLangchain estable el concepto de BaseLoader como clases que se encargan de cargar datos de diferentes fuentes de datos o con un procesamiento espec√≠fico. Estos pueden ser extendidos para crear Loaders personalizados. Y, a su vez, ser utilizados para indexar documentos dentro de nuestro pipeline de ingesta.\n\nfrom langchain.document_loaders.base import BaseLoader\n\n\nclass MyDocumentLoader(BaseLoader):\n    \"\"\"Here should be the logic to load the documents from the source.\n\n    The `load` method should return a list of `Document` objects.\n\n    In this example, we will just return the `docs` variable defined above.\n    \"\"\"\n\n    def load(self) -&gt; list[Document]:\n        return docs\n\n\nindex(\n    docs_source=MyDocumentLoader(),\n    record_manager=record_manager,\n    vector_store=vector_store,\n    source_id_key=\"source\",\n    cleanup=\"full\",\n)\n\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 895998 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 923036 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 912793 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 959601 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 936087 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 907817 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 921967 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 921261 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 916848 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 908604 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 902224 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n\n\n{'num_added': 2601, 'num_updated': 0, 'num_skipped': 2, 'num_deleted': 0}\n\n\n\nclear_index()"
  },
  {
    "objectID": "notebooks/08_semantic_reranking.html#librer√≠as",
    "href": "notebooks/08_semantic_reranking.html#librer√≠as",
    "title": "Re-ranking sem√°ntico",
    "section": "Librer√≠as",
    "text": "Librer√≠as\n\nimport cohere\nfrom dotenv import load_dotenv\nfrom langchain.retrievers import BM25Retriever\nfrom langchain.schema import Document\n\nfrom src.langchain_docs_loader import load_langchain_docs_splitted\n\nload_dotenv()\n\nTrue"
  },
  {
    "objectID": "notebooks/08_semantic_reranking.html#carga-de-datos",
    "href": "notebooks/08_semantic_reranking.html#carga-de-datos",
    "title": "Re-ranking sem√°ntico",
    "section": "Carga de datos",
    "text": "Carga de datos\n\ndocs = load_langchain_docs_splitted()"
  },
  {
    "objectID": "notebooks/08_semantic_reranking.html#creaci√≥n-de-herramienta-de-b√∫squeda-por-palabras-clave",
    "href": "notebooks/08_semantic_reranking.html#creaci√≥n-de-herramienta-de-b√∫squeda-por-palabras-clave",
    "title": "Re-ranking sem√°ntico",
    "section": "Creaci√≥n de herramienta de b√∫squeda por palabras clave",
    "text": "Creaci√≥n de herramienta de b√∫squeda por palabras clave\nBM25 es una funci√≥n de ranking avanzada usada para clasificar documentos en sistemas de recuperaci√≥n de informaci√≥n, bas√°ndose en su relevancia respecto a una consulta de b√∫squeda. A diferencia de la b√∫squeda por palabras clave b√°sica, que solo considera la presencia o ausencia de palabras, BM25 calcula un score de relevancia para cada documento, teniendo en cuenta la frecuencia de aparici√≥n del t√©rmino y su rareza en la colecci√≥n de documentos.\n\nkeywordk_retriever = BM25Retriever.from_documents(docs)\n\n\ndef keyword_document_search(query: str, k: int) -&gt; list[Document]:\n    keywordk_retriever.k = k\n    return keywordk_retriever.get_relevant_documents(query)"
  },
  {
    "objectID": "notebooks/08_semantic_reranking.html#b√∫squeda-de-documentos-relevantes-por-palabras-clave",
    "href": "notebooks/08_semantic_reranking.html#b√∫squeda-de-documentos-relevantes-por-palabras-clave",
    "title": "Re-ranking sem√°ntico",
    "section": "B√∫squeda de documentos relevantes por palabras clave",
    "text": "B√∫squeda de documentos relevantes por palabras clave\n\nrelevant_keyword_documents = keyword_document_search(\n    query=\"How to integrate LCEL into my Retrieval augmented generation system with a keyword search retriever?\",\n    k=100,\n)\n\nprint(\"Keyword search results:\")\nfor i, document in enumerate(relevant_keyword_documents):\n    print(f\"{i+1}. {document.metadata['source']}\")\n\nKeyword search results:\n1. https://python.langchain.com/docs/integrations/memory/remembrall\n2. https://python.langchain.com/docs/expression_language/\n3. https://python.langchain.com/docs/modules/memory/adding_memory\n4. https://python.langchain.com/docs/expression_language/cookbook/\n5. https://python.langchain.com/docs/modules/memory/\n6. https://python.langchain.com/docs/additional_resources/tutorials\n7. https://python.langchain.com/docs/additional_resources/tutorials\n8. https://python.langchain.com/docs/use_cases/question_answering/how_to/code/twitter-the-algorithm-analysis-deeplake\n9. https://python.langchain.com/docs/integrations/memory/motorhead_memory\n10. https://python.langchain.com/docs/use_cases/question_answering/how_to/chat_vector_db\n11. https://python.langchain.com/docs/guides/deployments/template_repos\n12. https://python.langchain.com/docs/integrations/vectorstores/elasticsearch\n13. https://python.langchain.com/docs/use_cases/question_answering/how_to/flare\n14. https://python.langchain.com/docs/modules/memory/adding_memory\n15. https://python.langchain.com/docs/integrations/providers/myscale\n16. https://python.langchain.com/docs/guides/langsmith/\n17. https://python.langchain.com/docs/use_cases/question_answering/how_to/multi_retrieval_qa_router\n18. https://python.langchain.com/docs/use_cases/question_answering/how_to/code/\n19. https://python.langchain.com/docs/integrations/document_loaders/reddit\n20. https://python.langchain.com/docs/use_cases/more/agents/agent_simulations/multi_player_dnd\n21. https://python.langchain.com/docs/modules/agents/agent_types/chat_conversation_agent\n22. https://python.langchain.com/docs/expression_language/cookbook/tools\n23. https://python.langchain.com/docs/integrations/vectorstores/neo4jvector\n24. https://python.langchain.com/docs/integrations/document_loaders/dropbox\n25. https://python.langchain.com/docs/integrations/providers/arangodb\n26. https://python.langchain.com/docs/additional_resources/youtube\n27. https://python.langchain.com/docs/use_cases/more/agents/agent_simulations/characters\n28. https://python.langchain.com/docs/use_cases/more/agents/agent_simulations/characters\n29. https://python.langchain.com/docs/integrations/chat/fireworks\n30. https://python.langchain.com/docs/use_cases/question_answering/how_to/vector_db_qa\n31. https://python.langchain.com/docs/integrations/vectorstores/timescalevector\n32. https://python.langchain.com/docs/integrations/chat/promptlayer_chatopenai\n33. https://python.langchain.com/docs/modules/memory/types/buffer\n34. https://python.langchain.com/docs/integrations/document_loaders/ifixit\n35. https://python.langchain.com/docs/integrations/document_loaders/ifixit\n36. https://python.langchain.com/docs/integrations/tools/metaphor_search\n37. https://python.langchain.com/docs/integrations/vectorstores/elasticsearch\n38. https://python.langchain.com/docs/use_cases/question_answering/how_to/flare\n39. https://python.langchain.com/docs/integrations/tools\n40. https://python.langchain.com/docs/integrations/chat/fireworks\n41. https://python.langchain.com/docs/integrations/document_loaders/rss\n42. https://python.langchain.com/docs/integrations/retrievers/docarray_retriever\n43. https://python.langchain.com/docs/integrations/retrievers/elastic_search_bm25\n44. https://python.langchain.com/docs/integrations/tools/dataforseo\n45. https://python.langchain.com/docs/integrations/tools\n46. https://python.langchain.com/docs/integrations/llms/fireworks\n47. https://python.langchain.com/docs/integrations/callbacks/promptlayer\n48. https://python.langchain.com/docs/use_cases/question_answering/how_to/conversational_retrieval_agents\n49. https://python.langchain.com/docs/integrations/document_loaders/sitemap\n50. https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/supabase_self_query\n51. https://python.langchain.com/docs/modules/data_connection/\n52. https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/prompts_pipelining\n53. https://python.langchain.com/docs/integrations/vectorstores/marqo\n54. https://python.langchain.com/docs/guides/privacy/presidio_data_anonymization/reversible\n55. https://python.langchain.com/docs/guides/deployments/\n56. https://python.langchain.com/docs/use_cases/question_answering/how_to/local_retrieval_qa\n57. https://python.langchain.com/docs/integrations/document_loaders/hugging_face_dataset\n58. https://python.langchain.com/docs/integrations/memory\n59. https://python.langchain.com/docs/integrations/providers/neo4j\n60. https://python.langchain.com/docs/modules/data_connection/retrievers/web_research\n61. https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/myscale_self_query\n62. https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/myscale_self_query\n63. https://python.langchain.com/docs/integrations/text_embedding/sagemaker-endpoint\n64. https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/timescalevector_self_query\n65. https://python.langchain.com/docs/use_cases/more/agents/agent_simulations/camel_role_playing\n66. https://python.langchain.com/docs/use_cases/more/agents/agents/camel_role_playing\n67. https://python.langchain.com/docs/integrations/providers/yeagerai\n68. https://python.langchain.com/docs/integrations/providers/langchain_decorators\n69. https://python.langchain.com/docs/modules/data_connection/document_transformers/\n70. https://python.langchain.com/docs/integrations/callbacks/streamlit\n71. https://python.langchain.com/docs/integrations/llms/\n72. https://python.langchain.com/docs/use_cases/chatbots\n73. https://python.langchain.com/docs/integrations/providers/gpt4all\n74. https://python.langchain.com/docs/use_cases/more/agents/agent_simulations/characters\n75. https://python.langchain.com/docs/integrations/llms/replicate\n76. https://python.langchain.com/docs/integrations/providers/weaviate\n77. https://python.langchain.com/docs/integrations/toolkits/office365\n78. https://python.langchain.com/docs/integrations/document_loaders/ifixit\n79. https://python.langchain.com/docs/modules/memory/types/vectorstore_retriever_memory\n80. https://python.langchain.com/docs/integrations/document_loaders\n81. https://python.langchain.com/docs/integrations/llms/octoai\n82. https://python.langchain.com/docs/integrations/vectorstores/supabase\n83. https://python.langchain.com/docs/integrations/document_loaders/ifixit\n84. https://python.langchain.com/docs/modules/agents/how_to/sharedmemory_for_tools\n85. https://python.langchain.com/docs/use_cases/extraction\n86. https://python.langchain.com/docs/modules/agents/how_to/add_memory_openai_functions\n87. https://python.langchain.com/docs/integrations/toolkits/gmail\n88. https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/timescalevector_self_query\n89. https://python.langchain.com/docs/integrations/vectorstores\n90. https://python.langchain.com/docs/integrations/tools/searchapi\n91. https://python.langchain.com/docs/modules/data_connection/document_loaders/markdown\n92. https://python.langchain.com/docs/use_cases/question_answering/how_to/local_retrieval_qa\n93. https://python.langchain.com/docs/guides/safety/moderation\n94. https://python.langchain.com/docs/integrations/llms/\n95. https://python.langchain.com/docs/integrations/providers/searchapi\n96. https://python.langchain.com/docs/integrations/document_loaders/blackboard\n97. https://python.langchain.com/docs/integrations/retrievers\n98. https://python.langchain.com/docs/modules/data_connection/retrievers/contextual_compression/\n99. https://python.langchain.com/docs/integrations/providers/motherduck\n100. https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/split_by_token"
  },
  {
    "objectID": "notebooks/08_semantic_reranking.html#re-ranking-sem√°ntico-de-los-documentos-relevantes",
    "href": "notebooks/08_semantic_reranking.html#re-ranking-sem√°ntico-de-los-documentos-relevantes",
    "title": "Re-ranking sem√°ntico",
    "section": "Re-ranking sem√°ntico de los documentos relevantes",
    "text": "Re-ranking sem√°ntico de los documentos relevantes\nUna vez que hemos obtenido los documentos m√°s relevantes para nuestra consulta de b√∫squeda, podemos re-rankearlos usando un modelo de b√∫squeda sem√°ntica basado en word embeddings.\nEn este caso, utilizaremos Cohere Rerank para obtener los documentos m√°s relevantes para nuestra consulta de b√∫squeda, re-rankeando los documentos obtenidos por BM25.\nPara que Cohere Rerank funcione necesitar√°s una cuenta de Cohere y un API key. Puedes obtener tu API key aqu√≠.\n\nco = cohere.Client()\n\n\nreranked_hits = co.rerank(\n    query=\"How to integrate LCEL into my Retrieval augmented generation system with a keyword search retriever?\",\n    documents=[doc.page_content for doc in relevant_keyword_documents],\n    top_n=10,\n    model=\"rerank-multilingual-v2.0\",\n)\n\nprint(\"Reranked results:\")\nfor hit in reranked_hits:\n    print(relevant_keyword_documents[hit.index].metadata[\"source\"])\n\nReranked results:\nhttps://python.langchain.com/docs/expression_language/cookbook/\nhttps://python.langchain.com/docs/use_cases/question_answering/how_to/flare\nhttps://python.langchain.com/docs/use_cases/question_answering/how_to/chat_vector_db\nhttps://python.langchain.com/docs/modules/data_connection/\nhttps://python.langchain.com/docs/modules/agents/agent_types/chat_conversation_agent\nhttps://python.langchain.com/docs/integrations/vectorstores/neo4jvector\nhttps://python.langchain.com/docs/use_cases/question_answering/how_to/multi_retrieval_qa_router\nhttps://python.langchain.com/docs/integrations/memory/remembrall\nhttps://python.langchain.com/docs/integrations/retrievers\nhttps://python.langchain.com/docs/modules/data_connection/retrievers/web_research"
  },
  {
    "objectID": "notebooks/02_context_aware_text_splitting.html#librer√≠as",
    "href": "notebooks/02_context_aware_text_splitting.html#librer√≠as",
    "title": "Fragmentaci√≥n de texto con base en el contexto",
    "section": "Librer√≠as",
    "text": "Librer√≠as\n\nfrom functools import partial\n\nimport pandas as pd\nimport seaborn as sns\nimport tiktoken\nfrom langchain.schema import Document\nfrom langchain.text_splitter import (\n    Language,\n    MarkdownHeaderTextSplitter,\n    RecursiveCharacterTextSplitter,\n)\n\nfrom src.langchain_docs_loader import LangchainDocsLoader\n\nUtilizaremos la funci√≥n print_example_doc_splits_from_docs a trav√©s de este notebook para imprimir el contenido de los documentos cuya metadata source coinsida con el valor proporcionado.\n\ndef print_example_doc_splits_from_docs(\n    docs: list[Document],\n    source: str,\n) -&gt; None:\n    for doc in docs:\n        if doc.metadata[\"source\"] == source:\n            print(\"\\n\")\n            print(f\" {doc.metadata['source']} \".center(80, \"=\"))\n            print(\"\\n\")\n            print(doc.page_content)\n\n\nprint_split_for_lcle = partial(\n    print_example_doc_splits_from_docs,\n    source=\"https://python.langchain.com/docs/expression_language/interface\",\n)"
  },
  {
    "objectID": "notebooks/02_context_aware_text_splitting.html#carga-de-datos",
    "href": "notebooks/02_context_aware_text_splitting.html#carga-de-datos",
    "title": "Fragmentaci√≥n de texto con base en el contexto",
    "section": "Carga de datos",
    "text": "Carga de datos\nDe aqu√≠ en adelante, utilizaremos el conjunto de documentos extra√≠dos en el notebook 01_context_aware_text_extraction.ipynb.\n\nloader = LangchainDocsLoader(include_output_cells=True)\ndocs = loader.load()\nf\"Loaded {len(docs)} documents\"\n\n'Loaded 962 documents'"
  },
  {
    "objectID": "notebooks/02_context_aware_text_splitting.html#fragmentaci√≥n-de-texto-sin-tener-en-cuenta-el-contexto",
    "href": "notebooks/02_context_aware_text_splitting.html#fragmentaci√≥n-de-texto-sin-tener-en-cuenta-el-contexto",
    "title": "Fragmentaci√≥n de texto con base en el contexto",
    "section": "Fragmentaci√≥n de texto sin tener en cuenta el contexto",
    "text": "Fragmentaci√≥n de texto sin tener en cuenta el contexto\nLa forma m√°s sencilla de fragmentar texto es utilizando la funci√≥n split de Python. Esta funci√≥n recibe como par√°metro un caracter o cadena de caracteres que se utilizar√° como separador. Por ejemplo, para fragmentar un texto en oraciones, se puede utilizar el caracter . como separador.\nSin embargo, podemos ir un poco m√°s all√° y utilizar RecursiveCharacterTextSplitter() de langchain para dividir texto observando caracteres de forma recursiva. Esta herramienta intenta, de manera recursiva, dividir el texto por diferentes caracteres para encontrar uno que funcione, permitiendo as√≠ una fragmentaci√≥n de texto m√°s precisa y adaptable a diferentes contextos y formatos de texto, aunque no tenga en cuenta el contexto sem√°ntico del mismo.\n\ntext_splitter_without_context = RecursiveCharacterTextSplitter()\n\nsplitted_documents_without_context = text_splitter_without_context.split_documents(docs)\nprint_split_for_lcle(splitted_documents_without_context)\n\n\n\n======= https://python.langchain.com/docs/expression_language/interface ========\n\n\n# Interface\n\nIn an effort to make it as easy as possible to create custom chains, we've implemented a [\"Runnable\"](https://api.python.langchain.com/en/latest/schema/langchain.schema.runnable.Runnable.html#langchain.schema.runnable.Runnable) protocol that most components implement. This is a standard interface with a few different methods, which makes it easy to define custom chains as well as making it possible to invoke them in a standard way. The standard interface exposed includes:\n\n- `stream`: stream back chunks of the response\n- `invoke`: call the chain on an input\n- `batch`: call the chain on a list of inputs\n\nThese also have corresponding async methods:\n\n- `astream`: stream back chunks of the response async\n- `ainvoke`: call the chain on an input async\n- `abatch`: call the chain on a list of inputs async\n\nThe type of the input varies by component:\n\n| Component | Input Type |\n| ---- | ---- |\n| Prompt | Dictionary |\n| Retriever | Single string |\n| Model | Single string, list of chat messages or a PromptValue |\n\nThe output type also varies by component:\n\n| Component | Output Type |\n| ---- | ---- |\n| LLM | String |\n| ChatModel | ChatMessage |\n| Prompt | PromptValue |\n| Retriever | List of documents |\n\nLet's take a look at these methods! To do so, we'll create a super simple PromptTemplate + ChatModel chain.\n\n```python\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.chat_models import ChatOpenAI\n```\n\n&gt; **API Reference:**\n&gt; - [ChatPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.ChatPromptTemplate.html)\n&gt; - [ChatOpenAI](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html)\n\n```python\nmodel = ChatOpenAI()\n```\n\n```python\nprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n```\n\n```python\nchain = prompt | model\n```\n\n## Stream‚Äã\n\n```python\nfor s in chain.stream({\"topic\": \"bears\"}):\n    print(s.content, end=\"\", flush=True)\n```\n\n```text\n    Sure, here's a bear-themed joke for you:\n    \n    Why don't bears wear shoes?\n    \n    Because they have bear feet!\n```\n\n## Invoke‚Äã\n\n```python\nchain.invoke({\"topic\": \"bears\"})\n```\n\n```text\n    AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they already have bear feet!\", additional_kwargs={}, example=False)\n```\n\n## Batch‚Äã\n\n```python\nchain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])\n```\n\n```text\n    [AIMessage(content=\"Why don't bears ever wear shoes?\\n\\nBecause they have bear feet!\", additional_kwargs={}, example=False),\n     AIMessage(content=\"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\", additional_kwargs={}, example=False)]\n```\n\nYou can set the number of concurrent requests by using the `max_concurrency` parameter\n\n```python\nchain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}], config={\"max_concurrency\": 5})\n```\n\n```text\n    [AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\", additional_kwargs={}, example=False),\n     AIMessage(content=\"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\", additional_kwargs={}, example=False)]\n```\n\n## Async Stream‚Äã\n\n```python\nasync for s in chain.astream({\"topic\": \"bears\"}):\n    print(s.content, end=\"\", flush=True)\n```\n\n```text\n    Why don't bears wear shoes?\n    \n    Because they have bear feet!\n```\n\n## Async Invoke‚Äã\n\n```python\nawait chain.ainvoke({\"topic\": \"bears\"})\n```\n\n```text\n    AIMessage(content=\"Sure, here you go:\\n\\nWhy don't bears wear shoes?\\n\\nBecause they have bear feet!\", additional_kwargs={}, example=False)\n```\n\n## Async Batch‚Äã\n\n```python\nawait chain.abatch([{\"topic\": \"bears\"}])\n```\n\n```text\n    [AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\", additional_kwargs={}, example=False)]\n```\n\n## Parallelism‚Äã\n\nLet's take a look at how LangChain Expression Language support parralel requests as much as possible. For example, when using a RunnableMapping (often written as a dictionary) it executes each element in parralel.\n\n\n======= https://python.langchain.com/docs/expression_language/interface ========\n\n\n```python\nfrom langchain.schema.runnable import RunnableMap\nchain1 = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\nchain2 = ChatPromptTemplate.from_template(\"write a short (2 line) poem about {topic}\") | model\ncombined = RunnableMap({\n    \"joke\": chain1,\n    \"poem\": chain2,\n})\n```\n\n&gt; **API Reference:**\n&gt; - [RunnableMap](https://api.python.langchain.com/en/latest/schema/langchain.schema.runnable.base.RunnableMap.html)\n\n```python\nchain1.invoke({\"topic\": \"bears\"})\n```\n\n```text\n    CPU times: user 31.7 ms, sys: 8.59 ms, total: 40.3 ms\n    Wall time: 1.05 s\n\n    AIMessage(content=\"Why don't bears like fast food?\\n\\nBecause they can't catch it!\", additional_kwargs={}, example=False)\n```\n\n```python\nchain2.invoke({\"topic\": \"bears\"})\n```\n\n```text\n    CPU times: user 42.9 ms, sys: 10.2 ms, total: 53 ms\n    Wall time: 1.93 s\n\n    AIMessage(content=\"In forest's embrace, bears roam free,\\nSilent strength, nature's majesty.\", additional_kwargs={}, example=False)\n```\n\n```python\ncombined.invoke({\"topic\": \"bears\"})\n```\n\n```text\n    CPU times: user 96.3 ms, sys: 20.4 ms, total: 117 ms\n    Wall time: 1.1 s\n\n    {'joke': AIMessage(content=\"Why don't bears wear socks?\\n\\nBecause they have bear feet!\", additional_kwargs={}, example=False),\n     'poem': AIMessage(content=\"In forest's embrace,\\nMajestic bears leave their trace.\", additional_kwargs={}, example=False)}\n```"
  },
  {
    "objectID": "notebooks/02_context_aware_text_splitting.html#framentaci√≥n-de-texto-con-contexto-de-distrubuci√≥n-de-tokens",
    "href": "notebooks/02_context_aware_text_splitting.html#framentaci√≥n-de-texto-con-contexto-de-distrubuci√≥n-de-tokens",
    "title": "Fragmentaci√≥n de texto con base en el contexto",
    "section": "Framentaci√≥n de texto con contexto de distrubuci√≥n de tokens",
    "text": "Framentaci√≥n de texto con contexto de distrubuci√≥n de tokens\nEn muchas ocasiones, el contexto de c√≥mo se distribuyen los tokens o caracteres en el texto puede ser de gran ayuda para decidir c√≥mo fragmentar el texto. Ve√°moslo con un ejemplo.\n\nFunciones de apoyo\n\ndef num_tokens_from_string(string: str, encoding_name: str = \"cl100k_base\") -&gt; int:\n    \"\"\"Returns the number of tokens in a text string.\"\"\"\n    encoding = tiktoken.get_encoding(encoding_name)\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n\n\ndef num_tokens_from_document(\n    document: Document, encoding_name: str = \"cl100k_base\"\n) -&gt; int:\n    \"\"\"Returns the number of tokens in a document.\"\"\"\n    return num_tokens_from_string(document.page_content, encoding_name)\n\n\n\nEstad√≠sticas de tokens en los textos\nCalculemos algunas estad√≠sticas de los tokens en los textos utilizado pandas.\n\ntokens_per_document = pd.Series([num_tokens_from_document(doc) for doc in docs])\ntokens_per_document.describe()\n\ncount      962.000000\nmean      1599.195426\nstd       3419.855171\nmin         42.000000\n25%        382.250000\n50%        809.500000\n75%       1646.750000\nmax      80983.000000\ndtype: float64\n\n\nLa amplia variabilidad en el n√∫mero de tokens por documento sugiere que se est√° tratando con documentos de longitudes muy diversas, desde muy cortos hasta muy largos. Esto podr√≠a afectar a los an√°lisis subsiguientes y deber√≠a tenerse en cuenta al desarrollar modelos de procesamiento de lenguaje natural, ajustando posiblemente los m√©todos de preprocesamiento o utilizando t√©cnicas que puedan manejar eficientemente documentos de diferentes longitudes. Adem√°s, el sesgo a la derecha en la distribuci√≥n sugiere que aunque la mayor√≠a de los documentos son relativamente cortos, hay algunos documentos extremadamente largos que podr√≠an ser at√≠picos y necesitar un tratamiento especial.\n\n\nVisualizaci√≥n de distribuci√≥n de tokens sin outliers\n\n# Calculate Q1, Q3, and IQR\nQ1 = tokens_per_document.quantile(0.25)\nQ3 = tokens_per_document.quantile(0.75)\nIQR = Q3 - Q1\n\nupper_bound = Q3 + 1.5 * IQR\n\n# We only filter outliers by upper bound since we don't have problems with short documents.\nfiltered_tokens = tokens_per_document[(tokens_per_document &lt;= upper_bound)]\n\n# Plot the important sections of the histogram\nsns.set_theme(style=\"whitegrid\")\nfig = sns.histplot(filtered_tokens, kde=True)\nfig.set(\n    xlabel=\"Number of tokens\",\n    ylabel=\"Number of documents\",\n    title=\"Number of tokens per document\",\n)\n\n[Text(0.5, 0, 'Number of tokens'),\n Text(0, 0.5, 'Number of documents'),\n Text(0.5, 1.0, 'Number of tokens per document')]\n\n\n\n\n\n\n\nFramentaci√≥n de texto con base en la distribuci√≥n de tokens\nConsiderando los resultados anteriores, podemos utilizar la informaci√≥n de la distribuci√≥n de tokens para fragmentar el texto de forma m√°s precisa. Para ello, utilizaremos la funci√≥n RecursiveCharacterTextSplitter() de langchain, pero ahora especif√≠caremos los par√°metros chunk_size y chunk_overlap.\n\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=50,\n    length_function=num_tokens_from_string,\n)\n\nsplitted_with_little_context = splitter.split_documents(docs)\nprint_split_for_lcle(splitted_with_little_context)\n\n\n\n======= https://python.langchain.com/docs/expression_language/interface ========\n\n\n# Interface\n\nIn an effort to make it as easy as possible to create custom chains, we've implemented a [\"Runnable\"](https://api.python.langchain.com/en/latest/schema/langchain.schema.runnable.Runnable.html#langchain.schema.runnable.Runnable) protocol that most components implement. This is a standard interface with a few different methods, which makes it easy to define custom chains as well as making it possible to invoke them in a standard way. The standard interface exposed includes:\n\n- `stream`: stream back chunks of the response\n- `invoke`: call the chain on an input\n- `batch`: call the chain on a list of inputs\n\nThese also have corresponding async methods:\n\n- `astream`: stream back chunks of the response async\n- `ainvoke`: call the chain on an input async\n- `abatch`: call the chain on a list of inputs async\n\nThe type of the input varies by component:\n\n| Component | Input Type |\n| ---- | ---- |\n| Prompt | Dictionary |\n| Retriever | Single string |\n| Model | Single string, list of chat messages or a PromptValue |\n\nThe output type also varies by component:\n\n| Component | Output Type |\n| ---- | ---- |\n| LLM | String |\n| ChatModel | ChatMessage |\n| Prompt | PromptValue |\n| Retriever | List of documents |\n\nLet's take a look at these methods! To do so, we'll create a super simple PromptTemplate + ChatModel chain.\n\n```python\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.chat_models import ChatOpenAI\n```\n\n&gt; **API Reference:**\n&gt; - [ChatPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.ChatPromptTemplate.html)\n&gt; - [ChatOpenAI](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html)\n\n```python\nmodel = ChatOpenAI()\n```\n\n```python\nprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n```\n\n```python\nchain = prompt | model\n```\n\n## Stream‚Äã\n\n```python\nfor s in chain.stream({\"topic\": \"bears\"}):\n    print(s.content, end=\"\", flush=True)\n```\n\n```text\n    Sure, here's a bear-themed joke for you:\n    \n    Why don't bears wear shoes?\n    \n    Because they have bear feet!\n```\n\n## Invoke‚Äã\n\n```python\nchain.invoke({\"topic\": \"bears\"})\n```\n\n```text\n    AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they already have bear feet!\", additional_kwargs={}, example=False)\n```\n\n## Batch‚Äã\n\n```python\nchain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])\n```\n\n```text\n    [AIMessage(content=\"Why don't bears ever wear shoes?\\n\\nBecause they have bear feet!\", additional_kwargs={}, example=False),\n     AIMessage(content=\"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\", additional_kwargs={}, example=False)]\n```\n\nYou can set the number of concurrent requests by using the `max_concurrency` parameter\n\n```python\nchain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}], config={\"max_concurrency\": 5})\n```\n\n```text\n    [AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\", additional_kwargs={}, example=False),\n     AIMessage(content=\"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\", additional_kwargs={}, example=False)]\n```\n\n## Async Stream‚Äã\n\n```python\nasync for s in chain.astream({\"topic\": \"bears\"}):\n    print(s.content, end=\"\", flush=True)\n```\n\n```text\n    Why don't bears wear shoes?\n    \n    Because they have bear feet!\n```\n\n## Async Invoke‚Äã\n\n```python\nawait chain.ainvoke({\"topic\": \"bears\"})\n```\n\n```text\n    AIMessage(content=\"Sure, here you go:\\n\\nWhy don't bears wear shoes?\\n\\nBecause they have bear feet!\", additional_kwargs={}, example=False)\n```\n\n## Async Batch‚Äã\n\n```python\nawait chain.abatch([{\"topic\": \"bears\"}])\n```\n\n```text\n    [AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\", additional_kwargs={}, example=False)]\n```\n\n## Parallelism‚Äã\n\nLet's take a look at how LangChain Expression Language support parralel requests as much as possible. For example, when using a RunnableMapping (often written as a dictionary) it executes each element in parralel.\n\n\n======= https://python.langchain.com/docs/expression_language/interface ========\n\n\nLet's take a look at how LangChain Expression Language support parralel requests as much as possible. For example, when using a RunnableMapping (often written as a dictionary) it executes each element in parralel.\n\n```python\nfrom langchain.schema.runnable import RunnableMap\nchain1 = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\nchain2 = ChatPromptTemplate.from_template(\"write a short (2 line) poem about {topic}\") | model\ncombined = RunnableMap({\n    \"joke\": chain1,\n    \"poem\": chain2,\n})\n```\n\n&gt; **API Reference:**\n&gt; - [RunnableMap](https://api.python.langchain.com/en/latest/schema/langchain.schema.runnable.base.RunnableMap.html)\n\n```python\nchain1.invoke({\"topic\": \"bears\"})\n```\n\n```text\n    CPU times: user 31.7 ms, sys: 8.59 ms, total: 40.3 ms\n    Wall time: 1.05 s\n\n    AIMessage(content=\"Why don't bears like fast food?\\n\\nBecause they can't catch it!\", additional_kwargs={}, example=False)\n```\n\n```python\nchain2.invoke({\"topic\": \"bears\"})\n```\n\n```text\n    CPU times: user 42.9 ms, sys: 10.2 ms, total: 53 ms\n    Wall time: 1.93 s\n\n    AIMessage(content=\"In forest's embrace, bears roam free,\\nSilent strength, nature's majesty.\", additional_kwargs={}, example=False)\n```\n\n```python\ncombined.invoke({\"topic\": \"bears\"})\n```\n\n```text\n    CPU times: user 96.3 ms, sys: 20.4 ms, total: 117 ms\n    Wall time: 1.1 s\n\n    {'joke': AIMessage(content=\"Why don't bears wear socks?\\n\\nBecause they have bear feet!\", additional_kwargs={}, example=False),\n     'poem': AIMessage(content=\"In forest's embrace,\\nMajestic bears leave their trace.\", additional_kwargs={}, example=False)}\n```"
  },
  {
    "objectID": "notebooks/02_context_aware_text_splitting.html#framentaci√≥n-de-texto-con-contexto",
    "href": "notebooks/02_context_aware_text_splitting.html#framentaci√≥n-de-texto-con-contexto",
    "title": "Fragmentaci√≥n de texto con base en el contexto",
    "section": "Framentaci√≥n de texto con contexto",
    "text": "Framentaci√≥n de texto con contexto\nCon el dominio del problema podemos fragmentar el texto de manera m√°s precisa.\n\nFramentaci√≥n de texto con base en una especificaci√≥n de lenguaje como contexto\nEn nuestro ejemplo, nuestros documentos son Markdown, por lo que podr√≠amos fragmentar el documento en funci√≥n de los caracteres que se utilizan para definir los encabezados de las secciones y otros elementos de formato.\nEn este caso, la funci√≥n internamente utiliza los siguientes patrones para fragmentar el texto:\n[\n    # First, try to split along Markdown headings (starting with level 2)\n    \"\\n#{1,6} \",\n    # Note the alternative syntax for headings (below) is not handled here\n    # Heading level 2\n    # ---------------\n    # End of code block\n    \"```\\n\",\n    # Horizontal lines\n    \"\\n\\\\*\\\\*\\\\*+\\n\",\n    \"\\n---+\\n\",\n    \"\\n___+\\n\",\n    # Note that this splitter doesn't handle horizontal lines defined\n    # by *three or more* of ***, ---, or ___, but this is not handled\n    \"\\n\\n\",\n    \"\\n\",\n    \" \",\n    \"\",\n]\n\nmd_language_splitter = RecursiveCharacterTextSplitter.from_language(\n    language=Language.MARKDOWN,\n    chunk_size=1000,\n    chunk_overlap=50,\n    length_function=num_tokens_from_string,\n)\nmd_language_splits = md_language_splitter.split_documents(docs)\nprint_split_for_lcle(md_language_splits)\n\n\n\n======= https://python.langchain.com/docs/expression_language/interface ========\n\n\n# Interface\n\nIn an effort to make it as easy as possible to create custom chains, we've implemented a [\"Runnable\"](https://api.python.langchain.com/en/latest/schema/langchain.schema.runnable.Runnable.html#langchain.schema.runnable.Runnable) protocol that most components implement. This is a standard interface with a few different methods, which makes it easy to define custom chains as well as making it possible to invoke them in a standard way. The standard interface exposed includes:\n\n- `stream`: stream back chunks of the response\n- `invoke`: call the chain on an input\n- `batch`: call the chain on a list of inputs\n\nThese also have corresponding async methods:\n\n- `astream`: stream back chunks of the response async\n- `ainvoke`: call the chain on an input async\n- `abatch`: call the chain on a list of inputs async\n\nThe type of the input varies by component:\n\n| Component | Input Type |\n| ---- | ---- |\n| Prompt | Dictionary |\n| Retriever | Single string |\n| Model | Single string, list of chat messages or a PromptValue |\n\nThe output type also varies by component:\n\n| Component | Output Type |\n| ---- | ---- |\n| LLM | String |\n| ChatModel | ChatMessage |\n| Prompt | PromptValue |\n| Retriever | List of documents |\n\nLet's take a look at these methods! To do so, we'll create a super simple PromptTemplate + ChatModel chain.\n\n```python\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.chat_models import ChatOpenAI\n```\n\n&gt; **API Reference:**\n&gt; - [ChatPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.ChatPromptTemplate.html)\n&gt; - [ChatOpenAI](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html)\n\n```python\nmodel = ChatOpenAI()\n```\n\n```python\nprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n```\n\n```python\nchain = prompt | model\n```\n\n## Stream‚Äã\n\n```python\nfor s in chain.stream({\"topic\": \"bears\"}):\n    print(s.content, end=\"\", flush=True)\n```\n\n```text\n    Sure, here's a bear-themed joke for you:\n    \n    Why don't bears wear shoes?\n    \n    Because they have bear feet!\n```\n\n## Invoke‚Äã\n\n```python\nchain.invoke({\"topic\": \"bears\"})\n```\n\n```text\n    AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they already have bear feet!\", additional_kwargs={}, example=False)\n```\n\n## Batch‚Äã\n\n```python\nchain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])\n```\n\n```text\n    [AIMessage(content=\"Why don't bears ever wear shoes?\\n\\nBecause they have bear feet!\", additional_kwargs={}, example=False),\n     AIMessage(content=\"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\", additional_kwargs={}, example=False)]\n```\n\nYou can set the number of concurrent requests by using the `max_concurrency` parameter\n\n```python\nchain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}], config={\"max_concurrency\": 5})\n```\n\n```text\n    [AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\", additional_kwargs={}, example=False),\n     AIMessage(content=\"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\", additional_kwargs={}, example=False)]\n```\n\n## Async Stream‚Äã\n\n```python\nasync for s in chain.astream({\"topic\": \"bears\"}):\n    print(s.content, end=\"\", flush=True)\n```\n\n```text\n    Why don't bears wear shoes?\n    \n    Because they have bear feet!\n```\n\n## Async Invoke‚Äã\n\n```python\nawait chain.ainvoke({\"topic\": \"bears\"})\n```\n\n```text\n    AIMessage(content=\"Sure, here you go:\\n\\nWhy don't bears wear shoes?\\n\\nBecause they have bear feet!\", additional_kwargs={}, example=False)\n```\n\n## Async Batch‚Äã\n\n```python\nawait chain.abatch([{\"topic\": \"bears\"}])\n```\n\n```text\n    [AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\", additional_kwargs={}, example=False)]\n```\n\n\n======= https://python.langchain.com/docs/expression_language/interface ========\n\n\n## Parallelism‚Äã\n\nLet's take a look at how LangChain Expression Language support parralel requests as much as possible. For example, when using a RunnableMapping (often written as a dictionary) it executes each element in parralel.\n\n```python\nfrom langchain.schema.runnable import RunnableMap\nchain1 = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\nchain2 = ChatPromptTemplate.from_template(\"write a short (2 line) poem about {topic}\") | model\ncombined = RunnableMap({\n    \"joke\": chain1,\n    \"poem\": chain2,\n})\n```\n\n&gt; **API Reference:**\n&gt; - [RunnableMap](https://api.python.langchain.com/en/latest/schema/langchain.schema.runnable.base.RunnableMap.html)\n\n```python\nchain1.invoke({\"topic\": \"bears\"})\n```\n\n```text\n    CPU times: user 31.7 ms, sys: 8.59 ms, total: 40.3 ms\n    Wall time: 1.05 s\n\n    AIMessage(content=\"Why don't bears like fast food?\\n\\nBecause they can't catch it!\", additional_kwargs={}, example=False)\n```\n\n```python\nchain2.invoke({\"topic\": \"bears\"})\n```\n\n```text\n    CPU times: user 42.9 ms, sys: 10.2 ms, total: 53 ms\n    Wall time: 1.93 s\n\n    AIMessage(content=\"In forest's embrace, bears roam free,\\nSilent strength, nature's majesty.\", additional_kwargs={}, example=False)\n```\n\n```python\ncombined.invoke({\"topic\": \"bears\"})\n```\n\n```text\n    CPU times: user 96.3 ms, sys: 20.4 ms, total: 117 ms\n    Wall time: 1.1 s\n\n    {'joke': AIMessage(content=\"Why don't bears wear socks?\\n\\nBecause they have bear feet!\", additional_kwargs={}, example=False),\n     'poem': AIMessage(content=\"In forest's embrace,\\nMajestic bears leave their trace.\", additional_kwargs={}, example=False)}\n```\n\n\n\n\nFramentaci√≥n de texto utilizando encabezados como contexto\nEn contrastraste con el ejemplo anterior, en este caso utilizaremos √∫nicamente los encabezados de los documentos como contexto para fragmentar el texto. Estos encabezados pasar√°n a formar parte de los meta-datos de los fragmentos.\nDentro de cada framento de encabezado, podr√≠amos repetir el proceso de fragmentaci√≥n de texto con base en la distribuci√≥n de tokens o en una especificaci√≥n de lenguaje como contexto.\n\nmd_headers_splits: list[Document] = []\n\nfor doc in docs:\n    md_header_splitter = MarkdownHeaderTextSplitter(\n        headers_to_split_on=[\n            (\"#\", \"Header 1\"),\n            (\"##\", \"Header 2\"),\n        ]\n    )\n    text_splitter = RecursiveCharacterTextSplitter.from_language(\n        language=Language.MARKDOWN,\n        chunk_size=1000,  # try then with 150\n        chunk_overlap=50,\n        length_function=num_tokens_from_string,\n    )\n\n    splits = md_header_splitter.split_text(doc.page_content)\n\n    splits = text_splitter.split_documents(splits)\n    splits = [\n        Document(\n            page_content=split.page_content,\n            metadata={\n                **split.metadata,\n                **doc.metadata,\n            },\n        )\n        for split in splits\n    ]\n    md_headers_splits.extend(splits)\n\nprint_split_for_lcle(md_headers_splits)\n\n\n\n======= https://python.langchain.com/docs/expression_language/interface ========\n\n\nIn an effort to make it as easy as possible to create custom chains, we've implemented a [\"Runnable\"](https://api.python.langchain.com/en/latest/schema/langchain.schema.runnable.Runnable.html#langchain.schema.runnable.Runnable) protocol that most components implement. This is a standard interface with a few different methods, which makes it easy to define custom chains as well as making it possible to invoke them in a standard way. The standard interface exposed includes:  \n- `stream`: stream back chunks of the response\n- `invoke`: call the chain on an input\n- `batch`: call the chain on a list of inputs  \nThese also have corresponding async methods:  \n- `astream`: stream back chunks of the response async\n- `ainvoke`: call the chain on an input async\n- `abatch`: call the chain on a list of inputs async  \nThe type of the input varies by component:  \n| Component | Input Type |\n| ---- | ---- |\n| Prompt | Dictionary |\n| Retriever | Single string |\n| Model | Single string, list of chat messages or a PromptValue |  \nThe output type also varies by component:  \n| Component | Output Type |\n| ---- | ---- |\n| LLM | String |\n| ChatModel | ChatMessage |\n| Prompt | PromptValue |\n| Retriever | List of documents |  \nLet's take a look at these methods! To do so, we'll create a super simple PromptTemplate + ChatModel chain.  \n```python\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.chat_models import ChatOpenAI\n```  \n&gt; **API Reference:**\n&gt; - [ChatPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.ChatPromptTemplate.html)\n&gt; - [ChatOpenAI](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html)  \n```python\nmodel = ChatOpenAI()\n```  \n```python\nprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n```  \n```python\nchain = prompt | model\n```\n\n\n======= https://python.langchain.com/docs/expression_language/interface ========\n\n\n```python\nfor s in chain.stream({\"topic\": \"bears\"}):\nprint(s.content, end=\"\", flush=True)\n```  \n```text\nSure, here's a bear-themed joke for you:  \nWhy don't bears wear shoes?  \nBecause they have bear feet!\n```\n\n\n======= https://python.langchain.com/docs/expression_language/interface ========\n\n\n```python\nchain.invoke({\"topic\": \"bears\"})\n```  \n```text\nAIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they already have bear feet!\", additional_kwargs={}, example=False)\n```\n\n\n======= https://python.langchain.com/docs/expression_language/interface ========\n\n\n```python\nchain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])\n```  \n```text\n[AIMessage(content=\"Why don't bears ever wear shoes?\\n\\nBecause they have bear feet!\", additional_kwargs={}, example=False),\nAIMessage(content=\"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\", additional_kwargs={}, example=False)]\n```  \nYou can set the number of concurrent requests by using the `max_concurrency` parameter  \n```python\nchain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}], config={\"max_concurrency\": 5})\n```  \n```text\n[AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\", additional_kwargs={}, example=False),\nAIMessage(content=\"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\", additional_kwargs={}, example=False)]\n```\n\n\n======= https://python.langchain.com/docs/expression_language/interface ========\n\n\n```python\nasync for s in chain.astream({\"topic\": \"bears\"}):\nprint(s.content, end=\"\", flush=True)\n```  \n```text\nWhy don't bears wear shoes?  \nBecause they have bear feet!\n```\n\n\n======= https://python.langchain.com/docs/expression_language/interface ========\n\n\n```python\nawait chain.ainvoke({\"topic\": \"bears\"})\n```  \n```text\nAIMessage(content=\"Sure, here you go:\\n\\nWhy don't bears wear shoes?\\n\\nBecause they have bear feet!\", additional_kwargs={}, example=False)\n```\n\n\n======= https://python.langchain.com/docs/expression_language/interface ========\n\n\n```python\nawait chain.abatch([{\"topic\": \"bears\"}])\n```  \n```text\n[AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\", additional_kwargs={}, example=False)]\n```\n\n\n======= https://python.langchain.com/docs/expression_language/interface ========\n\n\nLet's take a look at how LangChain Expression Language support parralel requests as much as possible. For example, when using a RunnableMapping (often written as a dictionary) it executes each element in parralel.  \n```python\nfrom langchain.schema.runnable import RunnableMap\nchain1 = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\nchain2 = ChatPromptTemplate.from_template(\"write a short (2 line) poem about {topic}\") | model\ncombined = RunnableMap({\n\"joke\": chain1,\n\"poem\": chain2,\n})\n```  \n&gt; **API Reference:**\n&gt; - [RunnableMap](https://api.python.langchain.com/en/latest/schema/langchain.schema.runnable.base.RunnableMap.html)  \n```python\nchain1.invoke({\"topic\": \"bears\"})\n```  \n```text\nCPU times: user 31.7 ms, sys: 8.59 ms, total: 40.3 ms\nWall time: 1.05 s  \nAIMessage(content=\"Why don't bears like fast food?\\n\\nBecause they can't catch it!\", additional_kwargs={}, example=False)\n```  \n```python\nchain2.invoke({\"topic\": \"bears\"})\n```  \n```text\nCPU times: user 42.9 ms, sys: 10.2 ms, total: 53 ms\nWall time: 1.93 s  \nAIMessage(content=\"In forest's embrace, bears roam free,\\nSilent strength, nature's majesty.\", additional_kwargs={}, example=False)\n```  \n```python\ncombined.invoke({\"topic\": \"bears\"})\n```  \n```text\nCPU times: user 96.3 ms, sys: 20.4 ms, total: 117 ms\nWall time: 1.1 s  \n{'joke': AIMessage(content=\"Why don't bears wear socks?\\n\\nBecause they have bear feet!\", additional_kwargs={}, example=False),\n'poem': AIMessage(content=\"In forest's embrace,\\nMajestic bears leave their trace.\", additional_kwargs={}, example=False)}\n```"
  },
  {
    "objectID": "notebooks/04_parent_retrievers.html#librer√≠as",
    "href": "notebooks/04_parent_retrievers.html#librer√≠as",
    "title": "Parent retrievers",
    "section": "Librer√≠as",
    "text": "Librer√≠as\n\nfrom functools import partial\n\nfrom dotenv import load_dotenv\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.text_splitter import Language, RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import Chroma\n\nfrom src.langchain_docs_loader import LangchainDocsLoader, num_tokens_from_string\n\nload_dotenv()\n\nTrue"
  },
  {
    "objectID": "notebooks/04_parent_retrievers.html#funciones-de-utilidad",
    "href": "notebooks/04_parent_retrievers.html#funciones-de-utilidad",
    "title": "Parent retrievers",
    "section": "Funciones de utilidad",
    "text": "Funciones de utilidad\n\nget_vectorstore = partial(\n    Chroma,\n    embedding_function=OpenAIEmbeddings(),\n)"
  },
  {
    "objectID": "notebooks/04_parent_retrievers.html#carga-de-datos",
    "href": "notebooks/04_parent_retrievers.html#carga-de-datos",
    "title": "Parent retrievers",
    "section": "Carga de datos",
    "text": "Carga de datos\n\nloader = LangchainDocsLoader()\ndocs = loader.load()\nlen(docs)\n\n962"
  },
  {
    "objectID": "notebooks/04_parent_retrievers.html#recuperaci√≥n-de-los-documentos-completos",
    "href": "notebooks/04_parent_retrievers.html#recuperaci√≥n-de-los-documentos-completos",
    "title": "Parent retrievers",
    "section": "Recuperaci√≥n de los documentos completos",
    "text": "Recuperaci√≥n de los documentos completos\n\nchild_splitter = RecursiveCharacterTextSplitter.from_language(\n    language=Language.MARKDOWN,\n    chunk_size=100,\n    chunk_overlap=10,\n    length_function=num_tokens_from_string,\n)\n\nvectorstore = get_vectorstore(collection_name=\"full_documents\")\n\nstore = InMemoryStore()\n\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n)\n\nretriever.add_documents(docs)\n\nLa cantidad de documentos en nuestra Store es igual a la cantidad de documentos en nuestro dataset.\n\nlen(list(store.yield_keys()))\n\n962\n\n\nAl buscar documentos directamente en la VectorStore, obtendr√°s fragmentos de documentos que fueron procesados por el TextSplitter.\n\nfull_documents_similarity = vectorstore.similarity_search(\n    \"Does the MultiQueryRetriever might be able to overcome some of the limitations of...?\"\n)\nfull_documents_similarity\n\n[Document(page_content='The `MultiQueryRetriever` automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set', metadata={'description': 'Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on \"distance\". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious.', 'doc_id': '82fe97d1-8cd4-4ce5-9cd6-f103ecabfcaa', 'language': 'en', 'source': 'https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever', 'title': 'MultiQueryRetriever | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='The `MergerRetriever` class can be used to improve the accuracy of document retrieval in a number of ways. First, it can combine the results of multiple retrievers, which can help to reduce the risk of bias in the results. Second, it can rank the results of the different retrievers, which can help to ensure that the most relevant documents are returned first.', metadata={'description': 'Lord of the Retrievers, also known as MergerRetriever, takes a list of retrievers as input and merges the results of their getrelevantdocuments() methods into a single list. The merged results will be a list of documents that are relevant to the query and that have been ranked by the different retrievers.', 'doc_id': '2d1e6455-6e96-40f0-bb64-c1bf62bb3cf9', 'language': 'en', 'source': 'https://python.langchain.com/docs/integrations/retrievers/merger_retriever', 'title': 'LOTR (Merger Retriever) | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='- `MultiQueryRetriever` [generates variants of the input question](/docs/modules/data_connection/retrievers/MultiQueryRetriever) to improve retrieval.\\n- `Max marginal relevance` selects for [relevance and diversity](https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf) among the retrieved documents.', metadata={'description': 'Open In Collab', 'doc_id': '03786673-b072-4e78-ab4a-31a187cc8292', 'language': 'en', 'source': 'https://python.langchain.com/docs/use_cases/question_answering/', 'title': 'Question Answering | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='- [Ensemble Retriever](/docs/modules/data_connection/retrievers/ensemble): Sometimes you may want to retrieve documents from multiple different sources, or using multiple different algorithms. The ensemble retriever allows you to easily do this.\\n- And more!', metadata={'description': \"Many LLM applications require user-specific data that is not part of the model's training set.\", 'doc_id': '41af301d-2cc9-4cda-a677-378e440c6ebb', 'language': 'en', 'source': 'https://python.langchain.com/docs/modules/data_connection/', 'title': 'Retrieval | ü¶úÔ∏èüîó Langchain'})]\n\n\nSi ahora realiza una b√∫squeda en el ParentDocumentRetriever, obtendr√°s los documentos completos. Esto se debe a que el ParentDocumentRetriever primero busca los fragmentos que hacen match con la query, despu√©s busca los documentos completos sin repeticiones y finalmente devuelve el resultado.\n\nfull_documents_retriever = retriever.get_relevant_documents(\n    \"Does the MultiQueryRetriever might be able to overcome some of the limitations of...?\"\n)\nfull_documents_retriever\n\n[Document(page_content='# MultiQueryRetriever\\n\\nDistance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on \"distance\". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious.\\n\\nThe `MultiQueryRetriever` automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results.\\n\\n```python\\n# Build a sample vectorDB\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.document_loaders import WebBaseLoader\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\n\\n# Load blog post\\nloader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\\ndata = loader.load()\\n\\n# Split\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\\nsplits = text_splitter.split_documents(data)\\n\\n# VectorDB\\nembedding = OpenAIEmbeddings()\\nvectordb = Chroma.from_documents(documents=splits, embedding=embedding)\\n```\\n\\n&gt; **API Reference:**\\n&gt; - [Chroma](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.chroma.Chroma.html)\\n&gt; - [WebBaseLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.web_base.WebBaseLoader.html)\\n&gt; - [OpenAIEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html)\\n&gt; - [RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html)\\n\\n#### Simple usage\\u200b\\n\\nSpecify the LLM to use for query generation, and the retriver will do the rest.\\n\\n```python\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\\n\\nquestion = \"What are the approaches to Task Decomposition?\"\\nllm = ChatOpenAI(temperature=0)\\nretriever_from_llm = MultiQueryRetriever.from_llm(\\n    retriever=vectordb.as_retriever(), llm=llm\\n)\\n```\\n\\n&gt; **API Reference:**\\n&gt; - [ChatOpenAI](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html)\\n&gt; - [MultiQueryRetriever](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.multi_query.MultiQueryRetriever.html)\\n\\n```python\\n# Set logging for the queries\\nimport logging\\n\\nlogging.basicConfig()\\nlogging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\\n```\\n\\n```python\\nunique_docs = retriever_from_llm.get_relevant_documents(query=question)\\nlen(unique_docs)\\n```\\n\\n```text\\n    INFO:langchain.retrievers.multi_query:Generated queries: [\\'1. How can Task Decomposition be approached?\\', \\'2. What are the different methods for Task Decomposition?\\', \\'3. What are the various approaches to decomposing tasks?\\']\\n\\n    5\\n```\\n\\n#### Supplying your own prompt\\u200b\\n\\nYou can also supply a prompt along with an output parser to split the results into a list of queries.\\n\\n```python\\nfrom typing import List\\nfrom langchain.chains import LLMChain\\nfrom pydantic import BaseModel, Field\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.output_parsers import PydanticOutputParser\\n\\n# Output parser will split the LLM result into a list of queries\\nclass LineList(BaseModel):\\n    # \"lines\" is the key (attribute name) of the parsed output\\n    lines: List[str] = Field(description=\"Lines of text\")\\n\\nclass LineListOutputParser(PydanticOutputParser):\\n    def __init__(self) -&gt; None:\\n        super().__init__(pydantic_object=LineList)\\n\\n    def parse(self, text: str) -&gt; LineList:\\n        lines = text.strip().split(\"\\\\n\")\\n        return LineList(lines=lines)\\n\\noutput_parser = LineListOutputParser()\\n\\nQUERY_PROMPT = PromptTemplate(\\n    input_variables=[\"question\"],\\n    template=\"\"\"You are an AI language model assistant. Your task is to generate five \\n    different versions of the given user question to retrieve relevant documents from a vector \\n    database. By generating multiple perspectives on the user question, your goal is to help\\n    the user overcome some of the limitations of the distance-based similarity search. \\n    Provide these alternative questions seperated by newlines.\\n    Original question: {question}\"\"\",\\n)\\nllm = ChatOpenAI(temperature=0)\\n\\n# Chain\\nllm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)\\n\\n# Other inputs\\nquestion = \"What are the approaches to Task Decomposition?\"\\n```\\n\\n&gt; **API Reference:**\\n&gt; - [LLMChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html)\\n&gt; - [PromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.prompt.PromptTemplate.html)\\n&gt; - [PydanticOutputParser](https://api.python.langchain.com/en/latest/output_parsers/langchain.output_parsers.pydantic.PydanticOutputParser.html)\\n\\n```python\\n# Run\\nretriever = MultiQueryRetriever(\\n    retriever=vectordb.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\"\\n)  # \"lines\" is the key (attribute name) of the parsed output\\n\\n# Results\\nunique_docs = retriever.get_relevant_documents(\\n    query=\"What does the course say about regression?\"\\n)\\nlen(unique_docs)\\n```\\n\\n```text\\n    INFO:langchain.retrievers.multi_query:Generated queries: [\"1. What is the course\\'s perspective on regression?\", \\'2. Can you provide information on regression as discussed in the course?\\', \\'3. How does the course cover the topic of regression?\\', \"4. What are the course\\'s teachings on regression?\", \\'5. In relation to the course, what is mentioned about regression?\\']\\n\\n    11\\n```', metadata={'source': 'https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever', 'title': 'MultiQueryRetriever | ü¶úÔ∏èüîó Langchain', 'description': 'Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on \"distance\". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious.', 'language': 'en'}),\n Document(page_content='# LOTR (Merger Retriever)\\n\\n`Lord of the Retrievers`, also known as `MergerRetriever`, takes a list of retrievers as input and merges the results of their get_relevant_documents() methods into a single list. The merged results will be a list of documents that are relevant to the query and that have been ranked by the different retrievers.\\n\\nThe `MergerRetriever` class can be used to improve the accuracy of document retrieval in a number of ways. First, it can combine the results of multiple retrievers, which can help to reduce the risk of bias in the results. Second, it can rank the results of the different retrievers, which can help to ensure that the most relevant documents are returned first.\\n\\n```python\\nimport os\\nimport chromadb\\nfrom langchain.retrievers.merger_retriever import MergerRetriever\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.document_transformers import (\\n    EmbeddingsRedundantFilter,\\n    EmbeddingsClusteringFilter,\\n)\\nfrom langchain.retrievers.document_compressors import DocumentCompressorPipeline\\nfrom langchain.retrievers import ContextualCompressionRetriever\\n\\n# Get 3 diff embeddings.\\nall_mini = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\\nmulti_qa_mini = HuggingFaceEmbeddings(model_name=\"multi-qa-MiniLM-L6-dot-v1\")\\nfilter_embeddings = OpenAIEmbeddings()\\n\\nABS_PATH = os.path.dirname(os.path.abspath(__file__))\\nDB_DIR = os.path.join(ABS_PATH, \"db\")\\n\\n# Instantiate 2 diff cromadb indexs, each one with a diff embedding.\\nclient_settings = chromadb.config.Settings(\\n    is_persistent=True,\\n    persist_directory=DB_DIR,\\n    anonymized_telemetry=False,\\n)\\ndb_all = Chroma(\\n    collection_name=\"project_store_all\",\\n    persist_directory=DB_DIR,\\n    client_settings=client_settings,\\n    embedding_function=all_mini,\\n)\\ndb_multi_qa = Chroma(\\n    collection_name=\"project_store_multi\",\\n    persist_directory=DB_DIR,\\n    client_settings=client_settings,\\n    embedding_function=multi_qa_mini,\\n)\\n\\n# Define 2 diff retrievers with 2 diff embeddings and diff search type.\\nretriever_all = db_all.as_retriever(\\n    search_type=\"similarity\", search_kwargs={\"k\": 5, \"include_metadata\": True}\\n)\\nretriever_multi_qa = db_multi_qa.as_retriever(\\n    search_type=\"mmr\", search_kwargs={\"k\": 5, \"include_metadata\": True}\\n)\\n\\n# The Lord of the Retrievers will hold the ouput of boths retrievers and can be used as any other\\n# retriever on different types of chains.\\nlotr = MergerRetriever(retrievers=[retriever_all, retriever_multi_qa])\\n```\\n\\n&gt; **API Reference:**\\n&gt; - [MergerRetriever](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.merger_retriever.MergerRetriever.html)\\n&gt; - [Chroma](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.chroma.Chroma.html)\\n&gt; - [HuggingFaceEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.huggingface.HuggingFaceEmbeddings.html)\\n&gt; - [OpenAIEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html)\\n&gt; - [EmbeddingsRedundantFilter](https://api.python.langchain.com/en/latest/document_transformers/langchain.document_transformers.embeddings_redundant_filter.EmbeddingsRedundantFilter.html)\\n&gt; - [EmbeddingsClusteringFilter](https://api.python.langchain.com/en/latest/document_transformers/langchain.document_transformers.embeddings_redundant_filter.EmbeddingsClusteringFilter.html)\\n&gt; - [DocumentCompressorPipeline](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.document_compressors.base.DocumentCompressorPipeline.html)\\n&gt; - [ContextualCompressionRetriever](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.contextual_compression.ContextualCompressionRetriever.html)\\n\\n## Remove redundant results from the merged retrievers.\\u200b\\n\\n```python\\n# We can remove redundant results from both retrievers using yet another embedding.\\n# Using multiples embeddings in diff steps could help reduce biases.\\nfilter = EmbeddingsRedundantFilter(embeddings=filter_embeddings)\\npipeline = DocumentCompressorPipeline(transformers=[filter])\\ncompression_retriever = ContextualCompressionRetriever(\\n    base_compressor=pipeline, base_retriever=lotr\\n)\\n```\\n\\n## Pick a representative sample of documents from the merged retrievers.\\u200b\\n\\n```python\\n# This filter will divide the documents vectors into clusters or \"centers\" of meaning.\\n# Then it will pick the closest document to that center for the final results.\\n# By default the result document will be ordered/grouped by clusters.\\nfilter_ordered_cluster = EmbeddingsClusteringFilter(\\n    embeddings=filter_embeddings,\\n    num_clusters=10,\\n    num_closest=1,\\n)\\n\\n# If you want the final document to be ordered by the original retriever scores\\n# you need to add the \"sorted\" parameter.\\nfilter_ordered_by_retriever = EmbeddingsClusteringFilter(\\n    embeddings=filter_embeddings,\\n    num_clusters=10,\\n    num_closest=1,\\n    sorted=True,\\n)\\n\\npipeline = DocumentCompressorPipeline(transformers=[filter_ordered_by_retriever])\\ncompression_retriever = ContextualCompressionRetriever(\\n    base_compressor=pipeline, base_retriever=lotr\\n)\\n```\\n\\n## Re-order results to avoid performance degradation.\\u200b\\n\\nNo matter the architecture of your model, there is a sustancial performance degradation when you include 10+ retrieved documents.\\nIn brief: When models must access relevant information  in the middle of long contexts, then tend to ignore the provided documents.\\nSee: [https://arxiv.org/abs//2307.03172](https://arxiv.org/abs//2307.03172)\\n\\n```python\\n# You can use an additional document transformer to reorder documents after removing redudance.\\nfrom langchain.document_transformers import LongContextReorder\\n\\nfilter = EmbeddingsRedundantFilter(embeddings=filter_embeddings)\\nreordering = LongContextReorder()\\npipeline = DocumentCompressorPipeline(transformers=[filter, reordering])\\ncompression_retriever_reordered = ContextualCompressionRetriever(\\n    base_compressor=pipeline, base_retriever=lotr\\n)\\n```\\n\\n&gt; **API Reference:**\\n&gt; - [LongContextReorder](https://api.python.langchain.com/en/latest/document_transformers/langchain.document_transformers.long_context_reorder.LongContextReorder.html)', metadata={'source': 'https://python.langchain.com/docs/integrations/retrievers/merger_retriever', 'title': 'LOTR (Merger Retriever) | ü¶úÔ∏èüîó Langchain', 'description': 'Lord of the Retrievers, also known as MergerRetriever, takes a list of retrievers as input and merges the results of their getrelevantdocuments() methods into a single list. The merged results will be a list of documents that are relevant to the query and that have been ranked by the different retrievers.', 'language': 'en'}),\n Document(page_content='# Question Answering\\n\\n[](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/extras/use_cases/question_answering/qa.ipynb)\\n\\n## Use case\\u200b\\n\\nSuppose you have some text documents (PDF, blog, Notion pages, etc.) and want to ask questions related to the contents of those documents. LLMs, given their proficiency in understanding text, are a great tool for this.\\n\\nIn this walkthrough we\\'ll go over how to build a question-answering over documents application using LLMs. Two very related use cases which we cover elsewhere are:\\n\\n- [QA over structured data](/docs/use_cases/qa_structured/sql) (e.g., SQL)\\n- [QA over code](/docs/use_cases/code_understanding) (e.g., Python)\\n\\n![intro.png](/assets/images/qa_intro-9b468dbffe1cbe7f0bd822b28648db9e.png)\\n\\n## Overview\\u200b\\n\\nThe pipeline for converting raw unstructured data into a QA chain looks like this:\\n\\n1. `Loading`: First we need to load our data. Unstructured data can be loaded from many sources. Use the [LangChain integration hub](https://integrations.langchain.com/) to browse the full set of loaders.\\nEach loader returns data as a LangChain [Document](/docs/components/schema/document).\\n\\n2. `Splitting`: [Text splitters](/docs/modules/data_connection/document_transformers/) break `Documents` into splits of specified size\\n\\n3. `Storage`: Storage (e.g., often a [vectorstore](/docs/modules/data_connection/vectorstores/)) will house [and often embed](https://www.pinecone.io/learn/vector-embeddings/) the splits\\n\\n4. `Retrieval`: The app retrieves splits from storage (e.g., often [with similar embeddings](https://www.pinecone.io/learn/k-nearest-neighbor/) to the input question)\\n\\n5. `Generation`: An [LLM](/docs/modules/model_io/models/llms/) produces an answer using a prompt that includes the question and the retrieved data\\n\\n6. `Conversation` (Extension): Hold a multi-turn conversation by adding [Memory](/docs/modules/memory/) to your QA chain.\\n\\n![flow.jpeg](/assets/images/qa_flow-9fbd91de9282eb806bda1c6db501ecec.jpeg)\\n\\n## Quickstart\\u200b\\n\\nTo give you a sneak preview, the above pipeline can be all be wrapped in a single object: `VectorstoreIndexCreator`. Suppose we want a QA app over this [blog post](https://lilianweng.github.io/posts/2023-06-23-agent/). We can create this in a few lines of code. First set environment variables and install packages:\\n\\n```python\\npip install openai chromadb\\n\\n# Set env var OPENAI_API_KEY or load from a .env file\\n# import dotenv\\n\\n# dotenv.load_dotenv()\\n```\\n\\n```python\\nfrom langchain.document_loaders import WebBaseLoader\\nfrom langchain.indexes import VectorstoreIndexCreator\\n\\nloader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\\nindex = VectorstoreIndexCreator().from_loaders([loader])\\n```\\n\\n&gt; **API Reference:**\\n&gt; - [WebBaseLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.web_base.WebBaseLoader.html)\\n&gt; - [VectorstoreIndexCreator](https://api.python.langchain.com/en/latest/indexes/langchain.indexes.vectorstore.VectorstoreIndexCreator.html)\\n\\n```python\\nindex.query(\"What is Task Decomposition?\")\\n```\\n\\n```text\\n    \\' Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done using LLM with simple prompting, task-specific instructions, or with human inputs. Tree of Thoughts (Yao et al. 2023) is an extension of Chain of Thought (Wei et al. 2022) which explores multiple reasoning possibilities at each step.\\'\\n```\\n\\nOk, but what\\'s going on under the hood, and how could we customize this for our specific use case? For that, let\\'s take a look at how we can construct this pipeline piece by piece.\\n\\n## Step 1. Load\\u200b\\n\\nSpecify a `DocumentLoader` to load in your unstructured data as `Documents`. A `Document` is a piece of text (the `page_content`) and associated metadata.\\n\\n```python\\nfrom langchain.document_loaders import WebBaseLoader\\n\\nloader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\\ndata = loader.load()\\n```\\n\\n&gt; **API Reference:**\\n&gt; - [WebBaseLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.web_base.WebBaseLoader.html)\\n\\n### Go deeper\\u200b\\n\\n- Browse the &gt; 120 data loader integrations [here](https://integrations.langchain.com/).\\n- See further documentation on loaders [here](/docs/modules/data_connection/document_loaders/).\\n\\n## Step 2. Split\\u200b\\n\\nSplit the `Document` into chunks for embedding and vector storage.\\n\\n```python\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\\nall_splits = text_splitter.split_documents(data)\\n```\\n\\n&gt; **API Reference:**\\n&gt; - [RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html)\\n\\n### Go deeper\\u200b\\n\\n- `DocumentSplitters` are just one type of the more generic `DocumentTransformers`, which can all be useful in this preprocessing step.\\n- See further documentation on transformers [here](/docs/modules/data_connection/document_transformers/).\\n- `Context-aware splitters` keep the location (\"context\") of each split in the original `Document`:- [Markdown files](/docs/use_cases/question_answering/how_to/document-context-aware-QA)\\n- [Code (py or js)](/docs/use_cases/question_answering/docs/integrations/document_loaders/source_code)\\n- [Documents](/docs/integrations/document_loaders/grobid)\\n\\n## Step 3. Store\\u200b\\n\\nTo be able to look up our document splits, we first need to store them where we can later look them up.\\nThe most common way to do this is to embed the contents of each document then store the embedding and document in a vector store, with the embedding being used to index the document.\\n\\n```python\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.vectorstores import Chroma\\n\\nvectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\\n```\\n\\n&gt; **API Reference:**\\n&gt; - [OpenAIEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html)\\n&gt; - [Chroma](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.chroma.Chroma.html)\\n\\n### Go deeper\\u200b\\n\\n- Browse the &gt; 40 vectorstores integrations [here](https://integrations.langchain.com/).\\n\\n- See further documentation on vectorstores [here](/docs/modules/data_connection/vectorstores/).\\n\\n- Browse the &gt; 30 text embedding integrations [here](https://integrations.langchain.com/).\\n\\n- See further documentation on embedding models [here](/docs/modules/data_connection/text_embedding/).\\n\\nHere are Steps 1-3:\\n\\n![lc.png](/assets/images/qa_data_load-70fac3ea6593b986613784dc056df21a.png)\\n\\n## Step 4. Retrieve\\u200b\\n\\nRetrieve relevant splits for any question using [similarity search](https://www.pinecone.io/learn/what-is-similarity-search/).\\n\\n```python\\nquestion = \"What are the approaches to Task Decomposition?\"\\ndocs = vectorstore.similarity_search(question)\\nlen(docs)\\n```\\n\\n```text\\n    4\\n```\\n\\n### Go deeper\\u200b\\n\\nVectorstores are commonly used for retrieval, but they are not the only option. For example, SVMs (see thread [here](https://twitter.com/karpathy/status/1647025230546886658?s=20)) can also be used.\\n\\nLangChain [has many retrievers](/docs/modules/data_connection/retrievers/) including, but not limited to, vectorstores. All retrievers implement a common method `get_relevant_documents()` (and its asynchronous variant `aget_relevant_documents()`).\\n\\n```python\\nfrom langchain.retrievers import SVMRetriever\\n\\nsvm_retriever = SVMRetriever.from_documents(all_splits,OpenAIEmbeddings())\\ndocs_svm=svm_retriever.get_relevant_documents(question)\\nlen(docs_svm)\\n```\\n\\n&gt; **API Reference:**\\n&gt; - [SVMRetriever](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.svm.SVMRetriever.html)\\n\\n```text\\n    4\\n```\\n\\nSome common ways to improve on vector similarity search include:\\n\\n- `MultiQueryRetriever` [generates variants of the input question](/docs/modules/data_connection/retrievers/MultiQueryRetriever) to improve retrieval.\\n- `Max marginal relevance` selects for [relevance and diversity](https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf) among the retrieved documents.\\n- Documents can be filtered during retrieval using [metadata filters](/docs/use_cases/question_answering/how_to/document-context-aware-QA).\\n\\n```python\\nimport logging\\n\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\\n\\nlogging.basicConfig()\\nlogging.getLogger(\\'langchain.retrievers.multi_query\\').setLevel(logging.INFO)\\n\\nretriever_from_llm = MultiQueryRetriever.from_llm(retriever=vectorstore.as_retriever(),\\n                                                  llm=ChatOpenAI(temperature=0))\\nunique_docs = retriever_from_llm.get_relevant_documents(query=question)\\nlen(unique_docs)\\n```\\n\\n&gt; **API Reference:**\\n&gt; - [ChatOpenAI](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html)\\n&gt; - [MultiQueryRetriever](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.multi_query.MultiQueryRetriever.html)\\n\\n## Step 5. Generate\\u200b\\n\\nDistill the retrieved documents into an answer using an LLM/Chat model (e.g., `gpt-3.5-turbo`) with `RetrievalQA` chain.\\n\\n```python\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.chat_models import ChatOpenAI\\n\\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\\nqa_chain = RetrievalQA.from_chain_type(llm,retriever=vectorstore.as_retriever())\\nqa_chain({\"query\": question})\\n```\\n\\n&gt; **API Reference:**\\n&gt; - [RetrievalQA](https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html)\\n&gt; - [ChatOpenAI](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html)\\n\\n```text\\n    {\\'query\\': \\'What are the approaches to Task Decomposition?\\',\\n     \\'result\\': \\'The approaches to task decomposition include:\\\\n\\\\n1. Simple prompting: This approach involves using simple prompts or questions to guide the agent in breaking down a task into smaller subgoals. For example, the agent can be prompted with \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" to facilitate task decomposition.\\\\n\\\\n2. Task-specific instructions: In this approach, task-specific instructions are provided to the agent to guide the decomposition process. For example, if the task is to write a novel, the agent can be instructed to \"Write a story outline\" as a step in the task decomposition.\\\\n\\\\n3. Human inputs: This approach involves incorporating human inputs in the task decomposition process. Humans can provide guidance, feedback, and assistance to the agent in breaking down complex tasks into manageable subgoals.\\\\n\\\\nThese approaches aim to enable efficient handling of complex tasks by breaking them down into smaller, more manageable subgoals.\\'}\\n```\\n\\nNote, you can pass in an `LLM` or a `ChatModel` (like we did here) to the `RetrievalQA` chain.\\n\\n### Go deeper\\u200b\\n\\n#### Choosing LLMs\\u200b\\n\\n- Browse the &gt; 55 LLM and chat model integrations [here](https://integrations.langchain.com/).\\n- See further documentation on LLMs and chat models [here](/docs/modules/model_io/models/).\\n- See a guide on local LLMS [here](/docs/modules/use_cases/question_answering/how_to/local_retrieval_qa).\\n\\n#### Customizing the prompt\\u200b\\n\\nThe prompt in `RetrievalQA` chain can be easily customized.\\n\\n```python\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.prompts import PromptTemplate\\n\\ntemplate = \"\"\"Use the following pieces of context to answer the question at the end. \\nIf you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer. \\nUse three sentences maximum and keep the answer as concise as possible. \\nAlways say \"thanks for asking!\" at the end of the answer. \\n{context}\\nQuestion: {question}\\nHelpful Answer:\"\"\"\\nQA_CHAIN_PROMPT = PromptTemplate.from_template(template)\\n\\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\\nqa_chain = RetrievalQA.from_chain_type(\\n    llm,\\n    retriever=vectorstore.as_retriever(),\\n    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\\n)\\nresult = qa_chain({\"query\": question})\\nresult[\"result\"]\\n```\\n\\n&gt; **API Reference:**\\n&gt; - [RetrievalQA](https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html)\\n&gt; - [PromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.prompt.PromptTemplate.html)\\n\\n```text\\n    \\'The approaches to Task Decomposition are (1) using simple prompting by LLM, (2) using task-specific instructions, and (3) incorporating human inputs. Thanks for asking!\\'\\n```\\n\\nWe can also store and fetch prompts from the LangChain prompt hub.\\n\\nThis will work with your [LangSmith API key](https://docs.smith.langchain.com/).\\n\\nFor example, see [here](https://smith.langchain.com/hub/rlm/rag-prompt) is a common prompt for RAG.\\n\\nWe can load this.\\n\\n```python\\npip install langchainhub\\n```\\n\\n```python\\n# RAG prompt\\nfrom langchain import hub\\nQA_CHAIN_PROMPT_HUB = hub.pull(\"rlm/rag-prompt\")\\n\\nqa_chain = RetrievalQA.from_chain_type(\\n    llm,\\n    retriever=vectorstore.as_retriever(),\\n    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT_HUB}\\n)\\nresult = qa_chain({\"query\": question})\\nresult[\"result\"]\\n```\\n\\n```text\\n    \\'The approaches to task decomposition include using LLM with simple prompting, task-specific instructions, and human inputs.\\'\\n```\\n\\n#### Return source documents\\u200b\\n\\nThe full set of retrieved documents used for answer distillation can be returned using `return_source_documents=True`.\\n\\n```python\\nfrom langchain.chains import RetrievalQA\\n\\nqa_chain = RetrievalQA.from_chain_type(llm,retriever=vectorstore.as_retriever(),\\n                                       return_source_documents=True)\\nresult = qa_chain({\"query\": question})\\nprint(len(result[\\'source_documents\\']))\\nresult[\\'source_documents\\'][0]\\n```\\n\\n&gt; **API Reference:**\\n&gt; - [RetrievalQA](https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html)\\n\\n```text\\n    4\\n\\n    Document(page_content=\\'Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\', metadata={\\'description\\': \\'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\', \\'language\\': \\'en\\', \\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'title\\': \"LLM Powered Autonomous Agents | Lil\\'Log\"})\\n```\\n\\n#### Return citations\\u200b\\n\\nAnswer citations can be returned using `RetrievalQAWithSourcesChain`.\\n\\n```python\\nfrom langchain.chains import RetrievalQAWithSourcesChain\\n\\nqa_chain = RetrievalQAWithSourcesChain.from_chain_type(llm,retriever=vectorstore.as_retriever())\\n\\nresult = qa_chain({\"question\": question})\\nresult\\n```\\n\\n&gt; **API Reference:**\\n&gt; - [RetrievalQAWithSourcesChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.qa_with_sources.retrieval.RetrievalQAWithSourcesChain.html)\\n\\n```text\\n    {\\'question\\': \\'What are the approaches to Task Decomposition?\\',\\n     \\'answer\\': \\'The approaches to Task Decomposition include:\\\\n1. Using LLM with simple prompting, such as providing steps or subgoals for achieving a task.\\\\n2. Using task-specific instructions, such as providing a specific instruction like \"Write a story outline\" for writing a novel.\\\\n3. Using human inputs to decompose the task.\\\\nAnother approach is the Tree of Thoughts, which extends the Chain of Thought (CoT) technique by exploring multiple reasoning possibilities at each step and generating multiple thoughts per step, creating a tree structure. The search process can be BFS or DFS, and each state can be evaluated by a classifier or majority vote.\\\\nSources: https://lilianweng.github.io/posts/2023-06-23-agent/\\',\\n     \\'sources\\': \\'\\'}\\n```\\n\\n#### Customizing retrieved document processing\\u200b\\n\\nRetrieved documents can be fed to an LLM for answer distillation in a few different ways.\\n\\n`stuff`, `refine`, `map-reduce`, and `map-rerank` chains for passing documents to an LLM prompt are well summarized [here](/docs/modules/chains/document/).\\n\\n`stuff` is commonly used because it simply \"stuffs\" all retrieved documents into the prompt.\\n\\nThe [load_qa_chain](/docs/use_cases/question_answering/how_to/question_answering.html) is an easy way to pass documents to an LLM using these various approaches (e.g., see `chain_type`).\\n\\n```python\\nfrom langchain.chains.question_answering import load_qa_chain\\n\\nchain = load_qa_chain(llm, chain_type=\"stuff\")\\nchain({\"input_documents\": unique_docs, \"question\": question},return_only_outputs=True)\\n```\\n\\n&gt; **API Reference:**\\n&gt; - [load_qa_chain](https://api.python.langchain.com/en/latest/chains/langchain.chains.question_answering.load_qa_chain.html)\\n\\n```text\\n    {\\'output_text\\': \\'The approaches to task decomposition mentioned in the provided context are:\\\\n\\\\n1. Chain of thought (CoT): This approach involves instructing the language model to \"think step by step\" and decompose complex tasks into smaller and simpler steps. It enhances model performance on complex tasks by utilizing more test-time computation.\\\\n\\\\n2. Tree of Thoughts: This approach extends CoT by exploring multiple reasoning possibilities at each step. It decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS or DFS, and each state is evaluated by a classifier or majority vote.\\\\n\\\\n3. LLM with simple prompting: This approach involves using a language model with simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" to perform task decomposition.\\\\n\\\\n4. Task-specific instructions: This approach involves providing task-specific instructions to guide the language model in decomposing the task. For example, providing the instruction \"Write a story outline\" for the task of writing a novel.\\\\n\\\\n5. Human inputs: Task decomposition can also be done with human inputs, where humans provide guidance and input to break down the task into smaller subtasks.\\'}\\n```\\n\\nWe can also pass the `chain_type` to `RetrievalQA`.\\n\\n```python\\nqa_chain = RetrievalQA.from_chain_type(llm,retriever=vectorstore.as_retriever(),\\n                                       chain_type=\"stuff\")\\nresult = qa_chain({\"query\": question})\\n```\\n\\nIn summary, the user can choose the desired level of abstraction for QA:\\n\\n![summary_chains.png](/assets/images/summary_chains-593fd101c40fe9b151634e5299d02665.png)\\n\\n## Step 6. Chat\\u200b\\n\\nSee our [use-case on chat](/docs/use_cases/chatbots) for detail on this!', metadata={'source': 'https://python.langchain.com/docs/use_cases/question_answering/', 'title': 'Question Answering | ü¶úÔ∏èüîó Langchain', 'description': 'Open In Collab', 'language': 'en'}),\n Document(page_content=\"# Retrieval\\n\\nMany LLM applications require user-specific data that is not part of the model's training set.\\nThe primary way of accomplishing this is through Retrieval Augmented Generation (RAG).\\nIn this process, external data is _retrieved_ and then passed to the LLM when doing the _generation_ step.\\n\\nLangChain provides all the building blocks for RAG applications - from simple to complex.\\nThis section of the documentation covers everything related to the _retrieval_ step - e.g. the fetching of the data.\\nAlthough this sounds simple, it can be subtly complex.\\nThis encompasses several key modules.\\n\\n![data_connection_diagram](/assets/images/data_connection-c42d68c3d092b85f50d08d4cc171fc25.jpg)\\n\\n**Document loaders**\\n\\nLoad documents from many different sources.\\nLangChain provides over 100 different document loaders as well as integrations with other major providers in the space,\\nlike AirByte and Unstructured.\\nWe provide integrations to load all types of documents (HTML, PDF, code) from all types of locations (private s3 buckets, public websites).\\n\\n**Document transformers**\\n\\nA key part of retrieval is fetching only the relevant parts of documents.\\nThis involves several transformation steps in order to best prepare the documents for retrieval.\\nOne of the primary ones here is splitting (or chunking) a large document into smaller chunks.\\nLangChain provides several different algorithms for doing this, as well as logic optimized for specific document types (code, markdown, etc).\\n\\n**Text embedding models**\\n\\nAnother key part of retrieval has become creating embeddings for documents.\\nEmbeddings capture the semantic meaning of the text, allowing you to quickly and\\nefficiently find other pieces of text that are similar.\\nLangChain provides integrations with over 25 different embedding providers and methods,\\nfrom open-source to proprietary API,\\nallowing you to choose the one best suited for your needs.\\nLangChain provides a standard interface, allowing you to easily swap between models.\\n\\n**Vector stores**\\n\\nWith the rise of embeddings, there has emerged a need for databases to support efficient storage and searching of these embeddings.\\nLangChain provides integrations with over 50 different vectorstores, from open-source local ones to cloud-hosted proprietary ones,\\nallowing you to choose the one best suited for your needs.\\nLangChain exposes a standard interface, allowing you to easily swap between vector stores.\\n\\n**Retrievers**\\n\\nOnce the data is in the database, you still need to retrieve it.\\nLangChain supports many different retrieval algorithms and is one of the places where we add the most value.\\nWe support basic methods that are easy to get started - namely simple semantic search.\\nHowever, we have also added a collection of algorithms on top of this to increase performance.\\nThese include:\\n\\n- [Parent Document Retriever](/docs/modules/data_connection/retrievers/parent_document_retriever): This allows you to create multiple embeddings per parent document, allowing you to look up smaller chunks but return larger context.\\n- [Self Query Retriever](/docs/modules/data_connection/retrievers/self_query): User questions often contain a reference to something that isn't just semantic but rather expresses some logic that can best be represented as a metadata filter. Self-query allows you to parse out the _semantic_ part of a query from other _metadata filters_ present in the query.\\n- [Ensemble Retriever](/docs/modules/data_connection/retrievers/ensemble): Sometimes you may want to retrieve documents from multiple different sources, or using multiple different algorithms. The ensemble retriever allows you to easily do this.\\n- And more!\", metadata={'source': 'https://python.langchain.com/docs/modules/data_connection/', 'title': 'Retrieval | ü¶úÔ∏èüîó Langchain', 'description': \"Many LLM applications require user-specific data that is not part of the model's training set.\", 'language': 'en'})]\n\n\nPuedes corroborar que el ParentDocumentRetriever est√° regresando el subconjunto √∫nico de documentos completos al comparar el n√∫mero de documentos recuperados por el VectorStore y el ParentDocumentRetriever.\n\n[doc.metadata[\"source\"] for doc in full_documents_similarity], [\n    doc.metadata[\"source\"] for doc in full_documents_retriever\n]\n\n(['https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever',\n  'https://python.langchain.com/docs/integrations/retrievers/merger_retriever',\n  'https://python.langchain.com/docs/use_cases/question_answering/',\n  'https://python.langchain.com/docs/modules/data_connection/'],\n ['https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever',\n  'https://python.langchain.com/docs/integrations/retrievers/merger_retriever',\n  'https://python.langchain.com/docs/use_cases/question_answering/',\n  'https://python.langchain.com/docs/modules/data_connection/'])"
  },
  {
    "objectID": "notebooks/04_parent_retrievers.html#recuperaci√≥n-de-fragmentos-largos-en-lugar-de-documentos-completos",
    "href": "notebooks/04_parent_retrievers.html#recuperaci√≥n-de-fragmentos-largos-en-lugar-de-documentos-completos",
    "title": "Parent retrievers",
    "section": "Recuperaci√≥n de fragmentos largos en lugar de documentos completos",
    "text": "Recuperaci√≥n de fragmentos largos en lugar de documentos completos\nLos documentos pueden ser muy grandes para ser recuperados en su totalidad y ser √∫tiles.\nPor ejemplo, un documento completo podr√≠a ser un libro, pero quiz√° s√≥lo necesito un cap√≠tulo para responder a mi pregunta. O quiz√° s√≥lo necesito un par de p√°rrafos.\nSi planeas utilizar los documentos recuperados en un proceso de Retrival Augmented Generation (RAG), es posible que los documentos gigantes ni siquiera puedan ser procesados por la ventana de contexto del modelo de lenguaje.\nPara este caso, el ParentDocumentRetriever puede ser configurado para romper los documentos en fragmentos peque√±os, buscar sobre ellos y luego devolver fragmentos m√°s largos (sin ser el documento completo).\n\nparent_splitter = RecursiveCharacterTextSplitter.from_language(\n    language=Language.MARKDOWN,\n    chunk_size=400,\n    chunk_overlap=40,\n    length_function=num_tokens_from_string,\n)\n\nchild_splitter = RecursiveCharacterTextSplitter.from_language(\n    language=Language.MARKDOWN,\n    chunk_size=100,\n    chunk_overlap=10,\n    length_function=num_tokens_from_string,\n)\n\nvectorstore = get_vectorstore(collection_name=\"big_fragments\")\n\nstore = InMemoryStore()\n\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter,\n)\n\nretriever.add_documents(docs)\n\nAhora hay m√°s documentos en el Store dado que cada documento se ha dividido en fragmentos m√°s peque√±os.\n\nlen(list(store.yield_keys()))\n\n6473\n\n\n\nvectorstore.similarity_search(\n    \"Does the MultiQueryRetriever might be able to overcome some of the limitations of...?\"\n)\n\n[Document(page_content='The `MultiQueryRetriever` automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set', metadata={'description': 'Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on \"distance\". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious.', 'doc_id': '44da0302-d174-4feb-9732-e4af358ce3fa', 'language': 'en', 'source': 'https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever', 'title': 'MultiQueryRetriever | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='The `MergerRetriever` class can be used to improve the accuracy of document retrieval in a number of ways. First, it can combine the results of multiple retrievers, which can help to reduce the risk of bias in the results. Second, it can rank the results of the different retrievers, which can help to ensure that the most relevant documents are returned first.', metadata={'description': 'Lord of the Retrievers, also known as MergerRetriever, takes a list of retrievers as input and merges the results of their getrelevantdocuments() methods into a single list. The merged results will be a list of documents that are relevant to the query and that have been ranked by the different retrievers.', 'doc_id': '4b3087da-49a5-46df-be5b-b120447f4b5f', 'language': 'en', 'source': 'https://python.langchain.com/docs/integrations/retrievers/merger_retriever', 'title': 'LOTR (Merger Retriever) | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='- `MultiQueryRetriever` [generates variants of the input question](/docs/modules/data_connection/retrievers/MultiQueryRetriever) to improve retrieval.\\n- `Max marginal relevance` selects for [relevance and diversity](https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf) among the retrieved documents.', metadata={'description': 'Open In Collab', 'doc_id': 'fe9286c0-5485-49c6-968a-9f53039ad31c', 'language': 'en', 'source': 'https://python.langchain.com/docs/use_cases/question_answering/', 'title': 'Question Answering | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='# MultiQueryRetriever\\n\\nDistance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on \"distance\". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious.', metadata={'description': 'Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on \"distance\". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious.', 'doc_id': '44da0302-d174-4feb-9732-e4af358ce3fa', 'language': 'en', 'source': 'https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever', 'title': 'MultiQueryRetriever | ü¶úÔ∏èüîó Langchain'})]\n\n\n\nretriever.get_relevant_documents(\n    \"Does the MultiQueryRetriever might be able to overcome some of the limitations of...?\"\n)\n\n[Document(page_content='# MultiQueryRetriever\\n\\nDistance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on \"distance\". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious.\\n\\nThe `MultiQueryRetriever` automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results.\\n\\n```python\\n# Build a sample vectorDB\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.document_loaders import WebBaseLoader\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\n\\n# Load blog post\\nloader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\\ndata = loader.load()\\n\\n# Split\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\\nsplits = text_splitter.split_documents(data)', metadata={'source': 'https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever', 'title': 'MultiQueryRetriever | ü¶úÔ∏èüîó Langchain', 'description': 'Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on \"distance\". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious.', 'language': 'en'}),\n Document(page_content='# LOTR (Merger Retriever)\\n\\n`Lord of the Retrievers`, also known as `MergerRetriever`, takes a list of retrievers as input and merges the results of their get_relevant_documents() methods into a single list. The merged results will be a list of documents that are relevant to the query and that have been ranked by the different retrievers.\\n\\nThe `MergerRetriever` class can be used to improve the accuracy of document retrieval in a number of ways. First, it can combine the results of multiple retrievers, which can help to reduce the risk of bias in the results. Second, it can rank the results of the different retrievers, which can help to ensure that the most relevant documents are returned first.\\n\\n```python\\nimport os\\nimport chromadb\\nfrom langchain.retrievers.merger_retriever import MergerRetriever\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.document_transformers import (\\n    EmbeddingsRedundantFilter,\\n    EmbeddingsClusteringFilter,\\n)\\nfrom langchain.retrievers.document_compressors import DocumentCompressorPipeline\\nfrom langchain.retrievers import ContextualCompressionRetriever\\n\\n# Get 3 diff embeddings.\\nall_mini = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\\nmulti_qa_mini = HuggingFaceEmbeddings(model_name=\"multi-qa-MiniLM-L6-dot-v1\")\\nfilter_embeddings = OpenAIEmbeddings()\\n\\nABS_PATH = os.path.dirname(os.path.abspath(__file__))\\nDB_DIR = os.path.join(ABS_PATH, \"db\")', metadata={'source': 'https://python.langchain.com/docs/integrations/retrievers/merger_retriever', 'title': 'LOTR (Merger Retriever) | ü¶úÔ∏èüîó Langchain', 'description': 'Lord of the Retrievers, also known as MergerRetriever, takes a list of retrievers as input and merges the results of their getrelevantdocuments() methods into a single list. The merged results will be a list of documents that are relevant to the query and that have been ranked by the different retrievers.', 'language': 'en'}),\n Document(page_content=\"```\\n\\nSome common ways to improve on vector similarity search include:\\n\\n- `MultiQueryRetriever` [generates variants of the input question](/docs/modules/data_connection/retrievers/MultiQueryRetriever) to improve retrieval.\\n- `Max marginal relevance` selects for [relevance and diversity](https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf) among the retrieved documents.\\n- Documents can be filtered during retrieval using [metadata filters](/docs/use_cases/question_answering/how_to/document-context-aware-QA).\\n\\n```python\\nimport logging\\n\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\\n\\nlogging.basicConfig()\\nlogging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)\\n\\nretriever_from_llm = MultiQueryRetriever.from_llm(retriever=vectorstore.as_retriever(),\\n                                                  llm=ChatOpenAI(temperature=0))\\nunique_docs = retriever_from_llm.get_relevant_documents(query=question)\\nlen(unique_docs)\\n```\\n\\n&gt; **API Reference:**\\n&gt; - [ChatOpenAI](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html)\\n&gt; - [MultiQueryRetriever](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.multi_query.MultiQueryRetriever.html)\", metadata={'source': 'https://python.langchain.com/docs/use_cases/question_answering/', 'title': 'Question Answering | ü¶úÔ∏èüîó Langchain', 'description': 'Open In Collab', 'language': 'en'})]"
  },
  {
    "objectID": "notebooks/05_self_retrievers.html#librer√≠as",
    "href": "notebooks/05_self_retrievers.html#librer√≠as",
    "title": "Self Retrievers",
    "section": "Librer√≠as",
    "text": "Librer√≠as\n\nfrom pprint import pprint\n\nfrom dotenv import load_dotenv\nfrom langchain.chains import create_tagging_chain_pydantic\nfrom langchain.chains.query_constructor.base import AttributeInfo\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.indexes import SQLRecordManager, index\nfrom langchain.retrievers import SelfQueryRetriever\nfrom langchain.schema import Document\nfrom langchain.text_splitter import Language, RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import Chroma\nfrom pydantic import BaseModel, Field\n\nfrom src.langchain_docs_loader import LangchainDocsLoader, num_tokens_from_string\n\nload_dotenv()\n\nTrue"
  },
  {
    "objectID": "notebooks/05_self_retrievers.html#carga-de-datos",
    "href": "notebooks/05_self_retrievers.html#carga-de-datos",
    "title": "Self Retrievers",
    "section": "Carga de datos",
    "text": "Carga de datos\n\ntext_splitter = RecursiveCharacterTextSplitter.from_language(\n    language=Language.MARKDOWN,\n    chunk_size=400,\n    chunk_overlap=50,\n    length_function=num_tokens_from_string,\n)\n\nloader = LangchainDocsLoader(include_output_cells=False)\ndocs = loader.load()\ndocs = text_splitter.split_documents(docs)\nlen(docs)\n\n3190\n\n\n\ndocs = [doc for doc in docs if doc.page_content != \"```\"]"
  },
  {
    "objectID": "notebooks/05_self_retrievers.html#inicializado-de-modelo-de-lenguaje",
    "href": "notebooks/05_self_retrievers.html#inicializado-de-modelo-de-lenguaje",
    "title": "Self Retrievers",
    "section": "Inicializado de modelo de lenguaje",
    "text": "Inicializado de modelo de lenguaje\n\nllm = ChatOpenAI(temperature=0.1)"
  },
  {
    "objectID": "notebooks/05_self_retrievers.html#etiquetado-de-documentos",
    "href": "notebooks/05_self_retrievers.html#etiquetado-de-documentos",
    "title": "Self Retrievers",
    "section": "Etiquetado de documentos",
    "text": "Etiquetado de documentos\nLos documentos por s√≠ mismos son √∫ltiles, pero cuando son etiquetados con informaci√≥n adicional, pueden volverse m√°s √∫tiles. Por ejemplo, si etiquetamos los documentos con su idioma, podemos filtrar los documentos que no est√©n en el idioma que nos interesa. Si etiquetamos los documentos con su tema, podemos filtrar los documentos que no est√©n relacionados con el tema que nos interesa. De esta manera, podemos reducir el espacio de b√∫squeda y obtener mejores resultados.\n\nCreaci√≥n de esquema de etiquetas\n\nclass Tags(BaseModel):\n    completness: str = Field(\n        description=\"Describes how useful is the text in terms of self-explanation. It is critical to excel.\",\n        enum=[\"Very\", \"Quite\", \"Medium\", \"Little\", \"Not\"],\n    )\n    code_snippet: bool = Field(\n        default=False,\n        description=\"Whether the text fragment includes a code snippet. Code snippets are valid markdown code blocks.\",\n    )\n    description: bool = Field(\n        default=False, description=\"Whether the text fragment includes a description.\"\n    )\n    talks_about_vectorstore: bool = Field(\n        default=False,\n        description=\"Whether the text fragment talks about a vectorstore.\",\n    )\n    talks_about_retriever: bool = Field(\n        default=False, description=\"Whether the text fragment talks about a retriever.\"\n    )\n    talks_about_chain: bool = Field(\n        default=False, description=\"Whether the text fragment talks about a chain.\"\n    )\n    talks_about_expression_language: bool = Field(\n        default=False,\n        description=\"Whether the text fragment talks about an langchain expression language.\",\n    )\n    contains_markdown_table: bool = Field(\n        default=False,\n        description=\"Whether the text fragment contains a markdown table.\",\n    )\n\n\npprint(Tags.schema())\n\n{'properties': {'code_snippet': {'default': False,\n                                 'description': 'Whether the text fragment '\n                                                'includes a code snippet. Code '\n                                                'snippets are valid markdown '\n                                                'code blocks.',\n                                 'title': 'Code Snippet',\n                                 'type': 'boolean'},\n                'completness': {'description': 'Describes how useful is the '\n                                               'text in terms of '\n                                               'self-explanation. It is '\n                                               'critical to excel.',\n                                'enum': ['Very',\n                                         'Quite',\n                                         'Medium',\n                                         'Little',\n                                         'Not'],\n                                'title': 'Completness',\n                                'type': 'string'},\n                'contains_markdown_table': {'default': False,\n                                            'description': 'Whether the text '\n                                                           'fragment contains '\n                                                           'a markdown table.',\n                                            'title': 'Contains Markdown Table',\n                                            'type': 'boolean'},\n                'description': {'default': False,\n                                'description': 'Whether the text fragment '\n                                               'includes a description.',\n                                'title': 'Description',\n                                'type': 'boolean'},\n                'talks_about_chain': {'default': False,\n                                      'description': 'Whether the text '\n                                                     'fragment talks about a '\n                                                     'chain.',\n                                      'title': 'Talks About Chain',\n                                      'type': 'boolean'},\n                'talks_about_expression_language': {'default': False,\n                                                    'description': 'Whether '\n                                                                   'the text '\n                                                                   'fragment '\n                                                                   'talks '\n                                                                   'about an '\n                                                                   'langchain '\n                                                                   'expression '\n                                                                   'language.',\n                                                    'title': 'Talks About '\n                                                             'Expression '\n                                                             'Language',\n                                                    'type': 'boolean'},\n                'talks_about_retriever': {'default': False,\n                                          'description': 'Whether the text '\n                                                         'fragment talks about '\n                                                         'a retriever.',\n                                          'title': 'Talks About Retriever',\n                                          'type': 'boolean'},\n                'talks_about_vectorstore': {'default': False,\n                                            'description': 'Whether the text '\n                                                           'fragment talks '\n                                                           'about a '\n                                                           'vectorstore.',\n                                            'title': 'Talks About Vectorstore',\n                                            'type': 'boolean'}},\n 'required': ['completness'],\n 'title': 'Tags',\n 'type': 'object'}\n\n\n\n\nCreaci√≥n de cadena de generaci√≥n de etiquetas (etiquetador)\n\ntagging_prompt = \"\"\"Extract the desired information from the following passage.\n\nOnly extract the properties mentioned in the 'information_extraction' function.\nCompletness should involve more than one sentence.\nTo consider that a passage talks about a property, it is enough that it mentions it once.\nIf there is no mention of a property, set it to False. It only applies for the talk_about_* properties.\n\nFor instance,\nTo set `talks_about_vectorstore` to True, document should contain the word 'vectorstore' at least once.\nTo set `talks_about_retriever` to True, document should contain the word 'retriever' at least once.\nTo set `talks_about_chain` to True, document should contain the word 'chain' at least once.\nTo set `talks_about_expression_language` to True, document should contain the word 'expression language' or 'LCEL' at least once.\n\nPassage:\n{input}\n\"\"\"\n\ntagging_chain = create_tagging_chain_pydantic(Tags, llm)\n\n\n\nEjemplos de uso del etiquetador\nProbablemente, un fragmento que √∫nicamente contiene una lista de enlaces a otros fragmentos que tambi√©n se encuentran indexados no es muy √∫til. Esto podr√≠a ocasionar que recuperemos un documento que no es relevante para la consulta, mientra el documento que s√≠ es relevante no se encuentrar√≠a en los primeros lugares de la lista de resultados.\n\nidx = 0\n\nresult = tagging_chain.invoke(input={\"input\": docs[idx].page_content})\nprint(result.get(\"input\"))\npprint(result.get(\"text\").dict())\n\n[üìÑÔ∏è DependentsDependents stats for langchain-ai/langchain](/docs/additional_resources/dependents)[üìÑÔ∏è TutorialsBelow are links to tutorials and courses on LangChain. For written guides on common use cases for LangChain, check out the use cases guides.](/docs/additional_resources/tutorials)[üìÑÔ∏è YouTube videos‚õì icon marks a new addition [last update 2023-09-21]](/docs/additional_resources/youtube)[üîó Gallery](https://github.com/kyrolabs/awesome-langchain)\n{'code_snippet': False,\n 'completness': 'Not',\n 'contains_markdown_table': False,\n 'description': True,\n 'talks_about_chain': True,\n 'talks_about_expression_language': True,\n 'talks_about_retriever': True,\n 'talks_about_vectorstore': True}\n\n\nUn fragmento con enlace a su documentaci√≥n y ejemplo de uso ser√≠a m√°s √∫til.\n\nidx = 1000\n\nresult = tagging_chain.invoke(input={\"input\": docs[idx].page_content})\nprint(result.get(\"input\"))\npprint(result.get(\"text\").dict())\n\n# AWS DynamoDB\n\n[Amazon AWS DynamoDB](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dynamodb/index.html) is a fully managed `NoSQL` database service that provides fast and predictable performance with seamless scalability.\n\nThis notebook goes over how to use `DynamoDB` to store chat message history.\n\nFirst make sure you have correctly configured the [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html). Then make sure you have installed `boto3`.\n\n```bash\npip install boto3\n```\n\nNext, create the `DynamoDB` Table where we will be storing messages:\n\n```python\nimport boto3\n\n# Get the service resource.\ndynamodb = boto3.resource(\"dynamodb\")\n\n# Create the DynamoDB table.\ntable = dynamodb.create_table(\n    TableName=\"SessionTable\",\n    KeySchema=[{\"AttributeName\": \"SessionId\", \"KeyType\": \"HASH\"}],\n    AttributeDefinitions=[{\"AttributeName\": \"SessionId\", \"AttributeType\": \"S\"}],\n    BillingMode=\"PAY_PER_REQUEST\",\n)\n\n# Wait until the table exists.\ntable.meta.client.get_waiter(\"table_exists\").wait(TableName=\"SessionTable\")\n\n# Print out some data about the table.\nprint(table.item_count)\n```\n\n## DynamoDBChatMessageHistory‚Äã\n\n```python\nfrom langchain.memory.chat_message_histories import DynamoDBChatMessageHistory\n\nhistory = DynamoDBChatMessageHistory(table_name=\"SessionTable\", session_id=\"0\")\n\nhistory.add_user_message(\"hi!\")\n\nhistory.add_ai_message(\"whats up?\")\n```\n\n&gt; **API Reference:**\n&gt; - [DynamoDBChatMessageHistory](https://api.python.langchain.com/en/latest/memory/langchain.memory.chat_message_histories.dynamodb.DynamoDBChatMessageHistory.html)\n\n```python\nhistory.messages\n```\n{'code_snippet': True,\n 'completness': 'Very',\n 'contains_markdown_table': True,\n 'description': True,\n 'talks_about_chain': True,\n 'talks_about_expression_language': True,\n 'talks_about_retriever': True,\n 'talks_about_vectorstore': True}\n\n\n\nidx = 1400\n\nresult = tagging_chain.invoke(input={\"input\": docs[idx].page_content})\nprint(result.get(\"input\"))\npprint(result.get(\"text\").dict())\n\ntext content from PubMed Central and publisher web sites.](/docs/integrations/retrievers/pubmed)[üìÑÔ∏è RePhraseQueryRetrieverSimple retriever that applies an LLM between the user input and the query pass the to retriever.](/docs/integrations/retrievers/re_phrase)[üìÑÔ∏è SEC filings dataSEC filings data powered by Kay.ai and Cybersyn.](/docs/integrations/retrievers/sec_filings)[üìÑÔ∏è SVMSupport vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.](/docs/integrations/retrievers/svm)[üìÑÔ∏è TF-IDFTF-IDF means term-frequency times inverse document-frequency.](/docs/integrations/retrievers/tf_idf)[üìÑÔ∏è VespaVespa is a fully featured search engine and vector database. It supports vector search (ANN), lexical search, and search in structured data, all in the same query.](/docs/integrations/retrievers/vespa)[üìÑÔ∏è Weaviate Hybrid SearchWeaviate is an open source vector database.](/docs/integrations/retrievers/weaviate-hybrid)[üìÑÔ∏è WikipediaWikipedia is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. Wikipedia is the largest and most-read reference work in history.](/docs/integrations/retrievers/wikipedia)[üìÑÔ∏è ZepRetriever Example for Zep - A long-term memory store for LLM applications.](/docs/integrations/retrievers/zep_memorystore)\n{'code_snippet': False,\n 'completness': 'Not',\n 'contains_markdown_table': False,\n 'description': False,\n 'talks_about_chain': False,\n 'talks_about_expression_language': False,\n 'talks_about_retriever': True,\n 'talks_about_vectorstore': False}\n\n\n\n\nEtiquetado de documentos\n\ntagging_results = tagging_chain.batch(\n    inputs=[{\"input\": doc.page_content} for doc in docs[:200]],\n    return_exceptions=True,\n    config={\n        \"max_concurrency\": 50,\n    },\n)\n\ndocs_with_tags = [\n    Document(\n        page_content=doc.page_content,\n        metadata={\n            **doc.metadata,\n            **result.get(\"text\").dict(),\n        },\n    )\n    for doc, result in zip(docs, tagging_results)\n    if not isinstance(result, Exception)\n]\n\nf\"Documents with tags: {len(docs_with_tags)}\"\n\n'Documents with tags: 184'"
  },
  {
    "objectID": "notebooks/05_self_retrievers.html#indexado-de-documentos",
    "href": "notebooks/05_self_retrievers.html#indexado-de-documentos",
    "title": "Self Retrievers",
    "section": "Indexado de documentos",
    "text": "Indexado de documentos\n\nvectorstore = Chroma(\n    collection_name=\"langchain_docs\",\n    embedding_function=OpenAIEmbeddings(),\n)\n\nrecord_manager = SQLRecordManager(\n    db_url=\"sqlite:///:memory:\",\n    namespace=\"chroma/langchain_docs\",\n)\n\nrecord_manager.create_schema()\n\nindex(\n    docs_source=docs_with_tags,\n    record_manager=record_manager,\n    vector_store=vectorstore,\n    cleanup=\"full\",\n    source_id_key=\"source\",\n)\n\n{'num_added': 184, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}"
  },
  {
    "objectID": "notebooks/05_self_retrievers.html#recuperaci√≥n-de-documentos-con-un-self-retriever",
    "href": "notebooks/05_self_retrievers.html#recuperaci√≥n-de-documentos-con-un-self-retriever",
    "title": "Self Retrievers",
    "section": "Recuperaci√≥n de documentos con un Self Retriever",
    "text": "Recuperaci√≥n de documentos con un Self Retriever\n\nCreaci√≥n de interfaz de los metadatos disponibles en el √≠ndice\n\nmetadata_field_info = [\n    AttributeInfo(\n        name=\"completness\",\n        description=\"Describes how useful is the text in terms of self-explanation. It is critical to excel.\",\n        type='enum=[\"Very\", \"Quite\", \"Medium\", \"Little\", \"Not\"]',\n    ),\n    AttributeInfo(\n        name=\"code_snippet\",\n        description=\"Whether the text fragment includes a code snippet. Code snippets are valid markdown code blocks.\",\n        type=\"bool\",\n    ),\n    AttributeInfo(\n        name=\"description\",\n        description=\"Whether the text fragment includes a description.\",\n        type=\"bool\",\n    ),\n    AttributeInfo(\n        name=\"talks_about_vectorstore\",\n        description=\"Whether the text fragment talks about a vectorstore.\",\n        type=\"bool\",\n    ),\n    AttributeInfo(\n        name=\"talks_about_retriever\",\n        description=\"Whether the text fragment talks about a retriever.\",\n        type=\"bool\",\n    ),\n    AttributeInfo(\n        name=\"talks_about_chain\",\n        description=\"Whether the text fragment talks about a chain.\",\n        type=\"bool\",\n    ),\n    AttributeInfo(\n        name=\"contains_markdown_table\",\n        description=\"Whether the text fragment contains a markdown table.\",\n        type=\"bool\",\n    ),\n]\n\ndocument_content_description = \"Langchain documentation\"\n\n\n\nCreaci√≥n de retriever\n\nllm = ChatOpenAI(temperature=0)\nretriever = SelfQueryRetriever.from_llm(\n    llm=llm,\n    vectorstore=vectorstore,\n    document_contents=document_content_description,\n    metadata_field_info=metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n\n\n\nRecuperaci√≥n de documentos con el retriever\n\nrelevant_documents = retriever.get_relevant_documents(\n    \"useful documents that talk about expression language and retrievers\"\n)\nrelevant_documents\n\n/Users/jvelezmagic/Documents/github/course/langchain-course/course_document_management/.venv/lib/python3.11/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n  warnings.warn(\n\n\nquery='expression language retrievers' filter=Operation(operator=&lt;Operator.AND: 'and'&gt;, arguments=[Comparison(comparator=&lt;Comparator.EQ: 'eq'&gt;, attribute='completness', value='Very'), Comparison(comparator=&lt;Comparator.EQ: 'eq'&gt;, attribute='talks_about_retriever', value=True)]) limit=None\n\n\n[Document(page_content='```\\n\\n```python\\nchain = (\\n    {\"context\": retriever, \"question\": RunnablePassthrough()} \\n    | prompt \\n    | model \\n    | StrOutputParser()\\n)\\n```\\n\\n```python\\nchain.invoke(\"where did harrison work?\")\\n```\\n\\n```python\\ntemplate = \"\"\"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer in the following language: {language}\\n\"\"\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\nchain = {\\n    \"context\": itemgetter(\"question\") | retriever, \\n    \"question\": itemgetter(\"question\"), \\n    \"language\": itemgetter(\"language\")\\n} | prompt | model | StrOutputParser()\\n```\\n\\n```python\\nchain.invoke({\"question\": \"where did harrison work\", \"language\": \"italian\"})\\n```', metadata={'code_snippet': True, 'completness': 'Very', 'contains_markdown_table': False, 'description': True, 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/cookbook/retrieval', 'talks_about_chain': True, 'talks_about_expression_language': True, 'talks_about_retriever': True, 'talks_about_vectorstore': False, 'title': 'RAG | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='# RAG\\n\\nLet\\'s look at adding in a retrieval step to a prompt and LLM, which adds up to a \"retrieval-augmented generation\" chain\\n\\n```bash\\npip install langchain openai faiss-cpu tiktoken\\n```\\n\\n```python\\nfrom operator import itemgetter\\n\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.schema.output_parser import StrOutputParser\\nfrom langchain.schema.runnable import RunnablePassthrough\\nfrom langchain.vectorstores import FAISS\\n```\\n\\n&gt; **API Reference:**\\n&gt; - [ChatPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.ChatPromptTemplate.html)\\n&gt; - [ChatOpenAI](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html)\\n&gt; - [OpenAIEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html)\\n&gt; - [StrOutputParser](https://api.python.langchain.com/en/latest/schema/langchain.schema.output_parser.StrOutputParser.html)\\n&gt; - [RunnablePassthrough](https://api.python.langchain.com/en/latest/schema/langchain.schema.runnable.passthrough.RunnablePassthrough.html)\\n&gt; - [FAISS](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.faiss.FAISS.html)\\n\\n```python\\nvectorstore = FAISS.from_texts([\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())\\nretriever = vectorstore.as_retriever()\\n\\ntemplate = \"\"\"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n\"\"\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\nmodel = ChatOpenAI()', metadata={'code_snippet': False, 'completness': 'Very', 'contains_markdown_table': False, 'description': False, 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/cookbook/retrieval', 'talks_about_chain': True, 'talks_about_expression_language': True, 'talks_about_retriever': True, 'talks_about_vectorstore': False, 'title': 'RAG | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='## Manipulating outputs/inputs\\u200b\\n\\nMaps can be useful for manipulating the output of one Runnable to match the input format of the next Runnable in a sequence.\\n\\n```python\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.schema.output_parser import StrOutputParser\\nfrom langchain.schema.runnable import RunnablePassthrough\\nfrom langchain.vectorstores import FAISS\\n\\nvectorstore = FAISS.from_texts([\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())\\nretriever = vectorstore.as_retriever()\\ntemplate = \"\"\"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n\"\"\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\nretrieval_chain = (\\n    {\"context\": retriever, \"question\": RunnablePassthrough()} \\n    | prompt \\n    | model \\n    | StrOutputParser()\\n)\\n\\nretrieval_chain.invoke(\"where did harrison work?\")', metadata={'code_snippet': True, 'completness': 'Very', 'contains_markdown_table': False, 'description': True, 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/how_to/map', 'talks_about_chain': True, 'talks_about_expression_language': True, 'talks_about_retriever': True, 'talks_about_vectorstore': True, 'title': 'Use RunnableMaps | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='## Without References\\u200b\\n\\nWhen references aren\\'t available, you can still predict the preferred response.\\nThe results will reflect the evaluation model\\'s preference, which is less reliable and may result\\nin preferences that are factually incorrect.\\n\\n```python\\nfrom langchain.evaluation import load_evaluator\\n\\nevaluator = load_evaluator(\"pairwise_string\")\\n```\\n\\n&gt; **API Reference:**\\n&gt; - [load_evaluator](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.loading.load_evaluator.html)\\n\\n```python\\nevaluator.evaluate_string_pairs(\\n    prediction=\"Addition is a mathematical operation.\",\\n    prediction_b=\"Addition is a mathematical operation that adds two numbers to create a third number, the \\'sum\\'.\",\\n    input=\"What is addition?\",\\n)\\n```', metadata={'code_snippet': True, 'completness': 'Very', 'contains_markdown_table': False, 'description': True, 'language': 'en', 'source': 'https://python.langchain.com/docs/guides/evaluation/comparison/pairwise_string', 'talks_about_chain': True, 'talks_about_expression_language': True, 'talks_about_retriever': True, 'talks_about_vectorstore': False, 'title': 'Pairwise String Comparison | ü¶úÔ∏èüîó Langchain'})]\n\n\n\nrelevant_documents = retriever.get_relevant_documents(\n    \"useful documents that talk about expression language and retrievers or vectorstores\"\n)\nrelevant_documents\n\nquery='expression language' filter=Operation(operator=&lt;Operator.AND: 'and'&gt;, arguments=[Comparison(comparator=&lt;Comparator.EQ: 'eq'&gt;, attribute='completness', value='Very'), Operation(operator=&lt;Operator.OR: 'or'&gt;, arguments=[Comparison(comparator=&lt;Comparator.EQ: 'eq'&gt;, attribute='talks_about_retriever', value=True), Comparison(comparator=&lt;Comparator.EQ: 'eq'&gt;, attribute='talks_about_vectorstore', value=True)])]) limit=None\n\n\n[Document(page_content='# Code writing\\n\\nExample of how to use LCEL to write Python code.\\n\\n```python\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\\nfrom langchain.schema.output_parser import StrOutputParser\\nfrom langchain.utilities import PythonREPL\\n```\\n\\n&gt; **API Reference:**\\n&gt; - [ChatOpenAI](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html)\\n&gt; - [ChatPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.ChatPromptTemplate.html)\\n&gt; - [SystemMessagePromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.SystemMessagePromptTemplate.html)\\n&gt; - [HumanMessagePromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.HumanMessagePromptTemplate.html)\\n&gt; - [StrOutputParser](https://api.python.langchain.com/en/latest/schema/langchain.schema.output_parser.StrOutputParser.html)\\n&gt; - [PythonREPL](https://api.python.langchain.com/en/latest/utilities/langchain.utilities.python.PythonREPL.html)\\n\\n```python\\ntemplate = \"\"\"Write some python code to solve the user\\'s problem. \\n\\nReturn only python code in Markdown format, e.g.:\\n\\n```python\\n....\\n```\"\"\"\\nprompt = ChatPromptTemplate.from_messages(\\n    [(\"system\", template), (\"human\", \"{input}\")]\\n)\\n\\nmodel = ChatOpenAI()\\n```\\n\\n```python\\ndef _sanitize_output(text: str):\\n    _, after = text.split(\"```python\")\\n    return after.split(\"```\")[0]\\n```\\n\\n```python\\nchain = prompt | model | StrOutputParser() | _sanitize_output | PythonREPL().run\\n```\\n\\n```python\\nchain.invoke({\"input\": \"whats 2 plus 2\"})\\n```', metadata={'code_snippet': True, 'completness': 'Very', 'contains_markdown_table': True, 'description': True, 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/cookbook/code_writing', 'talks_about_chain': True, 'talks_about_expression_language': True, 'talks_about_retriever': True, 'talks_about_vectorstore': True, 'title': 'Code writing | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='```\\n\\n```python\\nchain = (\\n    {\"context\": retriever, \"question\": RunnablePassthrough()} \\n    | prompt \\n    | model \\n    | StrOutputParser()\\n)\\n```\\n\\n```python\\nchain.invoke(\"where did harrison work?\")\\n```\\n\\n```python\\ntemplate = \"\"\"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer in the following language: {language}\\n\"\"\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\nchain = {\\n    \"context\": itemgetter(\"question\") | retriever, \\n    \"question\": itemgetter(\"question\"), \\n    \"language\": itemgetter(\"language\")\\n} | prompt | model | StrOutputParser()\\n```\\n\\n```python\\nchain.invoke({\"question\": \"where did harrison work\", \"language\": \"italian\"})\\n```', metadata={'code_snippet': True, 'completness': 'Very', 'contains_markdown_table': False, 'description': True, 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/cookbook/retrieval', 'talks_about_chain': True, 'talks_about_expression_language': True, 'talks_about_retriever': True, 'talks_about_vectorstore': False, 'title': 'RAG | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='## Using Constitutional Principles\\u200b\\n\\nCustom rubrics are similar to principles from [Constitutional AI](https://arxiv.org/abs/2212.08073). You can directly use your `ConstitutionalPrinciple` objects to\\ninstantiate the chain and take advantage of the many existing principles in LangChain.\\n\\n```python\\nfrom langchain.chains.constitutional_ai.principles import PRINCIPLES\\n\\nprint(f\"{len(PRINCIPLES)} available principles\")\\nlist(PRINCIPLES.items())[:5]\\n```\\n\\n```python\\nevaluator = load_evaluator(\\n    EvaluatorType.CRITERIA, criteria=PRINCIPLES[\"harmful1\"]\\n)\\neval_result = evaluator.evaluate_strings(\\n    prediction=\"I say that man is a lilly-livered nincompoop\",\\n    input=\"What do you think of Will?\",\\n)\\nprint(eval_result)\\n```\\n\\n## Configuring the LLM\\u200b\\n\\nIf you don\\'t specify an eval LLM, the `load_evaluator` method will initialize a `gpt-4` LLM to power the grading chain. Below, use an anthropic model instead.\\n\\n```python\\n# %pip install ChatAnthropic\\n# %env ANTHROPIC_API_KEY=&lt;API_KEY&gt;\\n```\\n\\n```python\\nfrom langchain.chat_models import ChatAnthropic\\n\\nllm = ChatAnthropic(temperature=0)\\nevaluator = load_evaluator(\"criteria\", llm=llm, criteria=\"conciseness\")\\n```\\n\\n&gt; **API Reference:**\\n&gt; - [ChatAnthropic](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.anthropic.ChatAnthropic.html)\\n\\n```python\\neval_result = evaluator.evaluate_strings(\\n    prediction=\"What\\'s 2+2? That\\'s an elementary question. The answer you\\'re looking for is that two and two is four.\",\\n    input=\"What\\'s 2+2?\",\\n)\\nprint(eval_result)\\n```', metadata={'code_snippet': True, 'completness': 'Very', 'contains_markdown_table': True, 'description': True, 'language': 'en', 'source': 'https://python.langchain.com/docs/guides/evaluation/string/criteria_eval_chain', 'talks_about_chain': True, 'talks_about_expression_language': True, 'talks_about_retriever': True, 'talks_about_vectorstore': True, 'title': 'Criteria Evaluation | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='# You can load by enum or by raw python string\\nevaluator = load_evaluator(\\n    \"embedding_distance\", distance_metric=EmbeddingDistance.EUCLIDEAN\\n)\\n```\\n\\n## Select Embeddings to Use\\u200b\\n\\nThe constructor uses `OpenAI` embeddings by default, but you can configure this however you want. Below, use huggingface local embeddings\\n\\n```python\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\nembedding_model = HuggingFaceEmbeddings()\\nhf_evaluator = load_evaluator(\"embedding_distance\", embeddings=embedding_model)\\n```\\n\\n&gt; **API Reference:**\\n&gt; - [HuggingFaceEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.huggingface.HuggingFaceEmbeddings.html)\\n\\n```python\\nhf_evaluator.evaluate_strings(prediction=\"I shall go\", reference=\"I shan\\'t go\")\\n```\\n\\n```python\\nhf_evaluator.evaluate_strings(prediction=\"I shall go\", reference=\"I will go\")\\n```\\n\\n[](None)_1. Note: When it comes to semantic similarity, this often gives better results than older string distance metrics (such as those in the [StringDistanceEvalChain](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.string_distance.base.StringDistanceEvalChain.html#langchain.evaluation.string_distance.base.StringDistanceEvalChain)), though it tends to be less reliable than evaluators that use the LLM directly (such as the [QAEvalChain](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.qa.eval_chain.QAEvalChain.html#langchain.evaluation.qa.eval_chain.QAEvalChain) or [LabeledCriteriaEvalChain](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.criteria.eval_chain.LabeledCriteriaEvalChain.html#langchain.evaluation.criteria.eval_chain.LabeledCriteriaEvalChain)) _', metadata={'code_snippet': True, 'completness': 'Very', 'contains_markdown_table': True, 'description': True, 'language': 'en', 'source': 'https://python.langchain.com/docs/guides/evaluation/string/embedding_distance', 'talks_about_chain': True, 'talks_about_expression_language': True, 'talks_about_retriever': True, 'talks_about_vectorstore': True, 'title': 'Embedding Distance | ü¶úÔ∏èüîó Langchain'})]"
  },
  {
    "objectID": "notebooks/06_multi_query_retriever.html#librer√≠as",
    "href": "notebooks/06_multi_query_retriever.html#librer√≠as",
    "title": "Multi-query retriever",
    "section": "Librer√≠as",
    "text": "Librer√≠as\n\nimport logging\nfrom typing import Any\n\nfrom dotenv import load_dotenv\nfrom langchain.chains import LLMChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.output_parsers import PydanticOutputParser\nfrom langchain.prompts import PromptTemplate\nfrom langchain.retrievers import MultiQueryRetriever\nfrom langchain.vectorstores import Chroma\nfrom pydantic import BaseModel, Field\n\nfrom src.langchain_docs_loader import load_langchain_docs_splitted\n\nload_dotenv()\n\nTrue"
  },
  {
    "objectID": "notebooks/06_multi_query_retriever.html#carga-de-datos",
    "href": "notebooks/06_multi_query_retriever.html#carga-de-datos",
    "title": "Multi-query retriever",
    "section": "Carga de datos",
    "text": "Carga de datos\n\ndocs = load_langchain_docs_splitted()\nlen(docs)\n\n2692"
  },
  {
    "objectID": "notebooks/06_multi_query_retriever.html#preparaci√≥n-de-vectorstore",
    "href": "notebooks/06_multi_query_retriever.html#preparaci√≥n-de-vectorstore",
    "title": "Multi-query retriever",
    "section": "Preparaci√≥n de vectorstore",
    "text": "Preparaci√≥n de vectorstore\n\nembedding = OpenAIEmbeddings()\nvectorstore = Chroma.from_documents(documents=docs, embedding=embedding)\n\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 722919 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 614137 / min. Contact us through our help center at help.openai.com if you continue to have issues.."
  },
  {
    "objectID": "notebooks/06_multi_query_retriever.html#preparaci√≥n-de-retriever",
    "href": "notebooks/06_multi_query_retriever.html#preparaci√≥n-de-retriever",
    "title": "Multi-query retriever",
    "section": "Preparaci√≥n de retriever",
    "text": "Preparaci√≥n de retriever\n\nllm = ChatOpenAI(temperature=0)\nretriever = MultiQueryRetriever.from_llm(\n    retriever=vectorstore.as_retriever(),\n    llm=llm,\n)\n\n\nlogging.basicConfig()\nlogging.getLogger(\"langchain.retrievers.multy_query\").setLevel(logging.INFO)"
  },
  {
    "objectID": "notebooks/06_multi_query_retriever.html#prueba-de-retriever",
    "href": "notebooks/06_multi_query_retriever.html#prueba-de-retriever",
    "title": "Multi-query retriever",
    "section": "Prueba de retriever",
    "text": "Prueba de retriever\n\nretriever.get_relevant_documents(\n    \"How to create a retriever with langchain expression language?\"\n)\n\n[Document(page_content='# Code understanding\\n\\nOverview\\n\\nLangChain is a useful tool designed to parse GitHub code repositories. By leveraging VectorStores, Conversational RetrieverChain, and GPT-4, it can answer questions in the context of an entire GitHub repository or generate new code. This documentation page outlines the essential components of the system and guides using LangChain for better code comprehension, contextual question answering, and code generation in GitHub repositories.\\n\\n## Conversational Retriever Chain\\u200b\\n\\nConversational RetrieverChain is a retrieval-focused system that interacts with the data stored in a VectorStore. Utilizing advanced techniques, like context-aware filtering and ranking, it retrieves the most relevant code snippets and information for a given user query. Conversational RetrieverChain is engineered to deliver high-quality, pertinent results while considering conversation history and context.\\n\\nLangChain Workflow for Code Understanding and Generation\\n\\n1. Index the code base: Clone the target repository, load all files within, chunk the files, and execute the indexing process. Optionally, you can skip this step and use an already indexed dataset.\\n\\n\\n\\n2. Embedding and Code Store: Code snippets are embedded using a code-aware embedding model and stored in a VectorStore.\\nQuery Understanding: GPT-4 processes user queries, grasping the context and extracting relevant details.\\n\\n\\n\\n3. Construct the Retriever: Conversational RetrieverChain searches the VectorStore to identify the most relevant code snippets for a given query.\\n\\n\\n\\n4. Build the Conversational Chain: Customize the retriever settings and define any user-defined filters as needed. \\n\\n\\n\\n5. Ask questions: Define a list of questions to ask about the codebase, and then use the ConversationalRetrievalChain to generate context-aware answers. The LLM (GPT-4) generates comprehensive, context-aware answers based on retrieved code snippets and conversation history.\\n\\n\\n\\nThe full tutorial is available below.\\n\\n- [Twitter the-algorithm codebase analysis with Deep Lake](/docs/use_cases/question_answering/how_to/code/twitter-the-algorithm-analysis-deeplake.html): A notebook walking through how to parse github source code and run queries conversation.\\n\\n- [LangChain codebase analysis with Deep Lake](/docs/use_cases/question_answering/how_to/code/code-analysis-deeplake.html): A notebook walking through how to analyze and do question answering over THIS code base.', metadata={'description': 'Overview', 'language': 'en', 'source': 'https://python.langchain.com/docs/use_cases/question_answering/how_to/code/', 'title': 'Code understanding | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='# Cookbook\\n\\nExample code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you\\'re just getting acquainted with LCEL, the [Prompt + LLM](/docs/expression_language/cookbook/prompt_llm_parser) page is a good place to start.\\n\\n[üìÑÔ∏è Prompt + LLMThe most common and valuable composition is taking:](/docs/expression_language/cookbook/prompt_llm_parser)[üìÑÔ∏è RAGLet\\'s look at adding in a retrieval step to a prompt and LLM, which adds up to a \"retrieval-augmented generation\" chain](/docs/expression_language/cookbook/retrieval)[üìÑÔ∏è Multiple chainsRunnables can easily be used to string together multiple Chains](/docs/expression_language/cookbook/multiple_chains)[üìÑÔ∏è Querying a SQL DBWe can replicate our SQLDatabaseChain with Runnables.](/docs/expression_language/cookbook/sql_db)[üìÑÔ∏è AgentsYou can pass a Runnable into an agent.](/docs/expression_language/cookbook/agent)[üìÑÔ∏è Code writingExample of how to use LCEL to write Python code.](/docs/expression_language/cookbook/code_writing)[üìÑÔ∏è Adding memoryThis shows how to add memory to an arbitrary chain. Right now, you can use the memory classes but need to hook it up manually](/docs/expression_language/cookbook/memory)[üìÑÔ∏è Adding moderationThis shows how to add in moderation (or other safeguards) around your LLM application.](/docs/expression_language/cookbook/moderation)[üìÑÔ∏è Using toolsYou can use any Tools with Runnables easily.](/docs/expression_language/cookbook/tools)', metadata={'description': \"Example code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the Prompt + LLM page is a good place to start.\", 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/cookbook/', 'title': 'Cookbook | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content=\"# Retrieval\\n\\nMany LLM applications require user-specific data that is not part of the model's training set.\\nThe primary way of accomplishing this is through Retrieval Augmented Generation (RAG).\\nIn this process, external data is _retrieved_ and then passed to the LLM when doing the _generation_ step.\\n\\nLangChain provides all the building blocks for RAG applications - from simple to complex.\\nThis section of the documentation covers everything related to the _retrieval_ step - e.g. the fetching of the data.\\nAlthough this sounds simple, it can be subtly complex.\\nThis encompasses several key modules.\\n\\n![data_connection_diagram](/assets/images/data_connection-c42d68c3d092b85f50d08d4cc171fc25.jpg)\\n\\n**Document loaders**\\n\\nLoad documents from many different sources.\\nLangChain provides over 100 different document loaders as well as integrations with other major providers in the space,\\nlike AirByte and Unstructured.\\nWe provide integrations to load all types of documents (HTML, PDF, code) from all types of locations (private s3 buckets, public websites).\\n\\n**Document transformers**\\n\\nA key part of retrieval is fetching only the relevant parts of documents.\\nThis involves several transformation steps in order to best prepare the documents for retrieval.\\nOne of the primary ones here is splitting (or chunking) a large document into smaller chunks.\\nLangChain provides several different algorithms for doing this, as well as logic optimized for specific document types (code, markdown, etc).\\n\\n**Text embedding models**\\n\\nAnother key part of retrieval has become creating embeddings for documents.\\nEmbeddings capture the semantic meaning of the text, allowing you to quickly and\\nefficiently find other pieces of text that are similar.\\nLangChain provides integrations with over 25 different embedding providers and methods,\\nfrom open-source to proprietary API,\\nallowing you to choose the one best suited for your needs.\\nLangChain provides a standard interface, allowing you to easily swap between models.\\n\\n**Vector stores**\\n\\nWith the rise of embeddings, there has emerged a need for databases to support efficient storage and searching of these embeddings.\\nLangChain provides integrations with over 50 different vectorstores, from open-source local ones to cloud-hosted proprietary ones,\\nallowing you to choose the one best suited for your needs.\\nLangChain exposes a standard interface, allowing you to easily swap between vector stores.\\n\\n**Retrievers**\\n\\nOnce the data is in the database, you still need to retrieve it.\\nLangChain supports many different retrieval algorithms and is one of the places where we add the most value.\\nWe support basic methods that are easy to get started - namely simple semantic search.\\nHowever, we have also added a collection of algorithms on top of this to increase performance.\\nThese include:\\n\\n- [Parent Document Retriever](/docs/modules/data_connection/retrievers/parent_document_retriever): This allows you to create multiple embeddings per parent document, allowing you to look up smaller chunks but return larger context.\\n\\n- [Self Query Retriever](/docs/modules/data_connection/retrievers/self_query): User questions often contain a reference to something that isn't just semantic but rather expresses some logic that can best be represented as a metadata filter. Self-query allows you to parse out the _semantic_ part of a query from other _metadata filters_ present in the query.\\n\\n- [Ensemble Retriever](/docs/modules/data_connection/retrievers/ensemble): Sometimes you may want to retrieve documents from multiple different sources, or using multiple different algorithms. The ensemble retriever allows you to easily do this.\\n\\n- And more!\", metadata={'description': \"Many LLM applications require user-specific data that is not part of the model's training set.\", 'language': 'en', 'source': 'https://python.langchain.com/docs/modules/data_connection/', 'title': 'Retrieval | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content=\"[üóÉÔ∏è Adapters1 items](/docs/guides/adapters/openai)[üìÑÔ∏è DebuggingIf you're building with LLMs, at some point something will break, and you'll need to debug. A model call will fail, or the model output will be misformatted, or there will be some nested model calls and it won't be clear where along the way an incorrect output was created.](/docs/guides/debugging)[üóÉÔ∏è Deployment1 items](/docs/guides/deployments/)[üóÉÔ∏è Evaluation4 items](/docs/guides/evaluation/)[üìÑÔ∏è FallbacksWhen working with language models, you may often encounter issues from the underlying APIs, whether these be rate limiting or downtime. Therefore, as you go to move your LLM applications into production it becomes more and more important to safe guard against these. That's why we've introduced the concept of fallbacks.](/docs/guides/fallbacks)[üóÉÔ∏è LangSmith1 items](/docs/guides/langsmith/)[üìÑÔ∏è Run LLMs locallyUse case](/docs/guides/local_llms)[üìÑÔ∏è Model comparisonConstructing your language model application will likely involved choosing between many different options of prompts, models, and even chains to use. When doing so, you will want to compare these different options on different inputs in an easy, flexible, and intuitive way.](/docs/guides/model_laboratory)[üóÉÔ∏è Privacy1 items](/docs/guides/privacy/presidio_data_anonymization/)[üìÑÔ∏è Pydantic compatibility- Pydantic v2 was released in June, 2023 (https://docs.pydantic.dev/2.0/blog/pydantic-v2-final/)](/docs/guides/pydantic_compatibility)[üóÉÔ∏è Safety5 items](/docs/guides/safety/)\", metadata={'description': 'Design guides for key parts of the development process', 'language': 'en', 'source': 'https://python.langchain.com/docs/guides', 'title': 'Guides | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='PubMedPubMed¬Æ by The National Center for Biotechnology Information, National Library of Medicine comprises more than 35 million citations for biomedical literature from MEDLINE, life science journals, and online books. Citations may include links to full text content from PubMed Central and publisher web sites.](/docs/integrations/retrievers/pubmed)[üìÑÔ∏è RePhraseQueryRetrieverSimple retriever that applies an LLM between the user input and the query pass the to retriever.](/docs/integrations/retrievers/re_phrase)[üìÑÔ∏è SEC filings dataSEC filings data powered by Kay.ai and Cybersyn.](/docs/integrations/retrievers/sec_filings)[üìÑÔ∏è SVMSupport vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.](/docs/integrations/retrievers/svm)[üìÑÔ∏è TF-IDFTF-IDF means term-frequency times inverse document-frequency.](/docs/integrations/retrievers/tf_idf)[üìÑÔ∏è VespaVespa is a fully featured search engine and vector database. It supports vector search (ANN), lexical search, and search in structured data, all in the same query.](/docs/integrations/retrievers/vespa)[üìÑÔ∏è Weaviate Hybrid SearchWeaviate is an open source vector database.](/docs/integrations/retrievers/weaviate-hybrid)[üìÑÔ∏è WikipediaWikipedia is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. Wikipedia is the largest and most-read reference work in history.](/docs/integrations/retrievers/wikipedia)[üìÑÔ∏è ZepRetriever Example for Zep - A long-term memory store for LLM applications.](/docs/integrations/retrievers/zep_memorystore)', metadata={'language': 'en', 'source': 'https://python.langchain.com/docs/integrations/retrievers', 'title': 'Retrievers | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='#### API Reference:\\n\\n- [OpenAI](https://api.python.langchain.com/en/latest/llms/langchain.llms.openai.OpenAI.html)\\n\\n- [SelfQueryRetriever](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.self_query.base.SelfQueryRetriever.html)\\n\\n- [AttributeInfo](https://api.python.langchain.com/en/latest/chains/langchain.chains.query_constructor.schema.AttributeInfo.html)\\n\\n## Testing it out\\u200b\\n\\nAnd now we can try actually using our retriever!\\n\\n```python\\n# This example only specifies a relevant query\\nretriever.get_relevant_documents(\"What are some movies about dinosaurs\")\\n```\\n\\n```text\\n    query=\\'dinosaur\\' filter=None limit=None\\n\\n\\n\\n\\n\\n    [Document(page_content=\\'A bunch of scientists bring back dinosaurs and mayhem breaks loose\\', metadata={\\'year\\': 1993, \\'rating\\': 7.7, \\'genre\\': \\'science fiction\\'}),\\n     Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'year\\': 1995, \\'genre\\': \\'animated\\'}),\\n     Document(page_content=\\'Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\\', metadata={\\'year\\': 2010, \\'director\\': \\'Christopher Nolan\\', \\'rating\\': 8.2}),\\n     Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'year\\': 1979, \\'rating\\': 9.9, \\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'science fiction\\'})]\\n```\\n\\n```python\\n# This example only specifies a filter\\nretriever.get_relevant_documents(\"I want to watch a movie rated higher than 8.5\")\\n```\\n\\n```text\\n    query=\\' \\' filter=Comparison(comparator=&lt;Comparator.GT: \\'gt\\'&gt;, attribute=\\'rating\\', value=8.5) limit=None\\n\\n\\n\\n\\n\\n    [Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'year\\': 1979, \\'rating\\': 9.9, \\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'science fiction\\'}),\\n     Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'year\\': 2006, \\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6})]\\n```\\n\\n```python\\n# This example specifies a query and a filter\\nretriever.get_relevant_documents(\"Has Greta Gerwig directed any movies about women\")\\n```\\n\\n```text\\n    query=\\'women\\' filter=Comparison(comparator=&lt;Comparator.EQ: \\'eq\\'&gt;, attribute=\\'director\\', value=\\'Greta Gerwig\\') limit=None\\n\\n\\n\\n\\n\\n    [Document(page_content=\\'A bunch of normal-sized women are supremely wholesome and some men pine after them\\', metadata={\\'year\\': 2019, \\'director\\': \\'Greta Gerwig\\', \\'rating\\': 8.3})]\\n```\\n\\n```python\\n# This example specifies a composite filter\\nretriever.get_relevant_documents(\"What\\'s a highly rated (above 8.5) science fiction film?\")\\n```\\n\\n```text\\n    query=\\' \\' filter=Operation(operator=&lt;Operator.AND: \\'and\\'&gt;, arguments=[Comparison(comparator=&lt;Comparator.GTE: \\'gte\\'&gt;, attribute=\\'rating\\', value=8.5), Comparison(comparator=&lt;Comparator.CONTAIN: \\'contain\\'&gt;, attribute=\\'genre\\', value=\\'science fiction\\')]) limit=None\\n\\n\\n\\n\\n\\n    [Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'year\\': 1979, \\'rating\\': 9.9, \\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'science fiction\\'})]\\n```\\n\\n## Filter k\\u200b\\n\\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\\n\\nWe can do this by passing `enable_limit=True` to the constructor.\\n\\n```python\\nretriever = SelfQueryRetriever.from_llm(\\n    llm,\\n    vectorstore,\\n    document_content_description,\\n    metadata_field_info,\\n    enable_limit=True,\\n    verbose=True,\\n)\\n```\\n\\n```python\\n# This example only specifies a relevant query\\nretriever.get_relevant_documents(\"what are two movies about dinosaurs\")\\n```\\n\\n```text\\n    query=\\'dinosaur\\' filter=None limit=2\\n\\n\\n\\n\\n\\n    [Document(page_content=\\'A bunch of scientists bring back dinosaurs and mayhem breaks loose\\', metadata={\\'year\\': 1993, \\'rating\\': 7.7, \\'genre\\': \\'science fiction\\'}),\\n     Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'year\\': 1995, \\'genre\\': \\'animated\\'})]\\n```', metadata={'description': 'OpenSearch is a scalable, flexible, and extensible open-source software suite for search, analytics, and observability applications licensed under Apache 2.0. OpenSearch is a distributed search and analytics engine based on Apache Lucene.', 'language': 'en', 'source': 'https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/opensearch_self_query', 'title': 'OpenSearch | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='```text\\n     Based on the given context, here is the answer to the question \"What are the approaches to Task Decomposition?\"\\n    \\n    There are three approaches to task decomposition:\\n    \\n    1. LLM with simple prompting, such as \"Steps for XYZ.\" or \"What are the subgoals for achieving XYZ?\"\\n    2. Using task-specific instructions, like \"Write a story outline\" for writing a novel.', metadata={'description': 'Ollama allows you to run open-source large language models, such as LLaMA2, locally.', 'language': 'en', 'source': 'https://python.langchain.com/docs/integrations/chat/ollama', 'title': 'Ollama | ü¶úÔ∏èüîó Langchain'})]"
  },
  {
    "objectID": "notebooks/06_multi_query_retriever.html#generaci√≥n-de-preguntas-alternativas-de-forma-personalizada",
    "href": "notebooks/06_multi_query_retriever.html#generaci√≥n-de-preguntas-alternativas-de-forma-personalizada",
    "title": "Multi-query retriever",
    "section": "Generaci√≥n de preguntas alternativas de forma personalizada",
    "text": "Generaci√≥n de preguntas alternativas de forma personalizada\n\nDefinici√≥n de esquema de salida de preguntas\n\nclass LineList(BaseModel):\n    # \"lines\" is the key (attribute name) of the parsed output\n    lines: list[str] = Field(description=\"Lines of text\")\n\n\nclass LineListOutputParser(PydanticOutputParser[Any]):\n    def __init__(self) -&gt; None:\n        super().__init__(pydantic_object=LineList)\n\n    def parse(self, text: str) -&gt; LineList:\n        lines = text.strip().splitlines()\n        return LineList(lines=lines)\n\n\n\nCreaci√≥n de prompt personalizado\n\nprompt = PromptTemplate.from_template(\n    \"\"\"You are an AI language assistant well versed in the Langchain Documentation.\nYour more precise task is to generate five different versions of the given question to retrieve relevant documents from a vector database.\nBy generating multiple perspectives on the question, your goal is to overcome some of the limitations of the distance-based similarity search.\n\nProvide these alternative questions separed by newlines.\n\nOriginal question: {question}\nNew questions:\"\"\"\n)\n\nllm_chain = LLMChain(\n    llm=ChatOpenAI(temperature=0),\n    prompt=prompt,\n    output_parser=LineListOutputParser(),\n)\n\n# In language expression language, you could create the chain with:\n# llm_chain = prompt | llm | LineListOutputParser()\n\n\n\nUse de cadena de generaci√≥n de preguntas personalizada\n\nllm_chain.invoke(\n    {\"question\": \"How to create a retriever with langchain expression language?\"}\n)\n\n{'question': 'How to create a retriever with language expression language?',\n 'text': LineList(lines=['1. What are the steps to build a retriever using language expression language?', '2. Can you explain the process of creating a retriever using language expression language?', '3. What is the procedure for constructing a retriever with language expression language?', '4. How can I develop a retriever using language expression language?', '5. Are there any guidelines for building a retriever with language expression language?'])}\n\n\n\n\nIntegraci√≥n de cadena de generaci√≥n de preguntas personalizada en retriever\n\nretriever = MultiQueryRetriever(\n    retriever=vectorstore.as_retriever(),\n    llm_chain=llm_chain,\n    parser_key=\"lines\",\n)\n\n\n\nUso de retriever con cadena de generaci√≥n de preguntas personalizada\n\nretriever.get_relevant_documents(\n    \"How to create a retriever with lagnchain expression language?\"\n)\n\n[Document(page_content='# Code understanding\\n\\nOverview\\n\\nLangChain is a useful tool designed to parse GitHub code repositories. By leveraging VectorStores, Conversational RetrieverChain, and GPT-4, it can answer questions in the context of an entire GitHub repository or generate new code. This documentation page outlines the essential components of the system and guides using LangChain for better code comprehension, contextual question answering, and code generation in GitHub repositories.\\n\\n## Conversational Retriever Chain\\u200b\\n\\nConversational RetrieverChain is a retrieval-focused system that interacts with the data stored in a VectorStore. Utilizing advanced techniques, like context-aware filtering and ranking, it retrieves the most relevant code snippets and information for a given user query. Conversational RetrieverChain is engineered to deliver high-quality, pertinent results while considering conversation history and context.\\n\\nLangChain Workflow for Code Understanding and Generation\\n\\n1. Index the code base: Clone the target repository, load all files within, chunk the files, and execute the indexing process. Optionally, you can skip this step and use an already indexed dataset.\\n\\n\\n\\n2. Embedding and Code Store: Code snippets are embedded using a code-aware embedding model and stored in a VectorStore.\\nQuery Understanding: GPT-4 processes user queries, grasping the context and extracting relevant details.\\n\\n\\n\\n3. Construct the Retriever: Conversational RetrieverChain searches the VectorStore to identify the most relevant code snippets for a given query.\\n\\n\\n\\n4. Build the Conversational Chain: Customize the retriever settings and define any user-defined filters as needed. \\n\\n\\n\\n5. Ask questions: Define a list of questions to ask about the codebase, and then use the ConversationalRetrievalChain to generate context-aware answers. The LLM (GPT-4) generates comprehensive, context-aware answers based on retrieved code snippets and conversation history.\\n\\n\\n\\nThe full tutorial is available below.\\n\\n- [Twitter the-algorithm codebase analysis with Deep Lake](/docs/use_cases/question_answering/how_to/code/twitter-the-algorithm-analysis-deeplake.html): A notebook walking through how to parse github source code and run queries conversation.\\n\\n- [LangChain codebase analysis with Deep Lake](/docs/use_cases/question_answering/how_to/code/code-analysis-deeplake.html): A notebook walking through how to analyze and do question answering over THIS code base.', metadata={'description': 'Overview', 'language': 'en', 'source': 'https://python.langchain.com/docs/use_cases/question_answering/how_to/code/', 'title': 'Code understanding | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content=\"# Retrieval\\n\\nMany LLM applications require user-specific data that is not part of the model's training set.\\nThe primary way of accomplishing this is through Retrieval Augmented Generation (RAG).\\nIn this process, external data is _retrieved_ and then passed to the LLM when doing the _generation_ step.\\n\\nLangChain provides all the building blocks for RAG applications - from simple to complex.\\nThis section of the documentation covers everything related to the _retrieval_ step - e.g. the fetching of the data.\\nAlthough this sounds simple, it can be subtly complex.\\nThis encompasses several key modules.\\n\\n![data_connection_diagram](/assets/images/data_connection-c42d68c3d092b85f50d08d4cc171fc25.jpg)\\n\\n**Document loaders**\\n\\nLoad documents from many different sources.\\nLangChain provides over 100 different document loaders as well as integrations with other major providers in the space,\\nlike AirByte and Unstructured.\\nWe provide integrations to load all types of documents (HTML, PDF, code) from all types of locations (private s3 buckets, public websites).\\n\\n**Document transformers**\\n\\nA key part of retrieval is fetching only the relevant parts of documents.\\nThis involves several transformation steps in order to best prepare the documents for retrieval.\\nOne of the primary ones here is splitting (or chunking) a large document into smaller chunks.\\nLangChain provides several different algorithms for doing this, as well as logic optimized for specific document types (code, markdown, etc).\\n\\n**Text embedding models**\\n\\nAnother key part of retrieval has become creating embeddings for documents.\\nEmbeddings capture the semantic meaning of the text, allowing you to quickly and\\nefficiently find other pieces of text that are similar.\\nLangChain provides integrations with over 25 different embedding providers and methods,\\nfrom open-source to proprietary API,\\nallowing you to choose the one best suited for your needs.\\nLangChain provides a standard interface, allowing you to easily swap between models.\\n\\n**Vector stores**\\n\\nWith the rise of embeddings, there has emerged a need for databases to support efficient storage and searching of these embeddings.\\nLangChain provides integrations with over 50 different vectorstores, from open-source local ones to cloud-hosted proprietary ones,\\nallowing you to choose the one best suited for your needs.\\nLangChain exposes a standard interface, allowing you to easily swap between vector stores.\\n\\n**Retrievers**\\n\\nOnce the data is in the database, you still need to retrieve it.\\nLangChain supports many different retrieval algorithms and is one of the places where we add the most value.\\nWe support basic methods that are easy to get started - namely simple semantic search.\\nHowever, we have also added a collection of algorithms on top of this to increase performance.\\nThese include:\\n\\n- [Parent Document Retriever](/docs/modules/data_connection/retrievers/parent_document_retriever): This allows you to create multiple embeddings per parent document, allowing you to look up smaller chunks but return larger context.\\n\\n- [Self Query Retriever](/docs/modules/data_connection/retrievers/self_query): User questions often contain a reference to something that isn't just semantic but rather expresses some logic that can best be represented as a metadata filter. Self-query allows you to parse out the _semantic_ part of a query from other _metadata filters_ present in the query.\\n\\n- [Ensemble Retriever](/docs/modules/data_connection/retrievers/ensemble): Sometimes you may want to retrieve documents from multiple different sources, or using multiple different algorithms. The ensemble retriever allows you to easily do this.\\n\\n- And more!\", metadata={'description': \"Many LLM applications require user-specific data that is not part of the model's training set.\", 'language': 'en', 'source': 'https://python.langchain.com/docs/modules/data_connection/', 'title': 'Retrieval | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='# Cookbook\\n\\nExample code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you\\'re just getting acquainted with LCEL, the [Prompt + LLM](/docs/expression_language/cookbook/prompt_llm_parser) page is a good place to start.\\n\\n[üìÑÔ∏è Prompt + LLMThe most common and valuable composition is taking:](/docs/expression_language/cookbook/prompt_llm_parser)[üìÑÔ∏è RAGLet\\'s look at adding in a retrieval step to a prompt and LLM, which adds up to a \"retrieval-augmented generation\" chain](/docs/expression_language/cookbook/retrieval)[üìÑÔ∏è Multiple chainsRunnables can easily be used to string together multiple Chains](/docs/expression_language/cookbook/multiple_chains)[üìÑÔ∏è Querying a SQL DBWe can replicate our SQLDatabaseChain with Runnables.](/docs/expression_language/cookbook/sql_db)[üìÑÔ∏è AgentsYou can pass a Runnable into an agent.](/docs/expression_language/cookbook/agent)[üìÑÔ∏è Code writingExample of how to use LCEL to write Python code.](/docs/expression_language/cookbook/code_writing)[üìÑÔ∏è Adding memoryThis shows how to add memory to an arbitrary chain. Right now, you can use the memory classes but need to hook it up manually](/docs/expression_language/cookbook/memory)[üìÑÔ∏è Adding moderationThis shows how to add in moderation (or other safeguards) around your LLM application.](/docs/expression_language/cookbook/moderation)[üìÑÔ∏è Using toolsYou can use any Tools with Runnables easily.](/docs/expression_language/cookbook/tools)', metadata={'description': \"Example code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the Prompt + LLM page is a good place to start.\", 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/cookbook/', 'title': 'Cookbook | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content=\"[üóÉÔ∏è Adapters1 items](/docs/guides/adapters/openai)[üìÑÔ∏è DebuggingIf you're building with LLMs, at some point something will break, and you'll need to debug. A model call will fail, or the model output will be misformatted, or there will be some nested model calls and it won't be clear where along the way an incorrect output was created.](/docs/guides/debugging)[üóÉÔ∏è Deployment1 items](/docs/guides/deployments/)[üóÉÔ∏è Evaluation4 items](/docs/guides/evaluation/)[üìÑÔ∏è FallbacksWhen working with language models, you may often encounter issues from the underlying APIs, whether these be rate limiting or downtime. Therefore, as you go to move your LLM applications into production it becomes more and more important to safe guard against these. That's why we've introduced the concept of fallbacks.](/docs/guides/fallbacks)[üóÉÔ∏è LangSmith1 items](/docs/guides/langsmith/)[üìÑÔ∏è Run LLMs locallyUse case](/docs/guides/local_llms)[üìÑÔ∏è Model comparisonConstructing your language model application will likely involved choosing between many different options of prompts, models, and even chains to use. When doing so, you will want to compare these different options on different inputs in an easy, flexible, and intuitive way.](/docs/guides/model_laboratory)[üóÉÔ∏è Privacy1 items](/docs/guides/privacy/presidio_data_anonymization/)[üìÑÔ∏è Pydantic compatibility- Pydantic v2 was released in June, 2023 (https://docs.pydantic.dev/2.0/blog/pydantic-v2-final/)](/docs/guides/pydantic_compatibility)[üóÉÔ∏è Safety5 items](/docs/guides/safety/)\", metadata={'description': 'Design guides for key parts of the development process', 'language': 'en', 'source': 'https://python.langchain.com/docs/guides', 'title': 'Guides | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='PubMedPubMed¬Æ by The National Center for Biotechnology Information, National Library of Medicine comprises more than 35 million citations for biomedical literature from MEDLINE, life science journals, and online books. Citations may include links to full text content from PubMed Central and publisher web sites.](/docs/integrations/retrievers/pubmed)[üìÑÔ∏è RePhraseQueryRetrieverSimple retriever that applies an LLM between the user input and the query pass the to retriever.](/docs/integrations/retrievers/re_phrase)[üìÑÔ∏è SEC filings dataSEC filings data powered by Kay.ai and Cybersyn.](/docs/integrations/retrievers/sec_filings)[üìÑÔ∏è SVMSupport vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.](/docs/integrations/retrievers/svm)[üìÑÔ∏è TF-IDFTF-IDF means term-frequency times inverse document-frequency.](/docs/integrations/retrievers/tf_idf)[üìÑÔ∏è VespaVespa is a fully featured search engine and vector database. It supports vector search (ANN), lexical search, and search in structured data, all in the same query.](/docs/integrations/retrievers/vespa)[üìÑÔ∏è Weaviate Hybrid SearchWeaviate is an open source vector database.](/docs/integrations/retrievers/weaviate-hybrid)[üìÑÔ∏è WikipediaWikipedia is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. Wikipedia is the largest and most-read reference work in history.](/docs/integrations/retrievers/wikipedia)[üìÑÔ∏è ZepRetriever Example for Zep - A long-term memory store for LLM applications.](/docs/integrations/retrievers/zep_memorystore)', metadata={'language': 'en', 'source': 'https://python.langchain.com/docs/integrations/retrievers', 'title': 'Retrievers | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='# Try out the retriever with an example query\\nqa_chain(\"What can tenants do with signage on their properties?\")', metadata={'description': 'This notebook covers how to load documents from Docugami. It provides the advantages of using this system over alternative data loaders.', 'language': 'en', 'source': 'https://python.langchain.com/docs/integrations/document_loaders/docugami', 'title': 'Docugami | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='# username = \"davitbun\"  # replace with your username from app.activeloop.ai\\n# db = DeepLake(\\n#     dataset_path=f\"hub://{username}/twitter-algorithm\",\\n#     embedding_function=embeddings,\\n#     runtime={\"tensor_db\": True}\\n# )\\n# db.add_documents(texts)\\n```\\n\\n### 2. Question Answering on Twitter algorithm codebase\\u200b\\n\\nFirst load the dataset, construct the retriever, then construct the Conversational Chain\\n\\n```python\\ndb = DeepLake(\\n    dataset_path=f\"hub://{username}/twitter-algorithm\",\\n    read_only=True,\\n    embedding=embeddings,\\n)\\n```\\n\\n```text\\n    Deep Lake Dataset in hub://adilkhan/twitter-algorithm already exists, loading from the storage\\n```\\n\\n```python\\nretriever = db.as_retriever()\\nretriever.search_kwargs[\"distance_metric\"] = \"cos\"\\nretriever.search_kwargs[\"fetch_k\"] = 100\\nretriever.search_kwargs[\"maximal_marginal_relevance\"] = True\\nretriever.search_kwargs[\"k\"] = 10\\n```\\n\\nYou can also specify user defined functions using [Deep Lake filters](https://docs.deeplake.ai/en/latest/deeplake.core.dataset.html#deeplake.core.dataset.Dataset.filter)\\n\\n```python\\ndef filter(x):\\n    # filter based on source code\\n    if \"com.google\" in x[\"text\"].data()[\"value\"]:\\n        return False\\n\\n    # filter based on path e.g. extension\\n    metadata = x[\"metadata\"].data()[\"value\"]\\n    return \"scala\" in metadata[\"source\"] or \"py\" in metadata[\"source\"]\\n\\n\\n### turn on below for custom filtering\\n# retriever.search_kwargs[\\'filter\\'] = filter\\n```\\n\\n```python\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.chains import ConversationalRetrievalChain\\n\\nmodel = ChatOpenAI(model_name=\"gpt-3.5-turbo-0613\")  # switch to \\'gpt-4\\'\\nqa = ConversationalRetrievalChain.from_llm(model, retriever=retriever)\\n```', metadata={'description': \"In this tutorial, we are going to use Langchain + Activeloop's Deep Lake with GPT4 to analyze the code base of the twitter algorithm.\", 'language': 'en', 'source': 'https://python.langchain.com/docs/use_cases/question_answering/how_to/code/twitter-the-algorithm-analysis-deeplake', 'title': \"Analysis of Twitter the-algorithm source code with LangChain, GPT4 and Activeloop's Deep Lake | ü¶úÔ∏èüîó Langchain\"}),\n Document(page_content='```', metadata={'description': 'A SmartLLMChain is a form of self-critique chain that can help you if have particularly complex questions to answer. Instead of doing a single LLM pass, it instead performs these 3 steps:', 'language': 'en', 'source': 'https://python.langchain.com/docs/use_cases/more/self_check/smart_llm', 'title': 'How to use a SmartLLMChain | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='```', metadata={'language': 'en', 'source': 'https://python.langchain.com/docs/integrations/vectorstores/vearch', 'title': 'vearch | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='```', metadata={'description': 'This example demonstrates the use of the SQLDatabaseChain for answering questions over a SQL database.', 'language': 'en', 'source': 'https://python.langchain.com/docs/use_cases/qa_structured/integrations/sqlite', 'title': 'SQL Database Chain | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='```', metadata={'description': 'This notebook shows examples of how to use SearchApi to search the web. Go to https://www.searchapi.io/ to sign up for a free account and get API key.', 'language': 'en', 'source': 'https://python.langchain.com/docs/integrations/tools/searchapi', 'title': 'SearchApi | ü¶úÔ∏èüîó Langchain'})]"
  },
  {
    "objectID": "notebooks/07_ensemble_retriever.html#librer√≠as",
    "href": "notebooks/07_ensemble_retriever.html#librer√≠as",
    "title": "Ensemble Retriever",
    "section": "Librer√≠as",
    "text": "Librer√≠as\n\nfrom dotenv import load_dotenv\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.retrievers import BM25Retriever, EnsembleRetriever\nfrom langchain.vectorstores import Chroma\n\nfrom src.langchain_docs_loader import load_langchain_docs_splitted\n\nload_dotenv()\n\nTrue"
  },
  {
    "objectID": "notebooks/07_ensemble_retriever.html#carga-de-datos",
    "href": "notebooks/07_ensemble_retriever.html#carga-de-datos",
    "title": "Ensemble Retriever",
    "section": "Carga de datos",
    "text": "Carga de datos\n\ndocs = load_langchain_docs_splitted()"
  },
  {
    "objectID": "notebooks/07_ensemble_retriever.html#inicializaci√≥n-de-retrievers-independientes",
    "href": "notebooks/07_ensemble_retriever.html#inicializaci√≥n-de-retrievers-independientes",
    "title": "Ensemble Retriever",
    "section": "Inicializaci√≥n de retrievers independientes",
    "text": "Inicializaci√≥n de retrievers independientes\n\nbm25_retriever = BM25Retriever.from_documents(docs)\nbm25_retriever.k = 2\n\nvector_retriever = Chroma.from_documents(\n    docs, embedding=OpenAIEmbeddings()\n).as_retriever(search_kwargs={\"k\": 2})\n\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 834322 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 742721 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 652165 / min. Contact us through our help center at help.openai.com if you continue to have issues.."
  },
  {
    "objectID": "notebooks/07_ensemble_retriever.html#ensamblaje-de-retrievers",
    "href": "notebooks/07_ensemble_retriever.html#ensamblaje-de-retrievers",
    "title": "Ensemble Retriever",
    "section": "Ensamblaje de retrievers",
    "text": "Ensamblaje de retrievers\n\nensemble_retriever = EnsembleRetriever(\n    retrievers=[bm25_retriever, vector_retriever], weights=[0.5, 0.5]\n)\n\n\nensemble_retriever.get_relevant_documents(\n    \"¬øC√≥mo utilizar un retriever con langchain expression language?\"\n)\n\n[Document(page_content='# Code understanding\\n\\nOverview\\n\\nLangChain is a useful tool designed to parse GitHub code repositories. By leveraging VectorStores, Conversational RetrieverChain, and GPT-4, it can answer questions in the context of an entire GitHub repository or generate new code. This documentation page outlines the essential components of the system and guides using LangChain for better code comprehension, contextual question answering, and code generation in GitHub repositories.\\n\\n## Conversational Retriever Chain\\u200b\\n\\nConversational RetrieverChain is a retrieval-focused system that interacts with the data stored in a VectorStore. Utilizing advanced techniques, like context-aware filtering and ranking, it retrieves the most relevant code snippets and information for a given user query. Conversational RetrieverChain is engineered to deliver high-quality, pertinent results while considering conversation history and context.\\n\\nLangChain Workflow for Code Understanding and Generation\\n\\n1. Index the code base: Clone the target repository, load all files within, chunk the files, and execute the indexing process. Optionally, you can skip this step and use an already indexed dataset.\\n\\n2. Embedding and Code Store: Code snippets are embedded using a code-aware embedding model and stored in a VectorStore.\\nQuery Understanding: GPT-4 processes user queries, grasping the context and extracting relevant details.\\n\\n3. Construct the Retriever: Conversational RetrieverChain searches the VectorStore to identify the most relevant code snippets for a given query.\\n\\n4. Build the Conversational Chain: Customize the retriever settings and define any user-defined filters as needed. \\n\\n5. Ask questions: Define a list of questions to ask about the codebase, and then use the ConversationalRetrievalChain to generate context-aware answers. The LLM (GPT-4) generates comprehensive, context-aware answers based on retrieved code snippets and conversation history.\\n\\nThe full tutorial is available below.\\n\\n- [Twitter the-algorithm codebase analysis with Deep Lake](/docs/use_cases/question_answering/how_to/code/twitter-the-algorithm-analysis-deeplake.html): A notebook walking through how to parse github source code and run queries conversation.\\n- [LangChain codebase analysis with Deep Lake](/docs/use_cases/question_answering/how_to/code/code-analysis-deeplake.html): A notebook walking through how to analyze and do question answering over THIS code base.', metadata={'description': 'Overview', 'language': 'en', 'source': 'https://python.langchain.com/docs/use_cases/question_answering/how_to/code/', 'title': 'Code understanding | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='## Output\\u200b\\n\\nAfter translating a document, the result will be returned as a new document with the page_content translated into the target language\\n\\n```python\\ntranslated_document = await qa_translator.atransform_documents(documents)\\n```\\n\\n```python\\nprint(translated_document[0].page_content)\\n```\\n\\n```text\\n    [Generado con ChatGPT]\\n    \\n    Documento confidencial - Solo para uso interno\\n    \\n    Fecha: 1 de julio de 2023\\n    \\n    Asunto: Actualizaciones y discusiones sobre varios temas\\n    \\n    Estimado equipo,\\n    \\n    Espero que este correo electr√≥nico les encuentre bien. En este documento, me gustar√≠a proporcionarles algunas actualizaciones importantes y discutir varios temas que requieren nuestra atenci√≥n. Por favor, traten la informaci√≥n contenida aqu√≠ como altamente confidencial.\\n    \\n    Medidas de seguridad y privacidad\\n    Como parte de nuestro compromiso continuo para garantizar la seguridad y privacidad de los datos de nuestros clientes, hemos implementado medidas robustas en todos nuestros sistemas. Nos gustar√≠a elogiar a John Doe (correo electr√≥nico: john.doe@example.com) del departamento de TI por su diligente trabajo en mejorar nuestra seguridad de red. En adelante, recordamos amablemente a todos que se adhieran estrictamente a nuestras pol√≠ticas y directrices de protecci√≥n de datos. Adem√°s, si se encuentran con cualquier riesgo de seguridad o incidente potencial, por favor rep√≥rtelo inmediatamente a nuestro equipo dedicado en security@example.com.\\n    \\n    Actualizaciones de RRHH y beneficios para empleados\\n    Recientemente, dimos la bienvenida a varios nuevos miembros del equipo que han hecho contribuciones significativas a sus respectivos departamentos. Me gustar√≠a reconocer a Jane Smith (SSN: 049-45-5928) por su sobresaliente rendimiento en el servicio al cliente. Jane ha recibido constantemente comentarios positivos de nuestros clientes. Adem√°s, recuerden que el per√≠odo de inscripci√≥n abierta para nuestro programa de beneficios para empleados se acerca r√°pidamente. Si tienen alguna pregunta o necesitan asistencia, por favor contacten a nuestro representante de RRHH, Michael Johnson (tel√©fono: 418-492-3850, correo electr√≥nico: michael.johnson@example.com).\\n    \\n    Iniciativas y campa√±as de marketing\\n    Nuestro equipo de marketing ha estado trabajando activamente en el desarrollo de nuevas estrategias para aumentar la conciencia de marca y fomentar la participaci√≥n del cliente. Nos gustar√≠a agradecer a Sarah Thompson (tel√©fono: 415-555-1234) por sus excepcionales esfuerzos en la gesti√≥n de nuestras plataformas de redes sociales. Sarah ha aumentado con √©xito nuestra base de seguidores en un 20% solo en el √∫ltimo mes. Adem√°s, por favor marquen sus calendarios para el pr√≥ximo evento de lanzamiento de producto el 15 de julio. Animamos a todos los miembros del equipo a asistir y apoyar este emocionante hito para nuestra empresa.\\n    \\n    Proyectos de investigaci√≥n y desarrollo\\n    En nuestra b√∫squeda de la innovaci√≥n, nuestro departamento de investigaci√≥n y desarrollo ha estado trabajando incansablemente en varios proyectos. Me gustar√≠a reconocer el excepcional trabajo de David Rodr√≠guez (correo electr√≥nico: david.rodriguez@example.com) en su papel de l√≠der de proyecto. Las contribuciones de David al desarrollo de nuestra tecnolog√≠a de vanguardia han sido fundamentales. Adem√°s, nos gustar√≠a recordar a todos que compartan sus ideas y sugerencias para posibles nuevos proyectos durante nuestra sesi√≥n de lluvia de ideas de I+D mensual, programada para el 10 de julio.\\n    \\n    Por favor, traten la informaci√≥n de este documento con la m√°xima confidencialidad y aseg√∫rense de que no se comparte con personas no autorizadas. Si tienen alguna pregunta o inquietud sobre los temas discutidos, no duden en ponerse en contacto conmigo directamente.\\n    \\n    Gracias por su atenci√≥n, y sigamos trabajando juntos para alcanzar nuestros objetivos.\\n    \\n    Saludos cordiales,\\n    \\n    Jason Fan\\n    Cofundador y CEO\\n    Psychic\\n    jason@psychic.dev\\n```', metadata={'source': 'https://python.langchain.com/docs/integrations/document_transformers/doctran_translate_document', 'title': 'Doctran: language translation | ü¶úÔ∏èüîó Langchain', 'description': 'Comparing documents through embeddings has the benefit of working across multiple languages. \"Harrison says hello\" and \"Harrison dice hola\" will occupy similar positions in the vector space because they have the same meaning semantically.', 'language': 'en'}),\n Document(page_content='# Tagging\\n\\n[](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/extras/use_cases/tagging.ipynb)\\n\\n## Use case\\u200b\\n\\nTagging means labeling a document with classes such as:\\n\\n- sentiment\\n- language\\n- style (formal, informal etc.)\\n- covered topics\\n- political tendency\\n\\n![Image description](/assets/images/tagging-93990e95451d92b715c2b47066384224.png)\\n\\n## Overview\\u200b\\n\\nTagging has a few components:\\n\\n- `function`: Like [extraction](/docs/use_cases/extraction), tagging uses [functions](https://openai.com/blog/function-calling-and-other-api-updates) to specify how the model should tag a document\\n- `schema`: defines how we want to tag the document\\n\\n## Quickstart\\u200b\\n\\nLet\\'s see a very straightforward example of how we can use OpenAI functions for tagging in LangChain.\\n\\n```bash\\npip install langchain openai \\n\\n# Set env var OPENAI_API_KEY or load from a .env file:\\n# import dotenv\\n# dotenv.load_dotenv()\\n```\\n\\n```python\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.chains import create_tagging_chain, create_tagging_chain_pydantic\\n```\\n\\n&gt; **API Reference:**\\n&gt; - [ChatOpenAI](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html)\\n&gt; - [ChatPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.ChatPromptTemplate.html)\\n&gt; - [create_tagging_chain](https://api.python.langchain.com/en/latest/chains/langchain.chains.openai_functions.tagging.create_tagging_chain.html)\\n&gt; - [create_tagging_chain_pydantic](https://api.python.langchain.com/en/latest/chains/langchain.chains.openai_functions.tagging.create_tagging_chain_pydantic.html)\\n\\nWe specify a few properties with their expected type in our schema.\\n\\n```python\\n# Schema\\nschema = {\\n    \"properties\": {\\n        \"sentiment\": {\"type\": \"string\"},\\n        \"aggressiveness\": {\"type\": \"integer\"},\\n        \"language\": {\"type\": \"string\"},\\n    }\\n}\\n\\n# LLM\\nllm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\\nchain = create_tagging_chain(schema, llm)\\n```\\n\\n```python\\ninp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\"\\nchain.run(inp)\\n```\\n\\n```text\\n    {\\'sentiment\\': \\'positive\\', \\'language\\': \\'Spanish\\'}\\n```\\n\\n```python\\ninp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\"\\nchain.run(inp)\\n```\\n\\n```text\\n    {\\'sentiment\\': \\'enojado\\', \\'aggressiveness\\': 1, \\'language\\': \\'es\\'}\\n```\\n\\nAs we can see in the examples, it correctly interprets what we want.\\n\\nThe results vary so that we get, for example, sentiments in different languages (\\'positive\\', \\'enojado\\' etc.).\\n\\nWe will see how to control these results in the next section.', metadata={'source': 'https://python.langchain.com/docs/use_cases/tagging', 'title': 'Tagging | ü¶úÔ∏èüîó Langchain', 'description': 'Open In Collab', 'language': 'en'}),\n Document(page_content=\"# Retrieval\\n\\nMany LLM applications require user-specific data that is not part of the model's training set.\\nThe primary way of accomplishing this is through Retrieval Augmented Generation (RAG).\\nIn this process, external data is _retrieved_ and then passed to the LLM when doing the _generation_ step.\\n\\nLangChain provides all the building blocks for RAG applications - from simple to complex.\\nThis section of the documentation covers everything related to the _retrieval_ step - e.g. the fetching of the data.\\nAlthough this sounds simple, it can be subtly complex.\\nThis encompasses several key modules.\\n\\n![data_connection_diagram](/assets/images/data_connection-c42d68c3d092b85f50d08d4cc171fc25.jpg)\\n\\n**Document loaders**\\n\\nLoad documents from many different sources.\\nLangChain provides over 100 different document loaders as well as integrations with other major providers in the space,\\nlike AirByte and Unstructured.\\nWe provide integrations to load all types of documents (HTML, PDF, code) from all types of locations (private s3 buckets, public websites).\\n\\n**Document transformers**\\n\\nA key part of retrieval is fetching only the relevant parts of documents.\\nThis involves several transformation steps in order to best prepare the documents for retrieval.\\nOne of the primary ones here is splitting (or chunking) a large document into smaller chunks.\\nLangChain provides several different algorithms for doing this, as well as logic optimized for specific document types (code, markdown, etc).\\n\\n**Text embedding models**\\n\\nAnother key part of retrieval has become creating embeddings for documents.\\nEmbeddings capture the semantic meaning of the text, allowing you to quickly and\\nefficiently find other pieces of text that are similar.\\nLangChain provides integrations with over 25 different embedding providers and methods,\\nfrom open-source to proprietary API,\\nallowing you to choose the one best suited for your needs.\\nLangChain provides a standard interface, allowing you to easily swap between models.\\n\\n**Vector stores**\\n\\nWith the rise of embeddings, there has emerged a need for databases to support efficient storage and searching of these embeddings.\\nLangChain provides integrations with over 50 different vectorstores, from open-source local ones to cloud-hosted proprietary ones,\\nallowing you to choose the one best suited for your needs.\\nLangChain exposes a standard interface, allowing you to easily swap between vector stores.\\n\\n**Retrievers**\\n\\nOnce the data is in the database, you still need to retrieve it.\\nLangChain supports many different retrieval algorithms and is one of the places where we add the most value.\\nWe support basic methods that are easy to get started - namely simple semantic search.\\nHowever, we have also added a collection of algorithms on top of this to increase performance.\\nThese include:\\n\\n- [Parent Document Retriever](/docs/modules/data_connection/retrievers/parent_document_retriever): This allows you to create multiple embeddings per parent document, allowing you to look up smaller chunks but return larger context.\\n- [Self Query Retriever](/docs/modules/data_connection/retrievers/self_query): User questions often contain a reference to something that isn't just semantic but rather expresses some logic that can best be represented as a metadata filter. Self-query allows you to parse out the _semantic_ part of a query from other _metadata filters_ present in the query.\\n- [Ensemble Retriever](/docs/modules/data_connection/retrievers/ensemble): Sometimes you may want to retrieve documents from multiple different sources, or using multiple different algorithms. The ensemble retriever allows you to easily do this.\\n- And more!\", metadata={'description': \"Many LLM applications require user-specific data that is not part of the model's training set.\", 'language': 'en', 'source': 'https://python.langchain.com/docs/modules/data_connection/', 'title': 'Retrieval | ü¶úÔ∏èüîó Langchain'})]"
  },
  {
    "objectID": "notebooks/09_maximal_marginal_relevance_reranking.html#librer√≠as",
    "href": "notebooks/09_maximal_marginal_relevance_reranking.html#librer√≠as",
    "title": "Re-ranking por relevancia marginal m√°xima (MMR)",
    "section": "Librer√≠as",
    "text": "Librer√≠as\n\nfrom dotenv import load_dotenv\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\n\nfrom src.langchain_docs_loader import load_langchain_docs_splitted\n\nload_dotenv()\n\nTrue"
  },
  {
    "objectID": "notebooks/09_maximal_marginal_relevance_reranking.html#carga-de-datos",
    "href": "notebooks/09_maximal_marginal_relevance_reranking.html#carga-de-datos",
    "title": "Re-ranking por relevancia marginal m√°xima (MMR)",
    "section": "Carga de datos",
    "text": "Carga de datos\n\ndocs = load_langchain_docs_splitted()"
  },
  {
    "objectID": "notebooks/09_maximal_marginal_relevance_reranking.html#creaci√≥n-de-retriever",
    "href": "notebooks/09_maximal_marginal_relevance_reranking.html#creaci√≥n-de-retriever",
    "title": "Re-ranking por relevancia marginal m√°xima (MMR)",
    "section": "Creaci√≥n de retriever",
    "text": "Creaci√≥n de retriever\nNormalmente creamos nuestro retriever de la siguiente manera:\n\nsimilarity_retriever = Chroma.from_documents(\n    documents=docs,\n    embedding=OpenAIEmbeddings(),\n).as_retriever(k=4)\n\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 792024 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 699650 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 606969 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n\n\nSin embargo, podr√°s notar que de hacerlo, el tipo de b√∫squeda que se realiza es por similitud de vectores. En este caso, queremos realizar una b√∫squeda por similitud de vectores, pero con un re-ranking por relevancia marginal m√°xima (MMR).\n\nsimilarity_retriever.search_type\n\n'similarity'\n\n\nEntonces, para crear un retriever con re-ranking por MMR, debemos hacer lo siguiente:\n\nmmr_retriever = Chroma.from_documents(\n    documents=docs,\n    embedding=OpenAIEmbeddings(),\n).as_retriever(\n    search_type=\"mmr\",\n    k=4,  # number of documents to retrieve after mmr\n    fetch_k=20,  # number of documents to fetch in the first step\n    # Lambda mult is a number between 0 and 1 that determines the degree\n    # of diversity among the results with 0 corresponding to maximum diversity\n    # and 1 to minimum diversity.\n    lambda_mult=0.5,\n)\n\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 768011 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 666659 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 558623 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 858146 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 759714 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 656845 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 554661 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 823705 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 731885 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 633547 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n\n\nAhora nuestro retriever est√° listo para ser utilizado con re-ranking por MMR.\n\nmmr_retriever.search_type\n\n'mmr'"
  },
  {
    "objectID": "notebooks/09_maximal_marginal_relevance_reranking.html#uso-del-retriever",
    "href": "notebooks/09_maximal_marginal_relevance_reranking.html#uso-del-retriever",
    "title": "Re-ranking por relevancia marginal m√°xima (MMR)",
    "section": "Uso del retriever",
    "text": "Uso del retriever\n\nsimilarity_retriever.get_relevant_documents(\n    \"How to integrate LCEL into my Retrieval augmented generation system with a keyword search retriever?\"\n)\n\n[Document(page_content=\"# Retrieval\\n\\nMany LLM applications require user-specific data that is not part of the model's training set.\\nThe primary way of accomplishing this is through Retrieval Augmented Generation (RAG).\\nIn this process, external data is _retrieved_ and then passed to the LLM when doing the _generation_ step.\\n\\nLangChain provides all the building blocks for RAG applications - from simple to complex.\\nThis section of the documentation covers everything related to the _retrieval_ step - e.g. the fetching of the data.\\nAlthough this sounds simple, it can be subtly complex.\\nThis encompasses several key modules.\\n\\n![data_connection_diagram](/assets/images/data_connection-c42d68c3d092b85f50d08d4cc171fc25.jpg)\\n\\n**Document loaders**\\n\\nLoad documents from many different sources.\\nLangChain provides over 100 different document loaders as well as integrations with other major providers in the space,\\nlike AirByte and Unstructured.\\nWe provide integrations to load all types of documents (HTML, PDF, code) from all types of locations (private s3 buckets, public websites).\\n\\n**Document transformers**\\n\\nA key part of retrieval is fetching only the relevant parts of documents.\\nThis involves several transformation steps in order to best prepare the documents for retrieval.\\nOne of the primary ones here is splitting (or chunking) a large document into smaller chunks.\\nLangChain provides several different algorithms for doing this, as well as logic optimized for specific document types (code, markdown, etc).\\n\\n**Text embedding models**\\n\\nAnother key part of retrieval has become creating embeddings for documents.\\nEmbeddings capture the semantic meaning of the text, allowing you to quickly and\\nefficiently find other pieces of text that are similar.\\nLangChain provides integrations with over 25 different embedding providers and methods,\\nfrom open-source to proprietary API,\\nallowing you to choose the one best suited for your needs.\\nLangChain provides a standard interface, allowing you to easily swap between models.\\n\\n**Vector stores**\\n\\nWith the rise of embeddings, there has emerged a need for databases to support efficient storage and searching of these embeddings.\\nLangChain provides integrations with over 50 different vectorstores, from open-source local ones to cloud-hosted proprietary ones,\\nallowing you to choose the one best suited for your needs.\\nLangChain exposes a standard interface, allowing you to easily swap between vector stores.\\n\\n**Retrievers**\\n\\nOnce the data is in the database, you still need to retrieve it.\\nLangChain supports many different retrieval algorithms and is one of the places where we add the most value.\\nWe support basic methods that are easy to get started - namely simple semantic search.\\nHowever, we have also added a collection of algorithms on top of this to increase performance.\\nThese include:\\n\\n- [Parent Document Retriever](/docs/modules/data_connection/retrievers/parent_document_retriever): This allows you to create multiple embeddings per parent document, allowing you to look up smaller chunks but return larger context.\\n- [Self Query Retriever](/docs/modules/data_connection/retrievers/self_query): User questions often contain a reference to something that isn't just semantic but rather expresses some logic that can best be represented as a metadata filter. Self-query allows you to parse out the _semantic_ part of a query from other _metadata filters_ present in the query.\\n- [Ensemble Retriever](/docs/modules/data_connection/retrievers/ensemble): Sometimes you may want to retrieve documents from multiple different sources, or using multiple different algorithms. The ensemble retriever allows you to easily do this.\\n- And more!\", metadata={'description': \"Many LLM applications require user-specific data that is not part of the model's training set.\", 'language': 'en', 'source': 'https://python.langchain.com/docs/modules/data_connection/', 'title': 'Retrieval | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='PubMedPubMed¬Æ by The National Center for Biotechnology Information, National Library of Medicine comprises more than 35 million citations for biomedical literature from MEDLINE, life science journals, and online books. Citations may include links to full text content from PubMed Central and publisher web sites.](/docs/integrations/retrievers/pubmed)[üìÑÔ∏è RePhraseQueryRetrieverSimple retriever that applies an LLM between the user input and the query pass the to retriever.](/docs/integrations/retrievers/re_phrase)[üìÑÔ∏è SEC filings dataSEC filings data powered by Kay.ai and Cybersyn.](/docs/integrations/retrievers/sec_filings)[üìÑÔ∏è SVMSupport vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.](/docs/integrations/retrievers/svm)[üìÑÔ∏è TF-IDFTF-IDF means term-frequency times inverse document-frequency.](/docs/integrations/retrievers/tf_idf)[üìÑÔ∏è VespaVespa is a fully featured search engine and vector database. It supports vector search (ANN), lexical search, and search in structured data, all in the same query.](/docs/integrations/retrievers/vespa)[üìÑÔ∏è Weaviate Hybrid SearchWeaviate is an open source vector database.](/docs/integrations/retrievers/weaviate-hybrid)[üìÑÔ∏è WikipediaWikipedia is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. Wikipedia is the largest and most-read reference work in history.](/docs/integrations/retrievers/wikipedia)[üìÑÔ∏è ZepRetriever Example for Zep - A long-term memory store for LLM applications.](/docs/integrations/retrievers/zep_memorystore)', metadata={'language': 'en', 'source': 'https://python.langchain.com/docs/integrations/retrievers', 'title': 'Retrievers | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='### Going deeper\\u200b\\n\\n- Agents, such as the [conversational retrieval agent](/docs/use_cases/question_answering/how_to/conversational_retrieval_agents), can be used for retrieval when necessary while also holding a conversation.', metadata={'description': 'Open In Collab', 'language': 'en', 'source': 'https://python.langchain.com/docs/use_cases/chatbots', 'title': 'Chatbots | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='## LLMRails as a Retriever\\u200b\\n\\nLLMRails, as all the other LangChain vectorstores, is most often used as a LangChain Retriever:\\n\\n```python\\nretriever = llm_rails.as_retriever()\\nretriever\\n```\\n\\n```text\\n    LLMRailsRetriever(tags=None, metadata=None, vectorstore=&lt;langchain.vectorstores.llm_rails.LLMRails object at 0x107b9c040&gt;, search_type=\\'similarity\\', search_kwargs={\\'k\\': 5})\\n```\\n\\n```python\\nquery = \"What is your approach to national defense\"\\nretriever.get_relevant_documents(query)[0]\\n```\\n\\n```text\\n    Document(page_content=\\'But we will do so as the last resort and only when the objectives and mission are clear and achievable, consistent with our values and laws, alongside non-military tools, and the mission is undertaken with the informed consent of the American people.\\\\n\\\\nOur approach to national defense is described in detail in the 2022 National Defense Strategy.\\\\n\\\\nOur starting premise is that a powerful U.S. military helps advance and safeguard vital U.S. national interests by backstopping diplomacy, confronting aggression, deterring conflict, projecting strength, and protecting the American people and their economic interests.\\\\n\\\\nAmid intensifying competition, the military‚Äôs role is to maintain and gain warfighting advantages while limiting those of our competitors.\\\\n\\\\nThe military will act urgently to sustain and strengthen deterrence, with the PRC as its pacing challenge.\\\\n\\\\nWe will make disciplined choices regarding our national defense and focus our attention on the military‚Äôs primary responsibilities: to defend the homeland, and deter attacks and aggression against the United States, our allies and partners, while being prepared to fight and win the Nation‚Äôs wars should diplomacy and deterrence fail.\\\\n\\\\nTo do so, we will combine our strengths to achieve maximum effect in deterring acts of aggression‚Äîan approach we refer to as integrated deterrence (see text box on page 22).\\\\n\\\\nWe will operate our military using a campaigning mindset‚Äîsequencing logically linked military activities to advance strategy-aligned priorities.\\\\n\\\\nAnd, we will build a resilient force and defense ecosystem to ensure we can perform these functions for decades to come.\\\\n\\\\nWe ended America‚Äôs longest war in Afghanistan, and with it an era of major military operations to remake other societies, even as we have maintained the capacity to address terrorist threats to the American people as they emerge.\\\\n\\\\n20  NATIONAL SECURITY STRATEGY Page 21 \\\\x90\\\\x90\\\\x90\\\\x90\\\\x90\\\\x90\\\\n\\\\nA combat-credible military is the foundation of deterrence and America‚Äôs ability to prevail in conflict.\\', metadata={\\'type\\': \\'file\\', \\'url\\': \\'https://cdn.llmrails.com/dst_d94b490c-4638-4247-ad5e-9aa0e7ef53c1/c2d63a2ea3cd406cb522f8312bc1535d\\', \\'name\\': \\'Biden-Harris-Administrations-National-Security-Strategy-10.2022.pdf\\'})\\n```', metadata={'description': 'LLMRails is a API platform for building GenAI applications. It provides an easy-to-use API for document indexing and querying that is managed by LLMRails and is optimized for performance and accuracy.', 'language': 'en', 'source': 'https://python.langchain.com/docs/integrations/vectorstores/llm_rails', 'title': 'LLMRails | ü¶úÔ∏èüîó Langchain'})]\n\n\n\nmmr_retriever.get_relevant_documents(\n    \"How to integrate LCEL into my Retrieval augmented generation system with a keyword search retriever?\"\n)\n\n[Document(page_content=\"# Retrieval\\n\\nMany LLM applications require user-specific data that is not part of the model's training set.\\nThe primary way of accomplishing this is through Retrieval Augmented Generation (RAG).\\nIn this process, external data is _retrieved_ and then passed to the LLM when doing the _generation_ step.\\n\\nLangChain provides all the building blocks for RAG applications - from simple to complex.\\nThis section of the documentation covers everything related to the _retrieval_ step - e.g. the fetching of the data.\\nAlthough this sounds simple, it can be subtly complex.\\nThis encompasses several key modules.\\n\\n![data_connection_diagram](/assets/images/data_connection-c42d68c3d092b85f50d08d4cc171fc25.jpg)\\n\\n**Document loaders**\\n\\nLoad documents from many different sources.\\nLangChain provides over 100 different document loaders as well as integrations with other major providers in the space,\\nlike AirByte and Unstructured.\\nWe provide integrations to load all types of documents (HTML, PDF, code) from all types of locations (private s3 buckets, public websites).\\n\\n**Document transformers**\\n\\nA key part of retrieval is fetching only the relevant parts of documents.\\nThis involves several transformation steps in order to best prepare the documents for retrieval.\\nOne of the primary ones here is splitting (or chunking) a large document into smaller chunks.\\nLangChain provides several different algorithms for doing this, as well as logic optimized for specific document types (code, markdown, etc).\\n\\n**Text embedding models**\\n\\nAnother key part of retrieval has become creating embeddings for documents.\\nEmbeddings capture the semantic meaning of the text, allowing you to quickly and\\nefficiently find other pieces of text that are similar.\\nLangChain provides integrations with over 25 different embedding providers and methods,\\nfrom open-source to proprietary API,\\nallowing you to choose the one best suited for your needs.\\nLangChain provides a standard interface, allowing you to easily swap between models.\\n\\n**Vector stores**\\n\\nWith the rise of embeddings, there has emerged a need for databases to support efficient storage and searching of these embeddings.\\nLangChain provides integrations with over 50 different vectorstores, from open-source local ones to cloud-hosted proprietary ones,\\nallowing you to choose the one best suited for your needs.\\nLangChain exposes a standard interface, allowing you to easily swap between vector stores.\\n\\n**Retrievers**\\n\\nOnce the data is in the database, you still need to retrieve it.\\nLangChain supports many different retrieval algorithms and is one of the places where we add the most value.\\nWe support basic methods that are easy to get started - namely simple semantic search.\\nHowever, we have also added a collection of algorithms on top of this to increase performance.\\nThese include:\\n\\n- [Parent Document Retriever](/docs/modules/data_connection/retrievers/parent_document_retriever): This allows you to create multiple embeddings per parent document, allowing you to look up smaller chunks but return larger context.\\n- [Self Query Retriever](/docs/modules/data_connection/retrievers/self_query): User questions often contain a reference to something that isn't just semantic but rather expresses some logic that can best be represented as a metadata filter. Self-query allows you to parse out the _semantic_ part of a query from other _metadata filters_ present in the query.\\n- [Ensemble Retriever](/docs/modules/data_connection/retrievers/ensemble): Sometimes you may want to retrieve documents from multiple different sources, or using multiple different algorithms. The ensemble retriever allows you to easily do this.\\n- And more!\", metadata={'description': \"Many LLM applications require user-specific data that is not part of the model's training set.\", 'language': 'en', 'source': 'https://python.langchain.com/docs/modules/data_connection/', 'title': 'Retrieval | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='### Going deeper\\u200b\\n\\n- Agents, such as the [conversational retrieval agent](/docs/use_cases/question_answering/how_to/conversational_retrieval_agents), can be used for retrieval when necessary while also holding a conversation.', metadata={'description': 'Open In Collab', 'language': 'en', 'source': 'https://python.langchain.com/docs/use_cases/chatbots', 'title': 'Chatbots | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content=\"[üìÑÔ∏è Amazon KendraAmazon Kendra is an intelligent search service provided by Amazon Web Services (AWS). It utilizes advanced natural language processing (NLP) and machine learning algorithms to enable powerful search capabilities across various data sources within an organization. Kendra is designed to help users find the information they need quickly and accurately, improving productivity and decision-making.](/docs/integrations/retrievers/amazon_kendra_retriever)[üìÑÔ∏è ArxivarXiv is an open-access archive for 2 million scholarly articles in the fields of physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and systems science, and economics.](/docs/integrations/retrievers/arxiv)[üìÑÔ∏è Azure Cognitive SearchAzure Cognitive Search (formerly known as Azure Search) is a cloud search service that gives developers infrastructure, APIs, and tools for building a rich search experience over private, heterogeneous content in web, mobile, and enterprise applications.](/docs/integrations/retrievers/azure_cognitive_search)[üìÑÔ∏è BM25BM25 also known as the Okapi BM25, is a ranking function used in information retrieval systems to estimate the relevance of documents to a given search query.](/docs/integrations/retrievers/bm25)[üìÑÔ∏è ChaindeskChaindesk platform brings data from anywhere (Datsources: Text, PDF, Word, PowerPpoint, Excel, Notion, Airtable, Google Sheets, etc..) into Datastores (container of multiple Datasources).](/docs/integrations/retrievers/chaindesk)[üìÑÔ∏è ChatGPT PluginOpenAI plugins connect ChatGPT to third-party applications. These plugins enable ChatGPT to interact with APIs defined by developers, enhancing ChatGPT's capabilities and allowing it to perform a wide range of actions.](/docs/integrations/retrievers/chatgpt-plugin)[üìÑÔ∏è Cohere RerankerCohere is a Canadian startup that provides natural language processing models that help companies improve human-machine interactions.](/docs/integrations/retrievers/cohere-reranker)[üìÑÔ∏è DocArray RetrieverDocArray is a versatile, open-source tool for managing your multi-modal data. It lets you shape your data however you want, and offers the flexibility to store and search it using various document index backends. Plus, it gets even better - you can utilize your DocArray document index to create a DocArrayRetriever, and build awesome Langchain apps!](/docs/integrations/retrievers/docarray_retriever)[üìÑÔ∏è ElasticSearch BM25Elasticsearch is a distributed, RESTful search and analytics engine. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents.](/docs/integrations/retrievers/elastic_search_bm25)[üìÑÔ∏è Google Cloud Enterprise SearchEnterprise Search is a part of the Generative AI App Builder suite of tools offered by Google Cloud.](/docs/integrations/retrievers/google_cloud_enterprise_search)[üìÑÔ∏è Google Drive RetrieverThis notebook covers how to retrieve documents from Google Drive.](/docs/integrations/retrievers/google_drive)[üìÑÔ∏è Kay.aiData API built for RAG üïµÔ∏è We are curating the world's largest datasets as high-quality embeddings so your AI agents can retrieve context on the fly. Latest models, fast retrieval, and zero infra.](/docs/integrations/retrievers/kay)[üìÑÔ∏è kNNIn statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression.](/docs/integrations/retrievers/knn)[üìÑÔ∏è LOTR (Merger Retriever)Lord of the Retrievers, also known as MergerRetriever, takes a list of retrievers as input and merges the results of their getrelevantdocuments() methods into a single list. The merged results will be a list of documents that are relevant to the query and that have been ranked by the different retrievers.](/docs/integrations/retrievers/merger_retriever)[üìÑÔ∏è MetalMetal is a managed service for ML Embeddings.](/docs/integrations/retrievers/metal)[üìÑÔ∏è Pinecone Hybrid SearchPinecone is a vector database with broad functionality.](/docs/integrations/retrievers/pinecone_hybrid_search)[üìÑÔ∏è PubMedPubMed¬Æ by The National Center for Biotechnology Information, National Library of Medicine comprises more than 35 million citations for biomedical literature from MEDLINE, life science\", metadata={'language': 'en', 'source': 'https://python.langchain.com/docs/integrations/retrievers', 'title': 'Retrievers | ü¶úÔ∏èüîó Langchain'}),\n Document(page_content='### Use the Zep Retriever to vector search over the Zep memory\\u200b\\n\\nZep provides native vector search over historical conversation memory. Embedding happens automatically.\\n\\nNOTE: Embedding of messages occurs asynchronously, so the first query may not return results. Subsequent queries will return results as the embeddings are generated.\\n\\n```python\\nfrom langchain.retrievers import ZepRetriever\\n\\nzep_retriever = ZepRetriever(\\n    session_id=session_id,  # Ensure that you provide the session_id when instantiating the Retriever\\n    url=ZEP_API_URL,\\n    top_k=5,\\n    api_key=zep_api_key,\\n)\\n\\nawait zep_retriever.aget_relevant_documents(\"Who wrote Parable of the Sower?\")', metadata={'description': 'Retriever Example for Zep - A long-term memory store for LLM applications.', 'language': 'en', 'source': 'https://python.langchain.com/docs/integrations/retrievers/zep_memorystore', 'title': 'Zep | ü¶úÔ∏èüîó Langchain'})]"
  },
  {
    "objectID": "notebooks/10_lost_in_the_middle_reranking.html",
    "href": "notebooks/10_lost_in_the_middle_reranking.html",
    "title": "Perdido en el medio: El problema con los contextos largos",
    "section": "",
    "text": "‚ÄúIndependientemente de la arquitectura de tu modelo, existe una degradaci√≥n sustancial del rendimiento cuando incluyes m√°s de 10 documentos recuperados. En resumen: Cuando los modelos deben acceder a informaci√≥n relevante en medio de contextos largos, tienden a ignorar los documentos proporcionados. Ver: https://arxiv.org/abs/2307.03172\nPara evitar este problema, puedes reordenar los documentos despu√©s de recuperarlos para evitar la degradaci√≥n del rendimiento.‚Äù\n\n\n\nLangchain\n\n\n\n\n{context}\n\n\n\nAnswer the following question, if you don‚Äôt know the answer, just write ‚ÄúI don‚Äôt know.\nQuestion: {question}‚Äú‚Äú‚Äù )\nllm = ChatOpenAI(temperature=0)\nstuff_chain = ( { ‚Äúcontext‚Äù: itemgetter(‚Äúquestion‚Äù) | retriever | reordering.transform_documents | combine_documents, ‚Äúquestion‚Äù: itemgetter(‚Äúquestion‚Äù), } | prompt | llm )\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nresponse = stuff_chain.invoke(input={\"question\": \"How to create a chain using LCEL?\"}).content\nprint(response)\n\nTo create a chain using LCEL, you can follow these steps:\n\n1. Import the necessary modules and classes from the LangChain library.\n2. Define the components of your chain, such as LLMs, prompts, and tools.\n3. Use the LCEL syntax to chain together the components in the desired order.\n4. Invoke the chain with the input data to get the output.\n\nHere is an example of creating a chain using LCEL:\n\n```python\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.anonymizers import PresidioReversibleAnonymizer\n\n# Define the components\nanonymizer = PresidioReversibleAnonymizer()\nprompt = PromptTemplate.from_template(template=\"{anonymized_text}\")\nllm = ChatOpenAI(temperature=0)\n\n# Chain the components together\nchain = {\"anonymized_text\": anonymizer.anonymize} | prompt | llm\n\n# Invoke the chain with input data\ntext = \"This is a sample text.\"\nresponse = chain.invoke(text)\n\n# Get the output\noutput = response.content\nprint(output)\n```\n\nIn this example, the chain starts with the `anonymizer.anonymize` function, which anonymizes the input text. The anonymized text is then passed to the `prompt` component, which generates a prompt using the anonymized text. Finally, the prompt is passed to the `llm` component, which generates the output response.\n\nNote that this is just a basic example, and you can customize the chain by adding more components or modifying the existing ones according to your requirements.\n\n:::"
  },
  {
    "objectID": "notebooks/11_everything_together.html#librer√≠as",
    "href": "notebooks/11_everything_together.html#librer√≠as",
    "title": "Conceptos aprendidos",
    "section": "Librer√≠as",
    "text": "Librer√≠as\n\nfrom functools import partial\nfrom operator import itemgetter\nfrom typing import Sequence\n\nfrom dotenv import load_dotenv\nfrom langchain.base_language import BaseLanguageModel\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.document_transformers import LongContextReorder\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.indexes import SQLRecordManager, index\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate\nfrom langchain.retrievers import BM25Retriever, EnsembleRetriever\nfrom langchain.schema import BaseRetriever, Document, StrOutputParser\nfrom langchain.schema.messages import BaseMessageChunk\nfrom langchain.schema.runnable import Runnable, RunnableMap\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import Chroma\n\nfrom src.langchain_docs_loader import LangchainDocsLoader, num_tokens_from_string\n\nload_dotenv()\n\nTrue"
  },
  {
    "objectID": "notebooks/11_everything_together.html#procesamiento-de-datos",
    "href": "notebooks/11_everything_together.html#procesamiento-de-datos",
    "title": "Conceptos aprendidos",
    "section": "Procesamiento de datos",
    "text": "Procesamiento de datos\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=800,\n    chunk_overlap=50,\n    length_function=num_tokens_from_string,\n)\n\n\nkeyword_docs = LangchainDocsLoader(\n    include_output_cells=True,\n    include_links_in_header=True,\n).load()\n\nsplitted_docs = text_splitter.split_documents(keyword_docs)\n\nfiltered_docs = [\n    doc\n    for doc in splitted_docs\n    if doc.page_content not in (\"```\", \"```text\", \"```python\")\n]\n\nlen(filtered_docs)\n\n2867"
  },
  {
    "objectID": "notebooks/11_everything_together.html#indexaci√≥n",
    "href": "notebooks/11_everything_together.html#indexaci√≥n",
    "title": "Conceptos aprendidos",
    "section": "Indexaci√≥n",
    "text": "Indexaci√≥n\n\nAlmacenaje de documento en Vectorstore\n\nrecord_manager = SQLRecordManager(\n    db_url=\"sqlite:///:memory:\",\n    namespace=\"langchain\",\n)\n\nrecord_manager.create_schema()\n\nembeddings = OpenAIEmbeddings()\n\nvectorstore = Chroma(collection_name=\"langchain\", embedding_function=embeddings)\n\nindexing_result = index(\n    docs_source=filtered_docs,\n    record_manager=record_manager,\n    vector_store=vectorstore,\n    batch_size=1000,\n    cleanup=\"full\",\n    source_id_key=\"source\",\n)\n\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 788782 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 712780 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 631346 / min. Contact us through our help center at help.openai.com if you continue to have issues..\nRetrying langchain.embeddings.openai.embed_with_retry.&lt;locals&gt;._embed_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-vwqjdaXGZeEg6mWAVSflJXD9 on tokens per min. Limit: 1000000 / min. Current: 554121 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n\n\n\nindexing_result\n\n{'num_added': 2851, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}\n\n\n\n\nObtenci√≥n de los documentos almacenados\nSi nuestra base de documentos inicial contenia documentos duplicados, estos se han eliminado en el proceso de indexaci√≥n. Por lo tanto, el n√∫mero de documentos almacenados en Vectorstore podr√≠a ser menor que el n√∫mero de documentos de la base inicial.\nAl obtener los documentos almacenados en Vectorstore podemos tener una copia fidedigna de la base de datos inicial, pero sin duplicados. Esta copia puede ser utlizada para crear un nuevo √≠ndice o inicializar un retriever.\n\nvector_keys = vectorstore.get(\n    ids=record_manager.list_keys(), include=[\"documents\", \"metadatas\"]\n)\n\ndocs_in_vectorstore = [\n    Document(page_content=page_content, metadata=metadata)\n    for page_content, metadata in zip(\n        vector_keys[\"documents\"], vector_keys[\"metadatas\"]\n    )\n]"
  },
  {
    "objectID": "notebooks/11_everything_together.html#inicializaci√≥n-de-retrievers",
    "href": "notebooks/11_everything_together.html#inicializaci√≥n-de-retrievers",
    "title": "Conceptos aprendidos",
    "section": "Inicializaci√≥n de retrievers",
    "text": "Inicializaci√≥n de retrievers\n\nkeyword_retriever = BM25Retriever.from_documents(docs_in_vectorstore)\nkeyword_retriever.k = 5\n\nsemantic_retriever = vectorstore.as_retriever(\n    search_type=\"mmr\",\n    search_kwargs={\n        \"k\": 5,\n        \"fetch_k\": 50,\n        \"lambda_mult\": 0.3,\n    },\n)\n\nretriever = EnsembleRetriever(\n    retrievers=[keyword_retriever, semantic_retriever],\n    weights=[0.3, 0.7],\n)"
  },
  {
    "objectID": "notebooks/11_everything_together.html#creaci√≥n-de-rag",
    "href": "notebooks/11_everything_together.html#creaci√≥n-de-rag",
    "title": "Conceptos aprendidos",
    "section": "Creaci√≥n de RAG",
    "text": "Creaci√≥n de RAG\n\nPrompts\n\nCONDENSE_QUESTION_TEMPLATE = \"\"\"\\\nGiven the following conversation and a follow up question, rephrase the follow up \\\nquestion to be a standalone question.\n\nChat History:\n====================\n{chat_history}\n====================\n\nFollow Up Input: {question}\nStandalone Question:\"\"\"\n\nSYSTEM_ANSWER_QUESTION_TEMPLATE = \"\"\"\\\nYou are an expert programmer and problem-solver, tasked with answering any question \\\nabout 'Langchain' with high quality answers and without making anything up.\n\nGenerate a comprehensive and informative answer of 80 words or less for the \\\ngiven question based solely on the provided search results (URL and content). You must \\\nonly use information from the provided search results. Use an unbiased and \\\njournalistic tone. Combine search results together into a coherent answer. Do not \\\nrepeat text. Cite search results using [${{number}}] noation. Only cite the most \\\nrelevant results that answer the question accurately. Place these citations at the end \\\nof the sentence or paragraph that reference them - do not put them all at the end. If \\\ndifferent results refer to different entities within the same name, write separate \\\nanswers for each entity.\n\nIf there is nothing in the context relevant to the question at hand, just say \"Hmm, \\\nI'm not sure.\". Don't try to make up an answer. This is not a suggestion. This is a rule.\n\nAnything between the following `context` html blocks is retrieved from a knowledge \\\nbank, not part of the conversation with the user.\n\n&lt;context&gt;\n    {context}\n&lt;/context&gt;\n\nREMBEMBER: If there is no relevant information withing the context, just say \"Hmm, \\\nI'm not sure.\". Don't try to make up an answer. This is not a suggestion. This is a rule. \\\nAnything between the preceding 'context' html blocks is retrieved from a knowledge bank, \\\nnot part of the conversation with the user.\n\nTake a deep breath and relax. You an exper programmer and problem-solver. You can do this.\nYou can cite all the relevant information from the search results. Let's go!\"\"\"\n\n\n\nCreaci√≥n de cadena de retrieval\n\ndef create_retriever_chain(\n    llm: BaseLanguageModel[BaseMessageChunk],\n    retriever: BaseRetriever,\n    use_chat_history: bool,\n):\n    CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(CONDENSE_QUESTION_TEMPLATE)\n    if not use_chat_history:\n        initial_chain = (itemgetter(\"question\")) | retriever\n        return initial_chain\n    else:\n        condense_question_chain = (\n            {\n                \"question\": itemgetter(\"question\"),\n                \"chat_history\": itemgetter(\"chat_history\"),\n            }\n            | CONDENSE_QUESTION_PROMPT\n            | llm\n            | StrOutputParser()\n        )\n        conversation_chain = condense_question_chain | retriever\n        return conversation_chain\n\n\n\nTruncado de documentos recuperados a un n√∫mero de documentos\n\ndef get_k_or_less_documents(documents: list[Document], k: int):\n    if len(documents) &lt;= k:\n        return documents\n    else:\n        return documents[:k]\n\n\n\nReordenado de documentos recuperados\n\ndef reorder_documents(documents: list[Document]):\n    reorder = LongContextReorder()\n    return reorder.transform_documents(documents)\n\n\n\nFormateo de documentos recuperados\n\ndef format_docs(docs: Sequence[Document]) -&gt; str:\n    formatted_docs: list[str] = []\n    for i, doc in enumerate(docs):\n        doc_string = f\"&lt;doc id='{i}'&gt;{doc.page_content}&lt;/doc&gt;\"\n        formatted_docs.append(doc_string)\n    return \"\\n\".join(formatted_docs)\n\n\n\nCreaci√≥n de cadena de respuesta\n\ndef create_answer_chain(\n    llm: BaseLanguageModel[BaseMessageChunk],\n    retriever: BaseRetriever,\n    use_chat_history: bool,\n    k: int = 5,\n) -&gt; Runnable:\n    retriever_chain = create_retriever_chain(llm, retriever, use_chat_history)\n\n    _get_k_or_less_documents = partial(get_k_or_less_documents, k=k)\n\n    context = RunnableMap(\n        {\n            \"context\": (\n                retriever_chain\n                | _get_k_or_less_documents\n                | reorder_documents\n                | format_docs\n            ),\n            \"question\": itemgetter(\"question\"),\n            \"chat_history\": itemgetter(\"chat_history\"),\n        }\n    )\n\n    prompt = ChatPromptTemplate.from_messages(\n        messages=[\n            (\"system\", SYSTEM_ANSWER_QUESTION_TEMPLATE),\n            MessagesPlaceholder(variable_name=\"chat_history\"),\n            (\"human\", \"{question}\"),\n        ]\n    )\n\n    response_synthesizer = prompt | llm | StrOutputParser()\n    response_chain = context | response_synthesizer\n\n    return response_chain"
  },
  {
    "objectID": "notebooks/11_everything_together.html#interacci√≥n-con-el-usuario",
    "href": "notebooks/11_everything_together.html#interacci√≥n-con-el-usuario",
    "title": "Conceptos aprendidos",
    "section": "Interacci√≥n con el usuario",
    "text": "Interacci√≥n con el usuario\n\nInicializaci√≥n del chatbot\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0.0)\n\nanswer_chain = create_answer_chain(\n    llm=llm, retriever=retriever, use_chat_history=False, k=6\n)\n\n\n\nEjemplo 1\n\nquestion = \"How to use .stream method in my chain with code example?\"\n\n\nprint(\n    answer_chain.invoke(  # type: ignore\n        {\n            \"question\": question,\n            \"chat_history\": [],\n        }\n    )\n)\n\nTo use the `.stream` method in your chain, you can follow these steps:\n\n1. Import the necessary classes:\n```python\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.chat_models import ChatOpenAI\n```\n\n2. Create an instance of the chat model:\n```python\nmodel = ChatOpenAI()\n```\n\n3. Define a prompt template using the `ChatPromptTemplate` class:\n```python\nprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n```\n\n4. Combine the prompt and model into a chain:\n```python\nchain = prompt | model\n```\n\n5. Use the `.stream` method to iterate over the streamed response:\n```python\nfor s in chain.stream({\"topic\": \"bears\"}):\n    print(s.content, end=\"\", flush=True)\n```\n\nThis will stream back chunks of the response, allowing you to process the output as it becomes available. In the example above, it will print a bear-themed joke.\n\nPlease note that this is just a basic example, and you can customize the prompt and model according to your specific use case.\n\n\n\nkeyword_docs = keyword_retriever.get_relevant_documents(\n    query=question,\n)\n\nfor i, doc in enumerate(keyword_docs, start=1):\n    print(f\"{i}. {doc.metadata['source']}: {doc.metadata.get('description', '')}\")\n\n1. https://python.langchain.com/docs/modules/memory/adding_memory: This notebook goes over how to use the Memory class with an LLMChain.\n2. https://python.langchain.com/docs/integrations/providers/langchain_decorators: lanchchain decorators is a layer on the top of LangChain that provides syntactic sugar üç≠ for writing custom langchain prompts and chains\n3. https://python.langchain.com/docs/use_cases/question_answering/how_to/code/twitter-the-algorithm-analysis-deeplake: In this tutorial, we are going to use Langchain + Activeloop's Deep Lake with GPT4 to analyze the code base of the twitter algorithm.\n4. https://python.langchain.com/docs/integrations/memory/motorhead_memory: Mot√∂rhead is a memory server implemented in Rust. It automatically handles incremental summarization in the background and allows for stateless applications.\n5. https://python.langchain.com/docs/guides/safety/moderation: This notebook walks through examples of how to use a moderation chain, and several common ways for doing so. Moderation chains are useful for detecting text that could be hateful, violent, etc. This can be useful to apply on both user input, but also on the output of a Language Model. Some API providers, like OpenAI, specifically prohibit you, or your end users, from generating some types of harmful content. To comply with this (and to just generally prevent your application from being harmful) you may often want to append a moderation chain to any LLMChains, in order to make sure any output the LLM generates is not harmful.\n\n\n\nsemantic_docs = semantic_retriever.get_relevant_documents(\n    query=question,\n)\n\nfor i, doc in enumerate(semantic_docs, start=1):\n    print(f\"{i}. {doc.metadata['source']}: {doc.metadata.get('description', '')}\")\n\n1. https://python.langchain.com/docs/expression_language/interface: In an effort to make it as easy as possible to create custom chains, we've implemented a \"Runnable\" protocol that most components implement. This is a standard interface with a few different methods, which makes it easy to define custom chains as well as making it possible to invoke them in a standard way. The standard interface exposed includes:\n2. https://python.langchain.com/docs/use_cases/apis: Open In Collab\n3. https://python.langchain.com/docs/modules/callbacks/: Head to Integrations for documentation on built-in callbacks integrations with 3rd-party tools.\n4. https://python.langchain.com/docs/guides/deployments/template_repos: So, you've created a really cool chain - now what? How do you deploy it and make it easily shareable with the world?\n5. https://python.langchain.com/docs/modules/chains/document/map_reduce: The map reduce documents chain first applies an LLM chain to each document individually (the Map step), treating the chain output as a new document. It then passes all the new documents to a separate combine documents chain to get a single output (the Reduce step). It can optionally first compress, or collapse, the mapped documents to make sure that they fit in the combine documents chain (which will often pass them to an LLM). This compression step is performed recursively if necessary.\n\n\n\nensemble_docs = retriever.get_relevant_documents(\n    query=question,\n)\n\nfor i, doc in enumerate(ensemble_docs, start=1):\n    print(f\"{i}. {doc.metadata['source']}: {doc.metadata.get('description', '')}\")\n\n1. https://python.langchain.com/docs/expression_language/interface: In an effort to make it as easy as possible to create custom chains, we've implemented a \"Runnable\" protocol that most components implement. This is a standard interface with a few different methods, which makes it easy to define custom chains as well as making it possible to invoke them in a standard way. The standard interface exposed includes:\n2. https://python.langchain.com/docs/use_cases/apis: Open In Collab\n3. https://python.langchain.com/docs/modules/callbacks/: Head to Integrations for documentation on built-in callbacks integrations with 3rd-party tools.\n4. https://python.langchain.com/docs/guides/deployments/template_repos: So, you've created a really cool chain - now what? How do you deploy it and make it easily shareable with the world?\n5. https://python.langchain.com/docs/modules/chains/document/map_reduce: The map reduce documents chain first applies an LLM chain to each document individually (the Map step), treating the chain output as a new document. It then passes all the new documents to a separate combine documents chain to get a single output (the Reduce step). It can optionally first compress, or collapse, the mapped documents to make sure that they fit in the combine documents chain (which will often pass them to an LLM). This compression step is performed recursively if necessary.\n6. https://python.langchain.com/docs/modules/memory/adding_memory: This notebook goes over how to use the Memory class with an LLMChain.\n7. https://python.langchain.com/docs/integrations/providers/langchain_decorators: lanchchain decorators is a layer on the top of LangChain that provides syntactic sugar üç≠ for writing custom langchain prompts and chains\n8. https://python.langchain.com/docs/use_cases/question_answering/how_to/code/twitter-the-algorithm-analysis-deeplake: In this tutorial, we are going to use Langchain + Activeloop's Deep Lake with GPT4 to analyze the code base of the twitter algorithm.\n9. https://python.langchain.com/docs/integrations/memory/motorhead_memory: Mot√∂rhead is a memory server implemented in Rust. It automatically handles incremental summarization in the background and allows for stateless applications.\n10. https://python.langchain.com/docs/guides/safety/moderation: This notebook walks through examples of how to use a moderation chain, and several common ways for doing so. Moderation chains are useful for detecting text that could be hateful, violent, etc. This can be useful to apply on both user input, but also on the output of a Language Model. Some API providers, like OpenAI, specifically prohibit you, or your end users, from generating some types of harmful content. To comply with this (and to just generally prevent your application from being harmful) you may often want to append a moderation chain to any LLMChains, in order to make sure any output the LLM generates is not harmful.\n\n\n\n\nEjemplo 2\n\nquestion = \"How to use .batch method in my chain with code example?\"\n\n\nprint(\n    answer_chain.invoke(  # type: ignore\n        {\n            \"question\": question,\n            \"chat_history\": [],\n        }\n    )\n)\n\nTo use the `.batch` method in your chain, you can follow the code example below:\n\n```python\nresults = agent_executor.batch([{\"input\": x} for x in inputs], return_exceptions=True)\n```\n\nIn this example, `agent_executor` is the instance of your chain, and `inputs` is a list of input questions or queries that you want to pass to the chain. The `.batch` method allows you to process multiple inputs in parallel, which can be more efficient than processing them one by one. The `return_exceptions=True` parameter ensures that any exceptions raised during the processing of inputs are returned instead of raising an error.\n\nPlease note that this code example assumes you have already set up your chain and have the necessary inputs ready.\n\n\n\nkeyword_docs = keyword_retriever.get_relevant_documents(\n    query=question,\n)\n\nfor i, doc in enumerate(keyword_docs, start=1):\n    print(f\"{i}. {doc.metadata['source']}: {doc.metadata.get('description', '')}\")\n\n1. https://python.langchain.com/docs/modules/memory/adding_memory: This notebook goes over how to use the Memory class with an LLMChain.\n2. https://python.langchain.com/docs/integrations/providers/langchain_decorators: lanchchain decorators is a layer on the top of LangChain that provides syntactic sugar üç≠ for writing custom langchain prompts and chains\n3. https://python.langchain.com/docs/use_cases/question_answering/how_to/code/twitter-the-algorithm-analysis-deeplake: In this tutorial, we are going to use Langchain + Activeloop's Deep Lake with GPT4 to analyze the code base of the twitter algorithm.\n4. https://python.langchain.com/docs/integrations/memory/motorhead_memory: Mot√∂rhead is a memory server implemented in Rust. It automatically handles incremental summarization in the background and allows for stateless applications.\n5. https://python.langchain.com/docs/guides/safety/moderation: This notebook walks through examples of how to use a moderation chain, and several common ways for doing so. Moderation chains are useful for detecting text that could be hateful, violent, etc. This can be useful to apply on both user input, but also on the output of a Language Model. Some API providers, like OpenAI, specifically prohibit you, or your end users, from generating some types of harmful content. To comply with this (and to just generally prevent your application from being harmful) you may often want to append a moderation chain to any LLMChains, in order to make sure any output the LLM generates is not harmful.\n\n\n\nsemantic_docs = semantic_retriever.get_relevant_documents(\n    query=question,\n)\n\nfor i, doc in enumerate(semantic_docs, start=1):\n    print(f\"{i}. {doc.metadata['source']}: {doc.metadata.get('description', '')}\")\n\n1. https://python.langchain.com/docs/use_cases/question_answering/how_to/conversational_retrieval_agents: This is an agent specifically optimized for doing retrieval when necessary and also holding a conversation.\n2. https://python.langchain.com/docs/use_cases/qa_structured/integrations/sqlite: This example demonstrates the use of the SQLDatabaseChain for answering questions over a SQL database.\n3. https://python.langchain.com/docs/integrations/vectorstores/chroma: Chroma is a AI-native open-source vector database focused on developer productivity and happiness. Chroma is licensed under Apache 2.0.\n4. https://python.langchain.com/docs/guides/langsmith/walkthrough: Open In Collab\n5. https://python.langchain.com/docs/modules/callbacks/: Head to Integrations for documentation on built-in callbacks integrations with 3rd-party tools.\n\n\n\nensemble_docs = retriever.get_relevant_documents(\n    query=question,\n)\n\nfor i, doc in enumerate(ensemble_docs, start=1):\n    print(f\"{i}. {doc.metadata['source']}: {doc.metadata.get('description', '')}\")\n\n1. https://python.langchain.com/docs/use_cases/question_answering/how_to/conversational_retrieval_agents: This is an agent specifically optimized for doing retrieval when necessary and also holding a conversation.\n2. https://python.langchain.com/docs/use_cases/qa_structured/integrations/sqlite: This example demonstrates the use of the SQLDatabaseChain for answering questions over a SQL database.\n3. https://python.langchain.com/docs/integrations/vectorstores/chroma: Chroma is a AI-native open-source vector database focused on developer productivity and happiness. Chroma is licensed under Apache 2.0.\n4. https://python.langchain.com/docs/guides/langsmith/walkthrough: Open In Collab\n5. https://python.langchain.com/docs/modules/callbacks/: Head to Integrations for documentation on built-in callbacks integrations with 3rd-party tools.\n6. https://python.langchain.com/docs/modules/memory/adding_memory: This notebook goes over how to use the Memory class with an LLMChain.\n7. https://python.langchain.com/docs/integrations/providers/langchain_decorators: lanchchain decorators is a layer on the top of LangChain that provides syntactic sugar üç≠ for writing custom langchain prompts and chains\n8. https://python.langchain.com/docs/use_cases/question_answering/how_to/code/twitter-the-algorithm-analysis-deeplake: In this tutorial, we are going to use Langchain + Activeloop's Deep Lake with GPT4 to analyze the code base of the twitter algorithm.\n9. https://python.langchain.com/docs/integrations/memory/motorhead_memory: Mot√∂rhead is a memory server implemented in Rust. It automatically handles incremental summarization in the background and allows for stateless applications.\n10. https://python.langchain.com/docs/guides/safety/moderation: This notebook walks through examples of how to use a moderation chain, and several common ways for doing so. Moderation chains are useful for detecting text that could be hateful, violent, etc. This can be useful to apply on both user input, but also on the output of a Language Model. Some API providers, like OpenAI, specifically prohibit you, or your end users, from generating some types of harmful content. To comply with this (and to just generally prevent your application from being harmful) you may often want to append a moderation chain to any LLMChains, in order to make sure any output the LLM generates is not harmful."
  }
]